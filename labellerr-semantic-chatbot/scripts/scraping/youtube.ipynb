{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6856f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Original entries: 33\n",
      "❌ Removing: LabelGPT: The Ultimate Auto Annotation Tool...\n",
      "❌ Removing: LabelGPT: Turns raw images into labeled images in ...\n",
      "❌ Removing: Labellerr: Unleash the power of automation in trai...\n",
      "❌ Removing: Labellerr Feature demo 4: Copy Previous file to sp...\n",
      "❌ Removing: Labellerr Feature Demo 3: Autolabel...\n",
      "❌ Removing: Labellerr feture demo 2: Quick Review...\n",
      "❌ Removing: Labellerr Feature demo 1: Search filters for files...\n",
      "❌ Removing: Track Safety project: Project setup and Image anno...\n",
      "❌ Removing: Image Annotation for Hard Hat Object Detection: De...\n",
      "❌ Removing: Data Annotation for fitness AI model training: pos...\n",
      "❌ Removing: How Conversational AI and IOT are Redefining the F...\n",
      "❌ Removing: Text Annotation for Named Entity Recognition using...\n",
      "❌ Removing: Intelligent Document Processing Automation with Da...\n",
      "❌ Removing: Revolutionizing entertainment industry with Labell...\n",
      "❌ Removing: Labellerr, AI in Retail - Supply Chain Machine Lea...\n",
      "❌ Removing: Attribute Recognisation by Labeller, Machine Learn...\n",
      "❌ Removing: Fashion Parsing - Data Annotation by Labellerr Aut...\n",
      "❌ Removing: AI Powered Data Annotation to Build Incredible Com...\n",
      "❌ Removing: Crubside Pickup Made Efficient with Data Annotatio...\n",
      "❌ Removing: Accurate Data Annotation Increases Online Sales...\n",
      "❌ Removing: How Planogram (POG) Compliance can Help Retailers ...\n",
      "❌ Removing: Bounding Box annotation in a retail use case with ...\n",
      "❌ Removing: Labellerr - Data Labelling Platform (B2B SaaS)...\n",
      "❌ Removing: Labeling process on Labellerr...\n",
      "❌ Removing: Creating new project and importing data in Labelle...\n",
      "❌ Removing: AI and Deep Learning assisted image classification...\n",
      "❌ Removing: Labellerr Demo(www.labellerr.com)...\n",
      "❌ Removing: Labellerr Demo(www.labellerr.com)...\n",
      "✅ Filtered entries: 5\n",
      "🗑️  Removed entries: 28\n",
      "💾 Saved to: kome_transcripts_filtered.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Filter out transcript entries with unavailable transcripts message\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def filter_transcripts(input_file, output_file):\n",
    "    \"\"\"Remove entries with unavailable transcript message\"\"\"\n",
    "    \n",
    "    # The text to look for and remove\n",
    "    unavailable_text = \"TranscriptCOPYTranscripts aren't available for this video. The publisher may\"\n",
    "    \n",
    "    # Read the input JSON\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"📊 Original entries: {len(data)}\")\n",
    "    \n",
    "    # Filter out entries containing the unavailable text\n",
    "    filtered_data = []\n",
    "    removed_count = 0\n",
    "    \n",
    "    for entry in data:\n",
    "        transcript = entry.get('transcript', '')\n",
    "        \n",
    "        if unavailable_text in transcript:\n",
    "            print(f\"❌ Removing: {entry.get('title', 'Unknown')[:50]}...\")\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            filtered_data.append(entry)\n",
    "    \n",
    "    # Save filtered data\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Filtered entries: {len(filtered_data)}\")\n",
    "    print(f\"🗑️  Removed entries: {removed_count}\")\n",
    "    print(f\"💾 Saved to: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"kome_transcripts.json\"  # Your current transcript file\n",
    "    output_file = \"kome_transcripts_filtered.json\"  # Filtered output\n",
    "    \n",
    "    filter_transcripts(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc8fcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Translating 4 transcripts...\n",
      "[1/4] Top Object Annotation Services | AI-Powe...\n",
      "  ✅ Translated 939 characters\n",
      "[2/4] Enhance Accuracy with Advanced Image Dis...\n",
      "  ✅ Translated 2171 characters\n",
      "[3/4] Labellerr Podcast: Insights from Soma Dh...\n",
      "Translation error: Request exception can happen due to an api connection error. Please check your connection and try again\n",
      "  ✅ Translated 7272 characters\n",
      "[4/4] AI-Powered Data Annotation Services and ...\n",
      "  ✅ Translated 549 characters\n",
      "💾 Saved to: kome_transcripts_english.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Translate transcripts using Deep Translator (more reliable)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "def translate_with_deep_translator(text, source='hi', target='en'):\n",
    "    \"\"\"Translate using Deep Translator\"\"\"\n",
    "    try:\n",
    "        translator = GoogleTranslator(source=source, target=target)\n",
    "        \n",
    "        # Handle long texts\n",
    "        if len(text) > 4500:\n",
    "            chunks = [text[i:i+4500] for i in range(0, len(text), 4500)]\n",
    "            translated_chunks = []\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                translated = translator.translate(chunk)\n",
    "                translated_chunks.append(translated)\n",
    "                time.sleep(0.3)\n",
    "            \n",
    "            return ' '.join(translated_chunks)\n",
    "        else:\n",
    "            return translator.translate(text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return text\n",
    "\n",
    "def batch_translate_transcripts(input_file, output_file):\n",
    "    \"\"\"Batch translate all transcripts\"\"\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"🔄 Translating {len(data)} transcripts...\")\n",
    "    \n",
    "    for i, entry in enumerate(data, 1):\n",
    "        transcript = entry.get('transcript', '')\n",
    "        \n",
    "        if transcript and len(transcript.strip()) > 20:\n",
    "            print(f\"[{i}/{len(data)}] {entry.get('title', 'Unknown')[:40]}...\")\n",
    "            \n",
    "            # Clean up the transcript text\n",
    "            cleaned_text = transcript.replace('\\\\n', ' ').strip()\n",
    "            \n",
    "            # Translate\n",
    "            english_translation = translate_with_deep_translator(cleaned_text)\n",
    "            \n",
    "            # Update entry\n",
    "            entry['transcript_english'] = english_translation\n",
    "            entry['transcript_original'] = transcript\n",
    "            entry['translation_status'] = 'success'\n",
    "            \n",
    "            print(f\"  ✅ Translated {len(english_translation)} characters\")\n",
    "        else:\n",
    "            entry['transcript_english'] = transcript\n",
    "            entry['transcript_original'] = transcript\n",
    "            entry['translation_status'] = 'skipped'\n",
    "        \n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "    # Save results\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved to: {output_file}\")\n",
    "\n",
    "# Install deep-translator if needed\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deep-translator\"])\n",
    "    from deep_translator import GoogleTranslator\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_translate_transcripts(\"kome_transcripts_filtered.json\", \"kome_transcripts_english.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277faadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed. Combined 4062 unique entries saved in combined_unique.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def remove_duplicates(json1, json2, key='url'):\n",
    "    combined = json1 + json2\n",
    "    seen = set()\n",
    "    unique_entries = []\n",
    "    for entry in combined:\n",
    "        identifier = entry.get(key)\n",
    "        if identifier and identifier not in seen:\n",
    "            seen.add(identifier)\n",
    "            unique_entries.append(entry)\n",
    "    return unique_entries\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_1 = \"data_ingest/raw/documentation/doc_headings_simple.json\"  # Replace with your first JSON file path\n",
    "    json_file_2 = \"data_ingest/raw/documentation/labellerr_documentation_headings.json\"  # Replace with your second JSON file path\n",
    "    output_file = \"combined_unique.json\"\n",
    "    \n",
    "    data1 = load_json(json_file_1)\n",
    "    data2 = load_json(json_file_2)\n",
    "    \n",
    "    unique_data = remove_duplicates(data1, data2, key='url')  # Use 'url' or other unique key relevant for your data\n",
    "    \n",
    "    save_json(unique_data, output_file)\n",
    "    \n",
    "    print(f\"Duplicates removed. Combined {len(unique_data)} unique entries saved in {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27701bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
