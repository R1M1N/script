[
  {
    "video_id": "L9RNTGcFbUg",
    "title": "Fine-Tune YOLO for Traffic Flow Counting",
    "description": "Unlock the power of computer vision with this step-by-step tutorial on Traffic Flow Counting using YOLO. 🚦\n\nIn this video, you’ll learn how to:\n\n- Fine-tune YOLO for vehicle detection and traffic analysis\n- Count vehicles across defined regions in live or recorded videos\n- Handle both normal camera and drone aerial traffic footage\n- Annotate datasets and train a custom YOLO model for accurate results\n- Apply traffic counting for smart cities, road safety, congestion monitoring, and delivery optimization\n\nResources:\n- Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine-Tune-YOLO-for-Traffic-Flow-Counting.ipynb\n- Labellerr’s Github:https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n#trafficflow #yolo #computervision #vehiclecounting #objectdetection #aianalytics #smartcity #trafficanalysis #deeplearning #machinelearning #dronedata #aivideoanalytics #yolov8 #trafficsolutions #aiprojects #smartmobility",
    "video_url": "https://www.youtube.com/watch?v=L9RNTGcFbUg",
    "embed_url": "https://www.youtube.com/embed/L9RNTGcFbUg",
    "duration": 609,
    "view_count": 73,
    "upload_date": "20250819",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "annotation software",
      "cvat features",
      "data workflow",
      "cvat tutorial",
      "workflow automation",
      "machine learning",
      "project tracking",
      "automation tools",
      "cvat guide",
      "data processing",
      "project dashboard",
      "computer vision",
      "data labeling",
      "project collaboration",
      "machine learning tutorial",
      "ml",
      "machine learning tutorial for beginners",
      "cvat import annotation",
      "what is machine learning"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Every day, millions of vehicles move through our roads, creating complex traffic patterns that impact safety, efficiency, and even the environment. But how do we truly understand this movement? That's where traffic flow counting comes in. By accurately counting vehicles, we unlock powerful insights like identifying congestion hotspots, predicting peak travel times, and planning smarter road layouts. Traffic authorities can even use this data to divert vehicles in real time, reducing gridlock and improving emergency response. Beyond city planning, businesses benefit, too. Delivery services optimize routes, ride sharing apps reduce waiting times, and smart mobility solutions become a reality. In this tutorial, I'll show you how to fine-tune YOLO for traffic flow counting, turning raw video feeds into meaningful data that drives smarter decisions. [Music] Hello everyone, welcome to this tutorial on fine-tuning YOLO for traffic flow counting. As you've seen our intro in this tutorial, we are going to learn how you can create computer ving logic and fine-tune yolo to create a script where you can count the flow of traffic traffic in a particular region. So to do that you just have to follow some steps. Our first step is to import the required libraries. Now for plotting region to track object I have created a simple script where we have to just uh store the polygon means the points where we have clicked to store the value of our region. So let's run this now. I have to just click the point I want to then this is our first reason then this is our second reason now we have selected our boat region let's press Q to quit as you seen This is the value I have gotten for our region where we will track the vehicle which pass through to it and count it. So let's show our reason. We should copy this and paste it here. So this is the reason I have selected. So our first step is completed. Let's get to next step. In our next step, we have to write the logic behind how we are going to count the vehicle when it passed to the region we have uh plotted. So to do that we just have to create a tracker dictionary with each polygon region as its key and its counter value as a dictionary value and when we track it using YOLO model it will just increase the value by one. So how our logic going to work here? We are going to track not only track the bounding boxes of the object but we are going to track the centr of the object. So it will make us easier to detect if that centroid is in our region and increase that region value by one. So I have written that code. So let's run this. In this code, I just have to provide the polygon which I have already created which has the region we have plotted and the video part I am going to use. So let's see our result. As you see in our inference result, the logic worked perfectly. All the vehicle which passed through the region were counted and the value counted increases perfectly. So let's get to our next step. Our previous step work perfectly because the camera was of a normal view. What will happen when we have a drone view like this. In this scenario, our YOLO model will not perfectly inference the video. So to do that, we have to train a custom YOLO model. So let's train a custom YOLO model for this kind of scenario where the input feed is of a drone video. Let's get to our annotation part. Before training YOLO model, we have to annotate our data set on which we are going to track our objects. So we can use our label platform to annotate our we just have to create the classes for it and use the bonding boxes on each of the detected vehicle or object we want to detect using YOLO. Similarly, we will repeat this process until all of our data set has been annotated and export that in Coco JSON format. When you export the annotation in Coco JSON format, next process is you have to clone this repo. Using this repo, you will convert your Coco to YOLO format for custom model training. I have already done that. It looks like this. As you seen it has images, labels and data set. This contain the names of my classes and the training and validation data set. But I am using all my images for training. So using this you can convert your cocojent to YOLO format and then use this command line task is equal to detect mode equals to train and the part to data set and model name which is yolo vatex.pd to train your model. I have trained this model. Let's see the result of tracking using our custom model. As you seen our tracking using custom model works perfectly. Now we have to repeat our previous task like creating region from where if the object passes the counter will be increased by one. I have already created a region. I am going to track. Let's see the region. So these are the four region I have selected. The this is road one, road two, road three, road four. And when the car pass through this, it will automatically increase the counter value by one for that specific reason. Let's see our result. As you've seen our new custom train model works perfectly and follow our previous logic. So now you have learned how you can track an vehicle when passed through a specific reason. You can get this notebook on our video description. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Every day, millions of vehicles move through our roads, creating complex traffic patterns that impact safety, efficiency, and even the environment. But how do we truly understand this movement? That's where traffic flow counting comes in. By accurately counting vehicles, we unlock powerful insights like identifying congestion hotspots, predicting peak travel times, and planning smarter road layouts. Traffic authorities can even use this data to divert vehicles in real time, reducing gridlock and improving emergency response. Beyond city planning, businesses benefit, too. Delivery services optimize routes, ride sharing apps reduce waiting times, and smart mobility solutions become a reality. In this tutorial, I'll show you how to fine-tune YOLO for traffic flow counting, turning raw video feeds into meaningful data that drives smarter decisions. [Music] Hello everyone, welcome to this tutorial on fine-tuning YOLO for traffic flow counting. As you've seen our intro in this tutorial, we are going to learn how you can create computer ving logic and fine-tune yolo to create a script where you can count the flow of traffic traffic in a particular region. So to do that you just have to follow some steps. Our first step is to import the required libraries. Now for plotting region to track object I have created a simple script where we have to just uh store the polygon means the points where we have clicked to store the value of our region. So let's run this now. I have to just click the point I want to then this is our first reason then this is our second reason now we have selected our boat region let's press Q to quit as you seen This is the value I have gotten for our region where we will track the vehicle which pass through to it and count it.",
      "So let's show our reason. We should copy this and paste it here. So this is the reason I have selected. So our first step is completed. Let's get to next step. In our next step, we have to write the logic behind how we are going to count the vehicle when it passed to the region we have uh plotted. So to do that we just have to create a tracker dictionary with each polygon region as its key and its counter value as a dictionary value and when we track it using YOLO model it will just increase the value by one. So how our logic going to work here? We are going to track not only track the bounding boxes of the object but we are going to track the centr of the object. So it will make us easier to detect if that centroid is in our region and increase that region value by one. So I have written that code. So let's run this. In this code, I just have to provide the polygon which I have already created which has the region we have plotted and the video part I am going to use. So let's see our result. As you see in our inference result, the logic worked perfectly. All the vehicle which passed through the region were counted and the value counted increases perfectly. So let's get to our next step. Our previous step work perfectly because the camera was of a normal view. What will happen when we have a drone view like this. In this scenario, our YOLO model will not perfectly inference the video. So to do that, we have to train a custom YOLO model. So let's train a custom YOLO model for this kind of scenario where",
      "the input feed is of a drone video. Let's get to our annotation part. Before training YOLO model, we have to annotate our data set on which we are going to track our objects. So we can use our label platform to annotate our we just have to create the classes for it and use the bonding boxes on each of the detected vehicle or object we want to detect using YOLO. Similarly, we will repeat this process until all of our data set has been annotated and export that in Coco JSON format. When you export the annotation in Coco JSON format, next process is you have to clone this repo. Using this repo, you will convert your Coco to YOLO format for custom model training. I have already done that. It looks like this. As you seen it has images, labels and data set. This contain the names of my classes and the training and validation data set. But I am using all my images for training. So using this you can convert your cocojent to YOLO format and then use this command line task is equal to detect mode equals to train and the part to data set and model name which is yolo vatex.pd to train your model. I have trained this model. Let's see the result of tracking using our custom model. As you seen our tracking using custom model works perfectly. Now we have to repeat our previous task like creating region from where if the object passes the counter will be increased by one. I have already created a region. I am going to track. Let's see the region. So these are the four region I have selected. The this is road one, road two, road three, road four. And when the car",
      "pass through this, it will automatically increase the counter value by one for that specific reason. Let's see our result. As you've seen our new custom train model works perfectly and follow our previous logic. So now you have learned how you can track an vehicle when passed through a specific reason. You can get this notebook on our video description. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 983,
    "transcript_chunk_count": 4
  },
  {
    "video_id": "fsYoIE9Txmc",
    "title": "Using YOLO for 3D Cuboid Detection",
    "description": "Ever wondered how robots detect objects in 3D with precision? 🚀\nIn this tutorial, we’ll walk you through 3D cuboid detection using YOLO for both images and videos. Learn how to estimate object position, orientation, and size in 3D space with simple step-by-step code implementation.\n\nYou will learn:\n✅ What is 3D cuboid detection and why it matters in robotics & computer vision\n✅ How to set up YOLO for 3D object detection\n✅ Running inference on images and videos\n✅ Real-world applications in navigation, robotics, and automation\n\nResources:\n- Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Using-YOLO-for-3D-Cuboid_detection.ipynb\n- Labellerr’s Official GitHub: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#3DCuboidDetection #YOLO3D #ObjectDetection #3DObjectDetection #ComputerVision #MachineLearning #ArtificialIntelligence #DeepLearning #Robotics #AI #3DTutorial #YOLONetwork #CVPR #Automation #TechTutorial #Coding",
    "video_url": "https://www.youtube.com/watch?v=fsYoIE9Txmc",
    "embed_url": "https://www.youtube.com/embed/fsYoIE9Txmc",
    "duration": 206,
    "view_count": 22,
    "upload_date": "20250818",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "data annotation tech",
      "dat",
      "yolo model",
      "model train",
      "yolo model for object detection",
      "yolo model tutorial",
      "yolo model training",
      "yolo model project",
      "machine learning",
      "object detection using opencv python",
      "object detection using yolo",
      "object detection project using python",
      "data science",
      "ai",
      "deep learning",
      "object detection",
      "computer vision",
      "object detection python",
      "what is machine learning"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Have you ever wondered how robots can grab objects with pinpoint precision, navigate complex environments, or even understand the spatial relationships between objects in their world? The secret lies in their ability to perceive their surrounding objects dimensionally, not just as flat images, but as rich spatial models they can interact with. One of the most powerful tools in this process is 3D cuboid detection, a technique that lets machines identify objects in three dimensions, estimating their position, orientation, and size. In this tutorial, I'll walk you through the code needed to detect 3D cuboids in real world scenes step by step, and in a way you can implement yourself. [Music] Hello everyone, welcome to this tutorial of using YOLO for 3D cover detection. In this tutorial, we are going to learn how to implement 3D cover detection on videos or images. So to do this, you just have to clone this repo. I have already cloned that repo. So let's move to next. To use this repo, simply I have created a script. So using command line you can reference a video. So let's clone this as it has already cloned. So this is the script I have written. You just have to move that script into YOLO 3D. I have already done that. So you can see it's here. So let's move to our next step. we have to install all the requirements txt modules. So let's wait for as the installation part is complete. Let's move to our next step. As the installation part is completed, now it's time to imperence on a sample video. Let's do on a simple video. We just have to provide the your inference py script the parts video and then the output name and also the class ID you want to detect and the yolo model you want. So using this you can inference on a sample video. Let's see the result. Similarly, you can in friends on other videos. Let's see the result of other sample videos with this. Now you have learned to implement 3D cover detection on images and video. >> For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Have you ever wondered how robots can grab objects with pinpoint precision, navigate complex environments, or even understand the spatial relationships between objects in their world? The secret lies in their ability to perceive their surrounding objects dimensionally, not just as flat images, but as rich spatial models they can interact with. One of the most powerful tools in this process is 3D cuboid detection, a technique that lets machines identify objects in three dimensions, estimating their position, orientation, and size. In this tutorial, I'll walk you through the code needed to detect 3D cuboids in real world scenes step by step, and in a way you can implement yourself. [Music] Hello everyone, welcome to this tutorial of using YOLO for 3D cover detection. In this tutorial, we are going to learn how to implement 3D cover detection on videos or images. So to do this, you just have to clone this repo. I have already cloned that repo. So let's move to next. To use this repo, simply I have created a script. So using command line you can reference a video. So let's clone this as it has already cloned. So this is the script I have written. You just have to move that script into YOLO 3D. I have already done that. So you can see it's here. So let's move to our next step. we have to install all the requirements txt modules. So let's wait for as the installation part is complete. Let's move to our next step. As the installation part is completed, now it's time to imperence on a sample video. Let's do on a simple video. We just have to provide the your inference py script the parts video and then the output name and also the class ID you want to",
      "detect and the yolo model you want. So using this you can inference on a sample video. Let's see the result. Similarly, you can in friends on other videos. Let's see the result of other sample videos with this. Now you have learned to implement 3D cover detection on images and video. >> For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 375,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "j9cO9HvDYrY",
    "title": "Top Award-Winning Research at CVPR 2025",
    "description": "CVPR 2025 wrapped up in Nashville this June, bringing together 9,375 researchers, engineers, and innovators from 75 countries for one of the world’s most prestigious computer vision conferences. With 13,008 submissions and only 22.1% acceptance, the event was a showcase of the most cutting-edge AI research in vision, robotics, and multimodal learning. From keynote talks and workshops to live demos of groundbreaking technology, the energy was unmatched.\n\nIn this video, we’ll highlight the award-winning papers that stole the spotlight: MegaSAM for accurate 3D mapping from everyday videos, Navigation World Models for adaptable robot navigation, Discrete Diffusion Timestep Tokens for next-gen multimodal AI, Neural Inverse Rendering for capturing hidden light information, and the Visual Geometry Transformer—the Best Paper of CVPR 2025—offering lightning-fast, real-time 3D reconstruction.\n\n- BLOG: https://www.labellerr.com/blog/cvpr-2025-part-1/\n\nChapters:\n[0:00] Introduction to CVPR 2025 and Conference Highlights\n[0:27] Overview of Top Award-Winning Papers and Models\n[0:49] Best Paper Honorable Mentions\n[0:54] Mega SAM: Accurate 3D Mapping from Casual Videos\n[1:53] Navigation World Models: Adaptive Robot Navigation\n[2:46] Best Student Paper Honorable Mentions\n[2:51] Generative Multimodal Pre-Training with Discrete Diffusion Timestep Tokens\n[3:50] Best Student Paper: Neural Inverse Rendering from Propagating Light\n[5:01] Best Paper: Visual Geometry Grounded Transformer\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#CVPR2025 #ComputerVision #ArtificialIntelligence #MachineLearning #DeepLearning #3DMapping #Robotics #AutonomousSystems #VisionLanguageModels #MultimodalAI #NeuralRendering #LightModeling #VisualGeometryTransformer #MegaSAM #NavigationWorldModels #DiscreteDiffusion #AIResearch #AR #VR #SelfDrivingCars",
    "video_url": "https://www.youtube.com/watch?v=j9cO9HvDYrY",
    "embed_url": "https://www.youtube.com/embed/j9cO9HvDYrY",
    "duration": 348,
    "view_count": 46,
    "upload_date": "20250812",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "data annotation tech",
      "image annotation",
      "image labeling tool",
      "image annotation software",
      "video annotation tool",
      "machine learning",
      "video labeling",
      "video labeling tool",
      "computer vision",
      "ml",
      "deep learning",
      "data labeling",
      "video labeling software",
      "best data labeling services",
      "computer science",
      "image labeling",
      "data labeling service",
      "training data for ai",
      "data annotation tool"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] CVPR 2025 wrapped up in Nashville this June, delivering another impressive showcase of computer vision research. This year brought together 9,375 attendees from 75 countries, all buzzing around 2,878 accepted papers. That's only 22.1% of the massive 13,08 submissions. So, you know, the ones that made it are something special. And in this video, I'll walk you through the papers and models which are conference's top award winners. Models like VGGT which rethinks real world 3D scene understanding. If you care about 3D annotation, video understanding or foundation models, this video is for you. See it till end. We start by mentioning the best paper honorable mentions by CVPR. In that category, first one is Mega SAM. Accurate, fast, and robust structure and motion from casual dynamic videos is chosen by CVPR. Most 3D mapping methods like slam work great in perfect conditions, lots of camera movement, still scenes, and controlled capture, but that's not how most of our videos look. Casual videos often have little camera motion, moving people, cars, and unpredictable views. And traditional methods fail here. Mega Sam changes that. It's built for dynamic everyday videos. It uses depth clues from single frames in a motion probability map to spot what's moving. Then it combines all that in an uncertainty aware adjustment step, figuring out the best camera path and scene layout, even with shaky footage and moving objects. The result is a more accurate, faster 3D maps from regular videos. Perfect for AR, VR, robotics, and autonomous systems. With Mega SAM, any video can become a reliable 3D scene. Next, an honorable mention is navigation world models. Most robots today navigate with fixed behaviors. They follow exactly what they were trained for. If you teach one to move through hallways, it won't adapt when you suddenly wanted to avoid a construction zone or follow new rules. Navigation world models changes that. Instead of reacting step by step, it predicts what the robot would see as it moves, like imagining the future. Using a powerful conditional diffusion transformer trained on massive amounts of firstperson video, it can mentally explore different routes, check if they meet new constraints, and pick the best one. This means it works in familiar spaces and can even plan in completely new places from just a single photo. It's a leap toward navigation that's flexible, predictive, and truly adaptable. Next in best student paper, honorable mentions chosen by CVPR. Generative multimodal pre-training with discrete diffusion timestep tokens was selected in as best student paper. Vision language models have a tricky problem. For understanding tasks, they need to compress an image into abstract meaning, ignoring extra details. But for generating images, they must keep enough detail to recreate them in high quality. Most current models treat images as grids of patches or spatial tokens, but this structure doesn't match how language models naturally process information. It's like giving them a language they can't speak fluently. This research paper fixes this with discrete diffusion timestep tokenization. Instead of spatial grids, it organizes tokens by how information disappears during image generation, creating a natural layered hierarchy like sentences in language. This makes it far easier for the model to understand and generate images at a high level and could shape the next generation of multimodal AI, balancing deep understanding with stunning image quality. Finally, best student paper in CVPR 2025. Neural inverse rendering from propagating light is selected as best student paper. Most litter systems only use direct light, the beam that bounces off an object once and comes straight back. But in reality, light also bounces around multiple times, carrying hidden clues about the scene's materials, shapes, and layout. Traditionally, this indirect light is thrown away as noise. This new research changes that with a time resolved radiance cache. Think of it as a smart memory that records how light moves through a scene over time. First, it stores all that light transport data. Then, a neural network quickly searches it to reconstruct scenes more accurately, even in tricky lighting. It can separate direct and indirect light, relight scenes, or even create videos showing light movement from new angles. This could make autonomous vehicles and 3D scanning far better in complex conditions, blending physics and AI for next level vision. Finally, the best paper of CVPR 2025. Visual geometry grounded transformer is awarded the best paper in CVPR 2025. Visual geometry grounded transformer takes a fresh approach. Instead of solving 3D geometry through slow optimization, it predicts it directly. Give it anywhere from one to hundreds of images, and it instantly outputs camera positions, depth maps, 3D points, and even motion paths. Its alternating attention layers let it zoom in on single frames for detail. Then connect the dots across all images for global accuracy. and it predicts multiple 3D properties at once, boosting performance. It's over 40 times faster than some leading methods while staying accurate. This makes real-time 3D mapping for AR robots and self-driving cars much more practical.",
    "transcript_chunks": [
      "[Music] CVPR 2025 wrapped up in Nashville this June, delivering another impressive showcase of computer vision research. This year brought together 9,375 attendees from 75 countries, all buzzing around 2,878 accepted papers. That's only 22.1% of the massive 13,08 submissions. So, you know, the ones that made it are something special. And in this video, I'll walk you through the papers and models which are conference's top award winners. Models like VGGT which rethinks real world 3D scene understanding. If you care about 3D annotation, video understanding or foundation models, this video is for you. See it till end. We start by mentioning the best paper honorable mentions by CVPR. In that category, first one is Mega SAM. Accurate, fast, and robust structure and motion from casual dynamic videos is chosen by CVPR. Most 3D mapping methods like slam work great in perfect conditions, lots of camera movement, still scenes, and controlled capture, but that's not how most of our videos look. Casual videos often have little camera motion, moving people, cars, and unpredictable views. And traditional methods fail here. Mega Sam changes that. It's built for dynamic everyday videos. It uses depth clues from single frames in a motion probability map to spot what's moving. Then it combines all that in an uncertainty aware adjustment step, figuring out the best camera path and scene layout, even with shaky footage and moving objects. The result is a more accurate, faster 3D maps from regular videos. Perfect for AR, VR, robotics, and autonomous systems. With Mega SAM, any video can become a reliable 3D scene. Next, an honorable mention is navigation world models. Most robots today navigate with fixed behaviors. They follow exactly what they were trained for. If you teach one to move through hallways, it won't adapt when you",
      "suddenly wanted to avoid a construction zone or follow new rules. Navigation world models changes that. Instead of reacting step by step, it predicts what the robot would see as it moves, like imagining the future. Using a powerful conditional diffusion transformer trained on massive amounts of firstperson video, it can mentally explore different routes, check if they meet new constraints, and pick the best one. This means it works in familiar spaces and can even plan in completely new places from just a single photo. It's a leap toward navigation that's flexible, predictive, and truly adaptable. Next in best student paper, honorable mentions chosen by CVPR. Generative multimodal pre-training with discrete diffusion timestep tokens was selected in as best student paper. Vision language models have a tricky problem. For understanding tasks, they need to compress an image into abstract meaning, ignoring extra details. But for generating images, they must keep enough detail to recreate them in high quality. Most current models treat images as grids of patches or spatial tokens, but this structure doesn't match how language models naturally process information. It's like giving them a language they can't speak fluently. This research paper fixes this with discrete diffusion timestep tokenization. Instead of spatial grids, it organizes tokens by how information disappears during image generation, creating a natural layered hierarchy like sentences in language. This makes it far easier for the model to understand and generate images at a high level and could shape the next generation of multimodal AI, balancing deep understanding with stunning image quality. Finally, best student paper in CVPR 2025. Neural inverse rendering from propagating light is selected as best student paper. Most litter systems only use direct light, the beam that bounces off an object once and comes straight back. But in reality, light",
      "also bounces around multiple times, carrying hidden clues about the scene's materials, shapes, and layout. Traditionally, this indirect light is thrown away as noise. This new research changes that with a time resolved radiance cache. Think of it as a smart memory that records how light moves through a scene over time. First, it stores all that light transport data. Then, a neural network quickly searches it to reconstruct scenes more accurately, even in tricky lighting. It can separate direct and indirect light, relight scenes, or even create videos showing light movement from new angles. This could make autonomous vehicles and 3D scanning far better in complex conditions, blending physics and AI for next level vision. Finally, the best paper of CVPR 2025. Visual geometry grounded transformer is awarded the best paper in CVPR 2025. Visual geometry grounded transformer takes a fresh approach. Instead of solving 3D geometry through slow optimization, it predicts it directly. Give it anywhere from one to hundreds of images, and it instantly outputs camera positions, depth maps, 3D points, and even motion paths. Its alternating attention layers let it zoom in on single frames for detail. Then connect the dots across all images for global accuracy. and it predicts multiple 3D properties at once, boosting performance. It's over 40 times faster than some leading methods while staying accurate. This makes real-time 3D mapping for AR robots and self-driving cars much more practical."
    ],
    "transcript_word_count": 837,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "fFjIkRte5R8",
    "title": "Fine-Tune YOLO for Automated Product Counting",
    "description": "Boost your manufacturing efficiency with AI-powered product counting! In this step-by-step tutorial, I’ll show you how to fine-tune YOLO for automated product counting on high-speed production lines.\n\nWhether in food packaging, electronics, or any fast-moving industry, this computer vision solution delivers 100% accuracy at thousands of items per minute.\n\nYou’ll learn how to:\n- Create and annotate your dataset\n- Convert annotations to YOLO format\n- Fine-tune YOLO for your specific products\n- Implement a counting logic with a virtual counting line\n- Deploy your real-time counting system\n\nResources:\n- Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine-Tune-YOLO-Production-Line-Counting.ipynb.ipynb\n- Labellerr’s official GitHub: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#YOLO #YOLOv8 #ProductCounting #AutomatedCounting #ProductionLineCounting #ManufacturingAutomation #ComputerVision #MachineLearning #DeepLearning #AIinManufacturing #IndustrialAutomation #FactoryAutomation #RealTimeDetection #ObjectDetection #YOLOTutorial #CountingSystem #VisionAI #ArtificialIntelligence #AutomationTools #Industry40",
    "video_url": "https://www.youtube.com/watch?v=fFjIkRte5R8",
    "embed_url": "https://www.youtube.com/embed/fFjIkRte5R8",
    "duration": 399,
    "view_count": 44,
    "upload_date": "20250809",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "In today's high-speed manufacturing environment, every second counts and every product matters. But what happens when traditional counting methods become your biggest bottleneck? Across industries, companies still rely on manual counting for thousands of fastmoving products every day on production lines running in high speed. This can lead to errors which will cause loss of significant amount as well as costly delays. Manual counting is slow, errorprone, and labor intensive. Averaging just 80% accuracy and consuming hours that automation could finish in minutes. Now, imagine counting with 100% accuracy at a speed of over thousands items per minute. That's the power of computer vision based product counting. Whether it's food packages or electronics, this adaptable solution can boost efficiency. In this tutorial, I'll show you exactly how to build this system. [Music] Welcome to this tutorial on fine-tuning YOLO for automated product counting. As you see our intro in this tutorial, we are going to create a computer vision algorithm using YOLO to count the product on a conveyor belt or a production pipeline. So to do that you just have to follow some steps. So let's start with our first step. Create data set and convert it into YOLO format. To create a automated product counting system, you just need to have a data set of the product you want to count. So you first create a data set and annotate it using our labellerr platform. We just go to labellerr and annotate it using bounding boxes like that and perform the steps for each the of the product instance you want to detect. So just like that you have annotated our f first image of that data set. You have to repeat this process till your data set has been annotated. Let's get to our next step. In the next part of creating automated product counting system using YOLO, you have to convert your annotation of your data set into YOLO format for fine-tuning it on YOLO. So to do that, you just have to clone this repo from our labellerr GitHub and then import this function. Using this function, you can convert your annotation.json which is in Coco JSL format and image directory into YOLO format. You just have to run this. Now your annotation.json is converted into YOLO format. Now to find tune, you just have to import YOLO from Ultralytics and provide the data set. Yiml file which is this. So this part and this data argument this will help us in fine-tuning your load. I have done this. It took me 2 hours to fine-tune my model. So my model has already been trained. So let's see its result. As you see, our fine-tuned YOLO model is detecting the product perfectly. Let's get to our next step. For the next step in our production pipeline counting system, we have to create a script where user will provide us with the line from where if the product goes, the counter will be up y1. So I created a script which help us draw a line on a frame. Let's run this. As you can see it using this I have easily drawn a line and it has stored the value of that line. Let's press okay. Now I have selected our line for our production line. Let's get to our next step. Now we have everything for creating the production line counting system. So our next step is to build a logic of the counter. So in this logic we are going to track each product and their center point of the bounding boxes. And if that center passes through the line we have hardwired in the variable counting line it will it will make our counter increase by one. So using that same logic I have created the script and when the center point of the bounding boxes detected or tracked using YOLO fine to model is passed through the line which I have drawn on the flame it will increase my counter. So let's run this. As you see the production pipeline counting system is working great. So using this simple logic you can build your own production pipeline counting system. For more computer vision or machine learning related tasks you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "In today's high-speed manufacturing environment, every second counts and every product matters. But what happens when traditional counting methods become your biggest bottleneck? Across industries, companies still rely on manual counting for thousands of fastmoving products every day on production lines running in high speed. This can lead to errors which will cause loss of significant amount as well as costly delays. Manual counting is slow, errorprone, and labor intensive. Averaging just 80% accuracy and consuming hours that automation could finish in minutes. Now, imagine counting with 100% accuracy at a speed of over thousands items per minute. That's the power of computer vision based product counting. Whether it's food packages or electronics, this adaptable solution can boost efficiency. In this tutorial, I'll show you exactly how to build this system. [Music] Welcome to this tutorial on fine-tuning YOLO for automated product counting. As you see our intro in this tutorial, we are going to create a computer vision algorithm using YOLO to count the product on a conveyor belt or a production pipeline. So to do that you just have to follow some steps. So let's start with our first step. Create data set and convert it into YOLO format. To create a automated product counting system, you just need to have a data set of the product you want to count. So you first create a data set and annotate it using our labellerr platform. We just go to labellerr and annotate it using bounding boxes like that and perform the steps for each the of the product instance you want to detect. So just like that you have annotated our f first image of that data set. You have to repeat this process till your data set has been annotated. Let's get to our next step. In",
      "the next part of creating automated product counting system using YOLO, you have to convert your annotation of your data set into YOLO format for fine-tuning it on YOLO. So to do that, you just have to clone this repo from our labellerr GitHub and then import this function. Using this function, you can convert your annotation.json which is in Coco JSL format and image directory into YOLO format. You just have to run this. Now your annotation.json is converted into YOLO format. Now to find tune, you just have to import YOLO from Ultralytics and provide the data set. Yiml file which is this. So this part and this data argument this will help us in fine-tuning your load. I have done this. It took me 2 hours to fine-tune my model. So my model has already been trained. So let's see its result. As you see, our fine-tuned YOLO model is detecting the product perfectly. Let's get to our next step. For the next step in our production pipeline counting system, we have to create a script where user will provide us with the line from where if the product goes, the counter will be up y1. So I created a script which help us draw a line on a frame. Let's run this. As you can see it using this I have easily drawn a line and it has stored the value of that line. Let's press okay. Now I have selected our line for our production line. Let's get to our next step. Now we have everything for creating the production line counting system. So our next step is to build a logic of the counter. So in this logic we are going to track each product and their center point of the bounding boxes. And",
      "if that center passes through the line we have hardwired in the variable counting line it will it will make our counter increase by one. So using that same logic I have created the script and when the center point of the bounding boxes detected or tracked using YOLO fine to model is passed through the line which I have drawn on the flame it will increase my counter. So let's run this. As you see the production pipeline counting system is working great. So using this simple logic you can build your own production pipeline counting system. For more computer vision or machine learning related tasks you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 720,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "NIGLTumGF5o",
    "title": "Fine-Tune Yolo for Football analytics",
    "description": "🚀 Unlock the Power of Real-Time Football Analytics with YOLO and Heatmaps!\nIn this hands-on computer vision tutorial, you'll learn how to track football players in real-time and generate player heatmaps using YOLO. From dataset creation and annotation to training your custom YOLO model and visualizing trajectory and heat zones — this video has everything you need to build your own football tracking system.\n\n📌 What You'll Learn:\n- Fine-tuning YOLO for football\n- Real-time player tracking\n- Heatmap generation for high-intensity zones\n- Trajectory tracking of players\n- Dataset creation and COCO to YOLO conversion\n- Live demonstration using football match footage\n\n💡 Whether you're a data scientist, ML engineer, or sports analyst, this project gives you the full roadmap to develop sports analytics systems using Python, OpenCV, and Ultralytics YOLO.\n\nResources:\n- Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine_tune_yolo_for_football_analytics.ipynb\n- Labellerr’s official GitHub: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\nChapters:\n[0:00] Introduction to Football Player Tracking Challenges\n[0:17] Proposing Computer Vision Solution with YOLO\n[0:56] Tutorial Overview: Building a Real-Time Player Tracking System\n[1:19] Step 1: Creating and Annotating the Dataset\n[2:48] Step 2: Converting COCO JSON to YOLO Format\n[2:58] Step 3: Training the YOLO Model\n[4:22] Step 4: Tracking Players on the Field\n[6:47] Step 5: Tracking Player Trajectory\n[8:29] Step 6: Generating Player Heat Maps\n[9:43] Conclusion and Further Resources\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=NIGLTumGF5o",
    "embed_url": "https://www.youtube.com/embed/NIGLTumGF5o",
    "duration": 603,
    "view_count": 95,
    "upload_date": "20250806",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "data annotation tech",
      "ai training",
      "image labeling",
      "video annotation tool",
      "image labeling tool",
      "ai training jobs remote",
      "image processing",
      "dataannotation",
      "image annotation",
      "training data",
      "dataannotation.tech starter assessment",
      "dataannotation.tech side hustle",
      "image segmentation",
      "ai",
      "dataannotation.tech",
      "object detection",
      "video annotation",
      "video labeling",
      "data science"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Imagine unlocking every secret movement on the pitch, knowing exactly where the game is won or lost. That's the power of real-time player tracking and heat maps. Right now, leading sports betting companies rely on this data to set sharper odds, power live bets, and monitor key performance indicators. Things like total distance covered, high intensity zone, and dominant possession zones. But it's not just about numbers. These insights turn into real profit. By knowing which areas of the field see the most action, betting firms minimize risk, increase wager volume, and deliver faster updates to eager fans. In today's tutorial, you'll learn how to build your own computer vision tracking system from scratch. You'll capture player positions live and also generate heat maps of high intensity zone on ground. Hello everyone, welcome to this tutorial on fine-tuning YOLO for football player tracking and player heat map generator. In this tutorial, you will learn how to track players in real time and even generate a player heat map which tell us about where the player has spent most of their time on the field. To to do this you have to just follow few steps. So first step you have to create a data set. You can use live football matches footage for your data set and then you have to annotate those images on the classes you want to detect. Like here I am using five classes team A, team B, ball, referee and goalkeeper. Then you have to select each one by one and annotate the After the annotation is completed, you just have to keep do repeating this process till every images of your data set has been annotated uh perfectly. Then you can export the annotation in JSON format using labellerr platform. Let's get to our next step. In the next part of our process, you just have to convert your cooco JSON annotation in YOLO format. To do this, you have to clone this repo which I have already clone. As you can see this using this repo you can easily uh convert your coco JSON to uh YOLO format just providing your annotation JSON for part your data set part and the output directory part you want to store your E your format then let's get to our next step. For our next step, you have to train the YOLO model. To train the YOLO model, you have to first import Ultralytics. Then from Ultralytics, import YOLO and even import OpenCV and MATL. Then you have to provide the yolo your yolo format this file location data set yiml to this command line. This command line will train your data set for around 200 epochs. So do this. I just provide the data set part data set part to this uh variable data set part and that data set part is with data set yimml part is provided to data equals to using this you can train your yolo model. I have already trained my yulo model. It took me around two hours to complete and it store my uh model in runs run folder. So let's get to our next step. So be our next step is tracking player on the field. So to track their on the field first we have to load our custom model which we have trained on our custom data set. So this is the location of our custom model called last.pt to the model. So let's run this. Now let's see our tracking on a specific frame. I'm using an example 38 frame to check how our tracking perform. It's a it's simple to do. You just have to provide the result model dot track and the source of a video and then process equals to and stream equals true. And that results gave us these value boxes track ID and class ids. So let's get to our result. As you can see these are the tracking happening. So this is the result it's showing. You can see we are getting a clear tracking. So let's see this in our video. I'm providing a sample video. Our tracking is happening right now. As you see, our tracking was successfully executed and the result was quite good. Let's get to our next step. In our next step, we will be tracking the trajectory of the player. So what is tracking the trajectory here? Trajectory means the previous part the player has uh follow. So it will give us a general idea on which part the player is running. So to do this we just have to create a dictionary. In that dictionary we have to create the value as a list which will store the central points of each bounding boxes of the player we are tracking. This will help us to remember each previous uh part the player was on. So to do this we just have to improvise our previous function just by adding a trajectory and then every time we detect we will append the tra and that central point for that particular player ID to the trajectory dictionary and then we will just display it on the screen. So yeah, here let's run this. As you see our trajectory tracking was quite successful and it will perform a great in every scenario. So let's get to our next step. In our next step, we have to generate the heat map of the player on the ground. This heat map help us find the key zone on the ground where the most important parts of match has been played. So to do this with this I have prepared a dictionary for each classes and stored their location throughout the video on that value with their class ID. So we just have to improvise our previous step and then there is our dictionary and then the the key key is our player ID and this is the bonding boxes I have to save. So and increase that by one. So let's run this code. Our heat map will look something like this which tell us about the key zones like this. This and these are the key zones on the ground where most of the player were there throughout the game. So with this you will you have learned how you can track layer and also a great heat map. For more computer vision or machine learning related tasks you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Imagine unlocking every secret movement on the pitch, knowing exactly where the game is won or lost. That's the power of real-time player tracking and heat maps. Right now, leading sports betting companies rely on this data to set sharper odds, power live bets, and monitor key performance indicators. Things like total distance covered, high intensity zone, and dominant possession zones. But it's not just about numbers. These insights turn into real profit. By knowing which areas of the field see the most action, betting firms minimize risk, increase wager volume, and deliver faster updates to eager fans. In today's tutorial, you'll learn how to build your own computer vision tracking system from scratch. You'll capture player positions live and also generate heat maps of high intensity zone on ground. Hello everyone, welcome to this tutorial on fine-tuning YOLO for football player tracking and player heat map generator. In this tutorial, you will learn how to track players in real time and even generate a player heat map which tell us about where the player has spent most of their time on the field. To to do this you have to just follow few steps. So first step you have to create a data set. You can use live football matches footage for your data set and then you have to annotate those images on the classes you want to detect. Like here I am using five classes team A, team B, ball, referee and goalkeeper. Then you have to select each one by one and annotate the After the annotation is completed, you just have to keep do repeating this process till every images of your data set has been annotated uh perfectly. Then you can export the annotation in JSON format using labellerr platform. Let's get to our",
      "next step. In the next part of our process, you just have to convert your cooco JSON annotation in YOLO format. To do this, you have to clone this repo which I have already clone. As you can see this using this repo you can easily uh convert your coco JSON to uh YOLO format just providing your annotation JSON for part your data set part and the output directory part you want to store your E your format then let's get to our next step. For our next step, you have to train the YOLO model. To train the YOLO model, you have to first import Ultralytics. Then from Ultralytics, import YOLO and even import OpenCV and MATL. Then you have to provide the yolo your yolo format this file location data set yiml to this command line. This command line will train your data set for around 200 epochs. So do this. I just provide the data set part data set part to this uh variable data set part and that data set part is with data set yimml part is provided to data equals to using this you can train your yolo model. I have already trained my yulo model. It took me around two hours to complete and it store my uh model in runs run folder. So let's get to our next step. So be our next step is tracking player on the field. So to track their on the field first we have to load our custom model which we have trained on our custom data set. So this is the location of our custom model called last.pt to the model. So let's run this. Now let's see our tracking on a specific frame. I'm using an example 38 frame to check how our",
      "tracking perform. It's a it's simple to do. You just have to provide the result model dot track and the source of a video and then process equals to and stream equals true. And that results gave us these value boxes track ID and class ids. So let's get to our result. As you can see these are the tracking happening. So this is the result it's showing. You can see we are getting a clear tracking. So let's see this in our video. I'm providing a sample video. Our tracking is happening right now. As you see, our tracking was successfully executed and the result was quite good. Let's get to our next step. In our next step, we will be tracking the trajectory of the player. So what is tracking the trajectory here? Trajectory means the previous part the player has uh follow. So it will give us a general idea on which part the player is running. So to do this we just have to create a dictionary. In that dictionary we have to create the value as a list which will store the central points of each bounding boxes of the player we are tracking. This will help us to remember each previous uh part the player was on. So to do this we just have to improvise our previous function just by adding a trajectory and then every time we detect we will append the tra and that central point for that particular player ID to the trajectory dictionary and then we will just display it on the screen. So yeah, here let's run this. As you see our trajectory tracking was quite successful and it will perform a great in every scenario. So let's get to our next step. In our next step, we",
      "have to generate the heat map of the player on the ground. This heat map help us find the key zone on the ground where the most important parts of match has been played. So to do this with this I have prepared a dictionary for each classes and stored their location throughout the video on that value with their class ID. So we just have to improvise our previous step and then there is our dictionary and then the the key key is our player ID and this is the bonding boxes I have to save. So and increase that by one. So let's run this code. Our heat map will look something like this which tell us about the key zones like this. This and these are the key zones on the ground where most of the player were there throughout the game. So with this you will you have learned how you can track layer and also a great heat map. For more computer vision or machine learning related tasks you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 1086,
    "transcript_chunk_count": 4
  },
  {
    "video_id": "kwQeokYDVcE",
    "title": "Build a Real-Time Intrusion Detection System with YOLO",
    "description": "In this hands-on tutorial, you'll learn how to build a real-time intrusion detection system using YOLO and computer vision. Whether it's for home security or industrial zones, this guide walks you through detecting unauthorized access before trespassers even enter your premises.\n\n✅ What You’ll Learn:\n- How to fine-tune YOLO for intrusion detection\n- Drawing and configuring restricted zones\n- Detecting and tracking people with bounding boxes\n- Triggering real-time alerts when a person crosses the line\n\nBy the end of this video, you'll have a fully functional AI-powered detection system using just a camera and some Python code.\n\n🔗 Resources:\n- Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Intrusion_detection_using_YOLO.ipynb\n- Labellerr’s Official GitHub: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\nChapters:\n[0:00] Introduction to Intrusion Detection Challenges\n[0:11] Proposing Computer Vision Solution with YOLO\n[0:35] Tutorial Overview: Building a Real-Time Intrusion Detection System\n[1:49] Step 1: Setting Up the Environment\n[2:43] Importing Required Libraries\n[3:08] Step 2: Creating the Restricted Zone Function\n[4:32] Capturing and Processing Video Frames\n[5:09] Defining the Restricted Zone with User Input\n[6:31] Step 3: Implementing the Polygon Selection Interface\n[7:27] Visualizing the Restricted Zone\n[8:24] Step 4: Building the Intrusion Detection Logic\n[9:45] Loading the YOLO Model for Object Detection\n[11:19] Processing Bounding Boxes and Detecting Trespassers\n[13:25] Final Function: Real-Time Intrusion Detection\n[14:37] Testing the Intrusion Detection System\n[14:55] Conclusion and Further Resources\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n#YOLOv8 #IntrusionDetection #ComputerVision #HomeSecurity #AIProjects #ObjectDetection #SurveillanceAI #PythonProjects #OpenCV #RealTimeDetection #MachineLearning #SmartSecurity #DeepLearning #UltralyticsYOLO #SecuritySystem",
    "video_url": "https://www.youtube.com/watch?v=kwQeokYDVcE",
    "embed_url": "https://www.youtube.com/embed/kwQeokYDVcE",
    "duration": 904,
    "view_count": 301,
    "upload_date": "20250804",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Every year, businesses and homes lose millions due to unauthorized access and trespassing. Traditional security systems often react after the damage is done. But what if we could detect intrusions in real time automatically before trespassers entry are property? That's where computer vision is used for detecting intruder. In this tutorial, I'll show you how to build your own realtime intrusion detection system using YOLO. From detecting people entering restricted zones to triggering alerts when boundaries are crossed, by the end of this video, you'll have a working system that can secure any environment using just a camera. [Music] So hello guys, welcome to this tutorial of using YOLO for early intrusion detection system. So as you have seen our intro in this tutorial we are going to use YOLO to create a system where you can detect any intruder in a specific zone early before even it entered uh your uh premises like for example I'm going to use this video in where some people are entering a a house or you can say in this tutorial a parimeter. So our goal in this tutorial is to create a line uh zone where if someone uh someone or this uh this object which we call people here uh enters this zone, it will be detected by our camera and a and a text called trespasser detected will be shown in this video. So let's get to it. So uh before uh creating the system for early intrusion discussion we have to install some libraries. So these libraries are alphalytics using which we will load our model YOLO open CV to perform camera related task or numpy in which you will define these zones using uh points and safely. This uh particular library help us in creating a polygon zone in the frame. So let's run this. I have already installed the libraries. So let's move to next part. Now let's import these library. As you can see I imported computer Open CV, NumPy, Ultralytics from Ultralytics, YOLO, Mattplot Live. Uh and safely you have to import point and polygon. Let's uh our libraries has been imported. Now let's start building intrusion detection system for real time. So to do this we just have to create two function. First function has to store the restrictive zone from user in the frame. So we have to create a function where user can manually input the zone in the frame where it doesn't want any object to be and if that object enters that zone it will display that some objects have entered that zone. So to do this we have to create a function. So as you can see this function select polygon takes a parameter video part. This video part is a example. You can give an example video for other real for web based interface or a sample video to be to test your model on. So let's see as you can see I have t captured the capturing this video using its part in tap and then taking a specific frame. As you can see, I have using the 38th frame of that video because I want the first second of that video because initially the video is not clear. After some time it will get clear. So I am using the first second of that video. Then here you can see I am providing capturing the frame that frame in frame. Now in this function you can see I'm pro writing a script to take points from user in which it want that no object could enter or to specify the restrictive zone. So it's very simple I have the defines a variable called display frame frame and points and instruction. These are my instruction like uh click to add polygon vertices, press D to delete last point, press C to clear all points and enter to when finished. So here I have taken a list in which all the points should be stored and here it will use to display that point on the screen. So then in this function I call back the mouse uh call back the mouse when clicked. So whenever an event happen you can see and when an event happen it will note that point where it was clicked and update the display as you can see cv2 and set windows name. It will update the frame that where there has been a point click and it will show on that frame and it will keep repeating until our all the logics are true and then after I press enter here you can see the enter key is 13 in our system or uh you can say 10 also so it will break out of the loop and release the our windows. So let's run this. Now I am providing this uh function with video part and output part. And let's run this. Now as you can see a screen has been created where you can input that point. So let's input that our point. This is one. This is two. This is three. This is four. This is five. Here it's six. Here it's seven. Here it's Here it's eight. Here it's nine. Here it's again 10. And then you should press enter. Now as you can see our points has been registered in the list. To check our restrictive zone we just have to provide that points and show that points on the frame we have chosen. So here I have given a 30th frame. I am showing that points using Matt plot or that on that frame. So this is the point I have taken. I have named it area order plane. You can also call it restrictive. This so restrictive zone on video pen. So here you can see this zone is our restrictive zone. If an object goes inside this zone, it will detected as the object has trespassed. Now let's get to our main function. So in the next step of our intrusion detection, we have to create a function to detect if an object is inside a receptive zone or not. To create this logic we have to first get to our this points. So as you can see this point represent our restrictive zones. Uh let's see I have created a polygon using saply uh of this restrictive zone uh of the polygon. So let's copy this. Let's see what is it is representing. So as you can see a polygon is created using the point I have inputed in that frame. So we have just make sure if an object uh lowest part is is inside that point which belongs to restrictive zone is yes or not. To create that logic first we have to import our yolo model for object detection and then in yolo model I am using bonding boxes. So the our logic will be simple. If the middle part of the low lowest line of bonding boxes is inside that restrictive zone, it means the object is in the restrictive zone. So to do that I just have to take the video part or you can put input here zero for real web time interface into cap and then here it is for saving that video. So and but it also here this variable store if there is a detection or tr trespassing or not. So while running the cap it stored every frame and then it's also drawing the restrictive zone on that frame and here the yolo inference inferencing happen. So I have provided the model YOLO model with frame and classes. Here is the trespass class ID means here it trespass ID I have been defined here is zero zero is for people and also provided the minimum confidence. So I have taken 50% confidence here. So let's see. So this will just detect our object. It will not provide that if that object is in our destructive. For do that I have to go inside that result. So for for results of the model I have to take its bounding boxes dot dot boxes why we did bonding boxes and here I have taken each point of the bonding boxes x1 y1 x2 and y2. So to find the feet x and feet y feet x and feet y are the middle point of the lowest line of the bonding box. So what I have to just add uh add the x values of both point and floor division by two. So after that our feet x and feet y has been uh has been finded. So I have you using that point using safely point and providing uh providing to trespass polygon dot contains. So it's a very simple method if that points uh contained in that trespass trespass polygon it will show its true uh current trespass true and trespass detector true. So it will also show our uh object in red bond boxes. Here RGB I have provided RG BGR where where R is 255 and these are the coordinates of the bonding box which I get using YOLO model. So and the frame and put the text on that frame trespasser on that bounding boxes. So and if the trespassor is not if that object is not inside that bonding boxes I just have to draw it in a green color that there is a person detected in the frame but it is not inside the restrictive zone. So it is not a trespasser. So, so with this simple logic we have created a system where you can detect if a trespasser is or is not if a object a person is in the uh restrictive zone or not. So let run that function our function. So let's process the video. Let's see the result of our uh model and similarly you can test our logic in any video. For example, I'm going to use this as an example to test if our logic succeed in detecting the detecting these people is inside our restrictive zone or not. Let's see that. As you seen our logic worked flawlessly in this video. Now you understand how you can build a system where you can detect if an object is inside a specific zone or not. with few lines of code. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Every year, businesses and homes lose millions due to unauthorized access and trespassing. Traditional security systems often react after the damage is done. But what if we could detect intrusions in real time automatically before trespassers entry are property? That's where computer vision is used for detecting intruder. In this tutorial, I'll show you how to build your own realtime intrusion detection system using YOLO. From detecting people entering restricted zones to triggering alerts when boundaries are crossed, by the end of this video, you'll have a working system that can secure any environment using just a camera. [Music] So hello guys, welcome to this tutorial of using YOLO for early intrusion detection system. So as you have seen our intro in this tutorial we are going to use YOLO to create a system where you can detect any intruder in a specific zone early before even it entered uh your uh premises like for example I'm going to use this video in where some people are entering a a house or you can say in this tutorial a parimeter. So our goal in this tutorial is to create a line uh zone where if someone uh someone or this uh this object which we call people here uh enters this zone, it will be detected by our camera and a and a text called trespasser detected will be shown in this video. So let's get to it. So uh before uh creating the system for early intrusion discussion we have to install some libraries. So these libraries are alphalytics using which we will load our model YOLO open CV to perform camera related task or numpy in which you will define these zones using uh points and safely. This uh particular library help us in creating a polygon zone",
      "in the frame. So let's run this. I have already installed the libraries. So let's move to next part. Now let's import these library. As you can see I imported computer Open CV, NumPy, Ultralytics from Ultralytics, YOLO, Mattplot Live. Uh and safely you have to import point and polygon. Let's uh our libraries has been imported. Now let's start building intrusion detection system for real time. So to do this we just have to create two function. First function has to store the restrictive zone from user in the frame. So we have to create a function where user can manually input the zone in the frame where it doesn't want any object to be and if that object enters that zone it will display that some objects have entered that zone. So to do this we have to create a function. So as you can see this function select polygon takes a parameter video part. This video part is a example. You can give an example video for other real for web based interface or a sample video to be to test your model on. So let's see as you can see I have t captured the capturing this video using its part in tap and then taking a specific frame. As you can see, I have using the 38th frame of that video because I want the first second of that video because initially the video is not clear. After some time it will get clear. So I am using the first second of that video. Then here you can see I am providing capturing the frame that frame in frame. Now in this function you can see I'm pro writing a script to take points from user in which it want that no object could enter",
      "or to specify the restrictive zone. So it's very simple I have the defines a variable called display frame frame and points and instruction. These are my instruction like uh click to add polygon vertices, press D to delete last point, press C to clear all points and enter to when finished. So here I have taken a list in which all the points should be stored and here it will use to display that point on the screen. So then in this function I call back the mouse uh call back the mouse when clicked. So whenever an event happen you can see and when an event happen it will note that point where it was clicked and update the display as you can see cv2 and set windows name. It will update the frame that where there has been a point click and it will show on that frame and it will keep repeating until our all the logics are true and then after I press enter here you can see the enter key is 13 in our system or uh you can say 10 also so it will break out of the loop and release the our windows. So let's run this. Now I am providing this uh function with video part and output part. And let's run this. Now as you can see a screen has been created where you can input that point. So let's input that our point. This is one. This is two. This is three. This is four. This is five. Here it's six. Here it's seven. Here it's Here it's eight. Here it's nine. Here it's again 10. And then you should press enter. Now as you can see our points has been registered in the list. To check our restrictive",
      "zone we just have to provide that points and show that points on the frame we have chosen. So here I have given a 30th frame. I am showing that points using Matt plot or that on that frame. So this is the point I have taken. I have named it area order plane. You can also call it restrictive. This so restrictive zone on video pen. So here you can see this zone is our restrictive zone. If an object goes inside this zone, it will detected as the object has trespassed. Now let's get to our main function. So in the next step of our intrusion detection, we have to create a function to detect if an object is inside a receptive zone or not. To create this logic we have to first get to our this points. So as you can see this point represent our restrictive zones. Uh let's see I have created a polygon using saply uh of this restrictive zone uh of the polygon. So let's copy this. Let's see what is it is representing. So as you can see a polygon is created using the point I have inputed in that frame. So we have just make sure if an object uh lowest part is is inside that point which belongs to restrictive zone is yes or not. To create that logic first we have to import our yolo model for object detection and then in yolo model I am using bonding boxes. So the our logic will be simple. If the middle part of the low lowest line of bonding boxes is inside that restrictive zone, it means the object is in the restrictive zone. So to do that I just have to take the video part or you can put input",
      "here zero for real web time interface into cap and then here it is for saving that video. So and but it also here this variable store if there is a detection or tr trespassing or not. So while running the cap it stored every frame and then it's also drawing the restrictive zone on that frame and here the yolo inference inferencing happen. So I have provided the model YOLO model with frame and classes. Here is the trespass class ID means here it trespass ID I have been defined here is zero zero is for people and also provided the minimum confidence. So I have taken 50% confidence here. So let's see. So this will just detect our object. It will not provide that if that object is in our destructive. For do that I have to go inside that result. So for for results of the model I have to take its bounding boxes dot dot boxes why we did bonding boxes and here I have taken each point of the bonding boxes x1 y1 x2 and y2. So to find the feet x and feet y feet x and feet y are the middle point of the lowest line of the bonding box. So what I have to just add uh add the x values of both point and floor division by two. So after that our feet x and feet y has been uh has been finded. So I have you using that point using safely point and providing uh providing to trespass polygon dot contains. So it's a very simple method if that points uh contained in that trespass trespass polygon it will show its true uh current trespass true and trespass detector true. So it will also show our uh object in red",
      "bond boxes. Here RGB I have provided RG BGR where where R is 255 and these are the coordinates of the bonding box which I get using YOLO model. So and the frame and put the text on that frame trespasser on that bounding boxes. So and if the trespassor is not if that object is not inside that bonding boxes I just have to draw it in a green color that there is a person detected in the frame but it is not inside the restrictive zone. So it is not a trespasser. So, so with this simple logic we have created a system where you can detect if a trespasser is or is not if a object a person is in the uh restrictive zone or not. So let run that function our function. So let's process the video. Let's see the result of our uh model and similarly you can test our logic in any video. For example, I'm going to use this as an example to test if our logic succeed in detecting the detecting these people is inside our restrictive zone or not. Let's see that. As you seen our logic worked flawlessly in this video. Now you understand how you can build a system where you can detect if an object is inside a specific zone or not. with few lines of code. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 1751,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "3qKqttdwxnM",
    "title": "Fine-Tune YOLO for Product Recognition in Retail Checkout",
    "description": "In this tutorial, we demonstrate how to fine-tune YOLO (You Only Look Once) for accurate product recognition in automated self-checkout systems. From dataset preparation and annotation to training and real-time inference, you'll learn each step to build a smart retail vision system capable of identifying multiple products simultaneously — no barcodes needed.\n\nWe explore the full pipeline using YOLOv8, covering:\n- How to collect and label a product dataset\n- Modifying YOLO for specific retail products\n- Training and evaluation best practices\n- Real-time object detection for retail checkout\n- Estimating total cost with recognized product classes\n\nWhether you're a computer vision engineer, AI researcher, or retail automation developer, this video will guide you through the practical implementation of vision-based checkout systems using state-of-the-art object detection.\n\n🚀 By the end of this video, you'll be able to:\n- Train a YOLO model for custom product datasets\n- Deploy it for real-time product detection\n- Automate the pricing logic for checkout\n\nThis project is ideal for smart retail, cashierless stores, and AI-driven billing counters. If you're looking to modernize checkout systems with deep learning and computer vision, this is the perfect place to start.\n\nResources:\nTutorial Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine-Tune_YOLO_for_Product_Recognition_for_Price_Verification.ipynb\n\nLabellerr’s Official Github: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\n[0:00] Introduction to Checkout System Challenges\n[0:12] Proposing Computer Vision Solution with YOLO\n[1:07] Tutorial Overview: Product Recognition for Price Verification\n[2:11] Step 1: Data Creation\n[2:57] Exploring the Retail Product Checkout Dataset\n[4:13] Downloading and Preparing the Dataset\n[5:47] Step 2: Model Training with YOLO\n[9:35] Testing the Trained Model\n[13:26] Step 3: Building the Checkout System\n[14:51] Annotating Images and Counting Instances\n[18:11] Adding Total Price Calculation\n[19:45] Final Function: Complete Checkout Pipeline\n[21:55] Conclusion and Further Resources\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n\n#YOLO #ProductRecognition #RetailAI #SelfCheckout #ComputerVision #YOLOv8 #RetailAutomation #DeepLearning #AIinRetail #ObjectDetection #PythonAI #SmartCheckout #RealtimeDetection #CVProject #MachineLearning #AIProjects",
    "video_url": "https://www.youtube.com/watch?v=3qKqttdwxnM",
    "embed_url": "https://www.youtube.com/embed/3qKqttdwxnM",
    "duration": 1326,
    "view_count": 186,
    "upload_date": "20250729",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "machine learning",
      "dataannotation.tech side hustle",
      "data annotation tech",
      "dataannotation.tech legit",
      "side business ideas at home",
      "datannotation.tech review",
      "dataannotation.tech scam",
      "data annotation.tech",
      "dataannotation",
      "dataannotationtech",
      "data annotation tech review",
      "dataannotation tech",
      "dataannotation.tech review",
      "data annotation tech legit"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "In today's fast-moving retail world, where every transaction matters and customer experience defines loyalty, the checkout counter has quietly become one of our biggest bottlenecks. Manually scanning barcodes, dealing with misread items, and managing cues during peak hours all slow us down. The longer our customers wait, the more we risk losing their trust and their time. So, we asked the question, what if we could use computer vision to solve this issue? And we found yes, we can. Using fine-tuning the YOLO model with your unique product catalog, you can build a smart checkout assistant that detects multiple items in real time and even estimates the total bill without a single barcode. The result, a faster, smoother, and more intelligent checkout experience that delights customers and transforms retail operations. This is the future of checkout, and it's powered by computer vision. Ready to see how it's done? [Music] Hello guys, welcome to this tutorial on computer vision in which we are going to create a product recognition for price ver verification. So what is that? So think of it like a checkout system where multiple products by user go inside that camera frame and camera click the photo and using computer vision algorithms we find recognize the product and find the total value of the products in that frame. So it will make the checkout system more efficient across retail environment and it will also reduce the error in that uh checkout system. To create that kind of system we just have to follow three steps. So the three steps are data creation, model training and building our checkout system. So let's get to our first step. In our first step of data creation, we have to ask ourself a question what kind of retail environment system uh we are talking about. Like if it is a grocery, we need data set of various vegetables, fruits and other things. But if we are talking about a checkout system for a clotting environment, we need various clots data sets for our checkout system. So based on the use case, we need to create a data set to for this tutorial, I'm using a data set which is present on Kaggle called retail product checkout data set. Let's check this out. This is the data set I'm talking about. It's a very large data set containing over one lakh images of a a retail uh environment of a normal uh general store like let's see if you see its content uh in a data set in great detail of product various angles you can see. So, and our job is to detect that kind of object in this environment. Let me show you. So in final in our final result we should be able to detect all the product in that image and also estimate or uh to verify the total price of that product in that frame. So to in download this data set we just have to go and copy this script in our s in our uh uh script. I have already done that and downloaded that uh uh data set. It's a very large data set of around 30 GB. So, so be careful with downloading with a stable internet. So it's a for this tutorial I have used only 5,000 image to train our model as with large data set training it time consume will be very large. So for the sake of this tutorial I have simplified that process. I have I have created a function which will convert this data set of JSON structure in a YOLO format and also count the maximum images I needed for my uh model training. I just have to provide the parts to the JSON file in which all the annotations of that data set is stored and the image directory in which images of that data sets are stored and then provide the output directory in which our YOLO format will be stored. So it will make our process very much easier and this tutorial less complex. So let's get to the next step of our tutorial of model training. In our next step of this tutorial, we have to train our model using YOLO. So to train our model, we have to provide it with the data set in YOLO format. So what this data set structure it is. So you will find that that yolo format is basically a directory which contain all the images you want to train. You want to train like this this and the folder which contain all the labels for that directory. Like if that image contain a class of 140 and this are this is the bonding box there where this is x y and this is height and bread of that bonding box. So this is the structure of uh yolo format. Let's see another one where multiple instance. So, so there are only one instance as you can know there the camera is in a perfect angle on taking pictures of only one objects. So every label should have only one classes. So it also need to have a let me show a data yiml file. So in that file you have a train and val validation. I am using using all these 5,000 image for training. So I'm providing both with train for this tutorial sake. You you need to create a val folder. You can use using this by providing a split equals to two. It will create a val folder also. So in this uh data set there are around 200 classes. But for auto we will simplify it into super classes like the stationary product will we all consider as stationary even if is 195 and 194 which we will do it later. So let's get to our model training. First we just have to provide our part yolo data yolo train format and data set part is this yolo train format and then we have to import our yolo then I'm using yolo vx model for our training and providing the data equals to data yiml part and running On epox 100 you can increase the epox size based on your requirement and image size 640 and batch size 10 and save create 10 on every 10 blocks the model will be saved itself and then after after running this script you will get a run folder like this I have run this script and it took me around 8 to 9 hour for for the model to be created and it's stored in here as you can see it's contained uh train it I have run it multiple time and has two models best and lastpt pt which stores the model of yolo model for our uh data set so let's run this Now we have trained our model and uh we are now our model is in this variable. So you can perform this model on a test image. I am providing it with a directory of images as you can see. Let's see the result. you will find that these are the images on which our model is performing. So let's see I have you can also store let's see it again you will find that there are one you will here it is so you will see that in image this there is a one instance of seven per two instead of instant noodles train sense of issues and here you can say see that every model is working good and detecting objects clearly so let's take a sample image from I have already done that so let's take a sample image this is a very easy detection like there are two chips uh packet there are two can and there is one cold drink so Let's run this. So on running it's finding these are the result you will know. So you will find a 72. There are total five instance of objects as we detected and these are the bonding boxes of all the five images. So it's good and let's see what 72 represent in in our data YML you will find that 72 represent the uh drink. So our model has represented this is drink as you can see only 172 is there and two 108 108 must be a chips or something like that 1080. So it's talking about this and what else are there a four. So what four can be puffed food which consider this as a puff food. So let's see now let's create uh you can also let me see you can just provide it with a save= to true and and run it. Yes, it will save our result and runs as you can see. Yes, there it is. So, as you can see, it is finding a drink uh puff food and uh two puff food and there are two canned food. So, it's working fine. So let's get to our next step in creating a full checkout system where you just have to provide the image and the total price will be calculated based on the product in that image. Let's get to our final step in creating a checkout system using computer vision. For this tutorial, I am simplifying our classes like I have over two 200 classes. So based on that, I am creating super classes like puff different puff food belongs to a same puff food classes, different dry foods uh belongs to same dry food classes and the same for everyone else. it will reduce my classes and complexity of this tutorial. So this is just a function in which based on index I have created uh which index belongs to which classes like 0 to uh to 10 11 sorry 11 belongs to the class of puff food and similarly for milk and everyone everything else. After running this, you will find a reverse dictionary where each uh per key value belongs to key is shown like so if uh index is a key of 11 belongs to puffford it will make our model more easy. So let's get to our first step in which we have to provide create a function to provide uh annotation on the image and count the super uh count the instance of classes in our image. So I have created a function where you have to just provide the image and the result from the model dulo model and it will provide us with the annotated image and a dictionary which contain the instances as a value and the key as the class in which it belongs. So here I just use the annotated image. I copy that image as an aggregator image and from that result boxes confident and classes are taken and here you can see I provide using this. Yeah, you can see I have used unique color for each superasses. So, so you have to see that this is the most important part where from result I have taken out boxes uh and that from that boxes I have taken out the bonding boxes data confidence and class ID. So these are the most important part of this function and based on this I have annotated that using this as you can see I use this to annotate our image based on bonding boxes it provided you can see it here so let's run this function now let's see how our function perform I'm using an example image same as previously I'm providing it with the So let's see our result image. As you can see now all the puff food belongs to puff food criteria and all the drinks belongs to drink criteria and this is a canned food. It also provide me with the let me see where is count. So there is one instance of drain, two instance of can food code, two instance of puff code. So our first step in creating check out system is created. We just have to automate this step. So we just create a pipeline in which we go to image provided with image part with and also with the model and provide this function in that uh in that in this function and then run this. So let's see it has provided me with our dictionary of the count instances and image in which our product is shown. So let's get to our next step of adding total price counting feature. So this is a superass price sheet I have created where I have used the class name as a key and the price as its value. It's a dummy price so you can change it on your own but for this uh tutorial I have kept it simple on based on the factors of 10 and five. So let's run this now. So we in this function I have to calculate and print total prices based on detection counts and superclass prices. So I have just to provide a superclass prices and nothing more I will add I will add that value times the uh the count is present in that dictionary. So if object is present two times the the value of this object is added two times to the total count. It's a simple function. So now let's see our function. You will see in that image our function have rank one 1 into 50 2 into 30 and 2 into 100. So total price is 310. So now is our time to create the final function in which you just have to provide the image and then our model will perform. So so this is I just added the pipeline and that pipeline result to calculate total price and then print out the total price of our frame. So let's perform this. As you can see it's perform good and now it also print out the total price and price of that product in that im in that frame. So here is our total price for that our frame. So let's take another complicated image. like this. As you can see, this image contain too much. Let's reduce this image contains various products which you can't count on your on your own. So, let's run our function. Let's see it result. It has detected uh instant drink two, personal hygiene two, tissue two, instant noodles, three, milk one, puff food, two, dried fruit two, drink two, and chocolate one, dried fruit two. As you can see, it has marked each and every object perfectly. So and also provided us with the total value of our product here. So with this you have created a checkout system based on computer vision. It will make the checkout system more efficient and faster in a crowded retail environment. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "In today's fast-moving retail world, where every transaction matters and customer experience defines loyalty, the checkout counter has quietly become one of our biggest bottlenecks. Manually scanning barcodes, dealing with misread items, and managing cues during peak hours all slow us down. The longer our customers wait, the more we risk losing their trust and their time. So, we asked the question, what if we could use computer vision to solve this issue? And we found yes, we can. Using fine-tuning the YOLO model with your unique product catalog, you can build a smart checkout assistant that detects multiple items in real time and even estimates the total bill without a single barcode. The result, a faster, smoother, and more intelligent checkout experience that delights customers and transforms retail operations. This is the future of checkout, and it's powered by computer vision. Ready to see how it's done? [Music] Hello guys, welcome to this tutorial on computer vision in which we are going to create a product recognition for price ver verification. So what is that? So think of it like a checkout system where multiple products by user go inside that camera frame and camera click the photo and using computer vision algorithms we find recognize the product and find the total value of the products in that frame. So it will make the checkout system more efficient across retail environment and it will also reduce the error in that uh checkout system. To create that kind of system we just have to follow three steps. So the three steps are data creation, model training and building our checkout system. So let's get to our first step. In our first step of data creation, we have to ask ourself a question what kind of retail environment system uh we",
      "are talking about. Like if it is a grocery, we need data set of various vegetables, fruits and other things. But if we are talking about a checkout system for a clotting environment, we need various clots data sets for our checkout system. So based on the use case, we need to create a data set to for this tutorial, I'm using a data set which is present on Kaggle called retail product checkout data set. Let's check this out. This is the data set I'm talking about. It's a very large data set containing over one lakh images of a a retail uh environment of a normal uh general store like let's see if you see its content uh in a data set in great detail of product various angles you can see. So, and our job is to detect that kind of object in this environment. Let me show you. So in final in our final result we should be able to detect all the product in that image and also estimate or uh to verify the total price of that product in that frame. So to in download this data set we just have to go and copy this script in our s in our uh uh script. I have already done that and downloaded that uh uh data set. It's a very large data set of around 30 GB. So, so be careful with downloading with a stable internet. So it's a for this tutorial I have used only 5,000 image to train our model as with large data set training it time consume will be very large. So for the sake of this tutorial I have simplified that process. I have I have created a function which will convert this data set of JSON structure",
      "in a YOLO format and also count the maximum images I needed for my uh model training. I just have to provide the parts to the JSON file in which all the annotations of that data set is stored and the image directory in which images of that data sets are stored and then provide the output directory in which our YOLO format will be stored. So it will make our process very much easier and this tutorial less complex. So let's get to the next step of our tutorial of model training. In our next step of this tutorial, we have to train our model using YOLO. So to train our model, we have to provide it with the data set in YOLO format. So what this data set structure it is. So you will find that that yolo format is basically a directory which contain all the images you want to train. You want to train like this this and the folder which contain all the labels for that directory. Like if that image contain a class of 140 and this are this is the bonding box there where this is x y and this is height and bread of that bonding box. So this is the structure of uh yolo format. Let's see another one where multiple instance. So, so there are only one instance as you can know there the camera is in a perfect angle on taking pictures of only one objects. So every label should have only one classes. So it also need to have a let me show a data yiml file. So in that file you have a train and val validation. I am using using all these 5,000 image for training. So I'm providing both with train for this tutorial sake.",
      "You you need to create a val folder. You can use using this by providing a split equals to two. It will create a val folder also. So in this uh data set there are around 200 classes. But for auto we will simplify it into super classes like the stationary product will we all consider as stationary even if is 195 and 194 which we will do it later. So let's get to our model training. First we just have to provide our part yolo data yolo train format and data set part is this yolo train format and then we have to import our yolo then I'm using yolo vx model for our training and providing the data equals to data yiml part and running On epox 100 you can increase the epox size based on your requirement and image size 640 and batch size 10 and save create 10 on every 10 blocks the model will be saved itself and then after after running this script you will get a run folder like this I have run this script and it took me around 8 to 9 hour for for the model to be created and it's stored in here as you can see it's contained uh train it I have run it multiple time and has two models best and lastpt pt which stores the model of yolo model for our uh data set so let's run this Now we have trained our model and uh we are now our model is in this variable. So you can perform this model on a test image. I am providing it with a directory of images as you can see. Let's see the result. you will find that these are the images on which our model is performing.",
      "So let's see I have you can also store let's see it again you will find that there are one you will here it is so you will see that in image this there is a one instance of seven per two instead of instant noodles train sense of issues and here you can say see that every model is working good and detecting objects clearly so let's take a sample image from I have already done that so let's take a sample image this is a very easy detection like there are two chips uh packet there are two can and there is one cold drink so Let's run this. So on running it's finding these are the result you will know. So you will find a 72. There are total five instance of objects as we detected and these are the bonding boxes of all the five images. So it's good and let's see what 72 represent in in our data YML you will find that 72 represent the uh drink. So our model has represented this is drink as you can see only 172 is there and two 108 108 must be a chips or something like that 1080. So it's talking about this and what else are there a four. So what four can be puffed food which consider this as a puff food. So let's see now let's create uh you can also let me see you can just provide it with a save= to true and and run it. Yes, it will save our result and runs as you can see. Yes, there it is. So, as you can see, it is finding a drink uh puff food and uh two puff food and there are two canned food. So, it's working fine. So",
      "let's get to our next step in creating a full checkout system where you just have to provide the image and the total price will be calculated based on the product in that image. Let's get to our final step in creating a checkout system using computer vision. For this tutorial, I am simplifying our classes like I have over two 200 classes. So based on that, I am creating super classes like puff different puff food belongs to a same puff food classes, different dry foods uh belongs to same dry food classes and the same for everyone else. it will reduce my classes and complexity of this tutorial. So this is just a function in which based on index I have created uh which index belongs to which classes like 0 to uh to 10 11 sorry 11 belongs to the class of puff food and similarly for milk and everyone everything else. After running this, you will find a reverse dictionary where each uh per key value belongs to key is shown like so if uh index is a key of 11 belongs to puffford it will make our model more easy. So let's get to our first step in which we have to provide create a function to provide uh annotation on the image and count the super uh count the instance of classes in our image. So I have created a function where you have to just provide the image and the result from the model dulo model and it will provide us with the annotated image and a dictionary which contain the instances as a value and the key as the class in which it belongs. So here I just use the annotated image. I copy that image as an aggregator image and from",
      "that result boxes confident and classes are taken and here you can see I provide using this. Yeah, you can see I have used unique color for each superasses. So, so you have to see that this is the most important part where from result I have taken out boxes uh and that from that boxes I have taken out the bonding boxes data confidence and class ID. So these are the most important part of this function and based on this I have annotated that using this as you can see I use this to annotate our image based on bonding boxes it provided you can see it here so let's run this function now let's see how our function perform I'm using an example image same as previously I'm providing it with the So let's see our result image. As you can see now all the puff food belongs to puff food criteria and all the drinks belongs to drink criteria and this is a canned food. It also provide me with the let me see where is count. So there is one instance of drain, two instance of can food code, two instance of puff code. So our first step in creating check out system is created. We just have to automate this step. So we just create a pipeline in which we go to image provided with image part with and also with the model and provide this function in that uh in that in this function and then run this. So let's see it has provided me with our dictionary of the count instances and image in which our product is shown. So let's get to our next step of adding total price counting feature. So this is a superass price sheet I have created",
      "where I have used the class name as a key and the price as its value. It's a dummy price so you can change it on your own but for this uh tutorial I have kept it simple on based on the factors of 10 and five. So let's run this now. So we in this function I have to calculate and print total prices based on detection counts and superclass prices. So I have just to provide a superclass prices and nothing more I will add I will add that value times the uh the count is present in that dictionary. So if object is present two times the the value of this object is added two times to the total count. It's a simple function. So now let's see our function. You will see in that image our function have rank one 1 into 50 2 into 30 and 2 into 100. So total price is 310. So now is our time to create the final function in which you just have to provide the image and then our model will perform. So so this is I just added the pipeline and that pipeline result to calculate total price and then print out the total price of our frame. So let's perform this. As you can see it's perform good and now it also print out the total price and price of that product in that im in that frame. So here is our total price for that our frame. So let's take another complicated image. like this. As you can see, this image contain too much. Let's reduce this image contains various products which you can't count on your on your own. So, let's run our function. Let's see it result. It has detected uh instant drink",
      "two, personal hygiene two, tissue two, instant noodles, three, milk one, puff food, two, dried fruit two, drink two, and chocolate one, dried fruit two. As you can see, it has marked each and every object perfectly. So and also provided us with the total value of our product here. So with this you have created a checkout system based on computer vision. It will make the checkout system more efficient and faster in a crowded retail environment. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 2500,
    "transcript_chunk_count": 9
  },
  {
    "video_id": "Mzk8ynEibGs",
    "title": "Fine-Tune YOLO for Emergency Vehicle Detection | CV in Vehicles & Traffic Monitoring",
    "description": "Want to build a smart traffic system that detects ambulances in real-time? In this hands-on tutorial, learn how to fine-tune YOLO for emergency vehicle detection using segmentation.\n\nWe walk you through dataset collection, annotation, YOLO format conversion, model training, and real-world inference on video data. This project is perfect for AI enthusiasts, traffic system developers, and anyone exploring smart city solutions with computer vision.\n\n🚑 Detect ambulances with high accuracy\n📦 Complete YOLO fine-tuning workflow\n🔧 Real-world use case for autonomous systems\n📽️ Inference demo included\n\nResources:\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/fine-tune-yolo-for-Emergency-vehicle-detection.ipynb\n\nLabellerr’s Official GitHub: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\n[0:00] Introduction to Emergency Vehicle Detection Challenge\n[0:36] Introducing YOLO for Ambulance Detection\n[1:12] Tutorial Overview: Fine-Tuning YOLO\n[1:39] Step 1: Creating a Dataset\n[2:08] Exploring the Ambulance Dataset\n[2:46] Step 2: Annotating the Dataset\n[3:41] Completing Image Annotations\n[4:00] Step 3: Converting Annotations to YOLO Format\n[5:29] Conversion Process Completion\n[5:43] Step 4: Fine-Tuning YOLO Model\n[8:56] Running the Model Training\n[9:14] Accessing Trained Model Weights\n[10:14] Step 5: Performing Inference\n[11:34] Testing on Additional Videos\n[12:01] Conclusion and Further Resources\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:     / labellerr  \nTwitter: https://x.com/Labellerr1\n\n\n\n#EmergencyVehicleDetection #YOLO #ComputerVision #AmbulanceDetection #SmartTrafficSystem #YOLOv8 #SegmentationModel #DeepLearning #ObjectDetection #RealTimeDetection #TrafficAI #AIforCities #MachineLearning #AutonomousVehicles #SmartCityAI #FineTuneYOLO #VisionAI",
    "video_url": "https://www.youtube.com/watch?v=Mzk8ynEibGs",
    "embed_url": "https://www.youtube.com/embed/Mzk8ynEibGs",
    "duration": 749,
    "view_count": 129,
    "upload_date": "20250726",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "machine learning",
      "dataannotation.tech side hustle",
      "data annotation tech",
      "dataannotation.tech legit",
      "dataannotation.tech scam",
      "datannotation.tech review",
      "data annotation.tech",
      "dataannotation",
      "dataannotationtech",
      "data annotation tech review",
      "dataannotation tech",
      "dataannotation.tech review",
      "data annotation tech legit"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Every second counts when it comes to saving lives. But what if traffic jams delay ambulances, costing precious time? What if smart systems could detect emergency vehicles in real time and automatically clear the path for them? Can computer vision help ambulances save lives faster? In most cities, traffic congestion slows down emergency vehicles like ambulances. The delay isn't just inconvenient, it's dangerous. Autonomous vehicles and smart traffic cameras need to identify emergency vehicles instantly to respond appropriately. But most systems lack real-time object detection tuned specifically for ambulances. That's where YOLO, one of the fastest and most powerful object detection models, comes in. In this tutorial, we'll show you how to fine-tune YOLO to detect emergency vehicles like ambulances with high accuracy, helping power smarter self-driving cars and responsive traffic systems. I'll walk you through every step collecting data, annotating images, training the model using YOLO segmentation model and testing on real world scenario. Welcome to our tutorial on finetune YOLO for emergency vehicle detection. So to find your yolo for ambulance detection which is our main motive in this tutorial we need to follow some steps. So let's get to our first step which is creating a data set. To create a data set means you have to collect various images of the object you want to detect using YOLO model. In our case we will collect images of an ambulance. So where you can find the images of the the ambulance you can use Kaggle or various other sites like hugging face data set library. Here I'm using Kaggle. As you can see this is the casual data set I using for our it is the data set which has im images of ambulance. There are around uh 529 images of ambulance in various point of view. So you just have to download this data set. Then your first step is completed. Let's get to our next step. For our next step, we have to perform annotation on the data set we have downloaded. You can use labellerr platform just like I have did. So you have just to go labellerr and create a data set. I have already created a data set and create a project. Then you just have to create add a class called ambulance and using li platform you can update with just few clicks. As you can see our model has been fully annotated. We are using segmentation here which is also known as polygon annotation. So we just pass submit this. You have to keep repeating this process until until all of your images has been annotated. With this your next step is completed. Let's get to our next step. So for the next step you have to convert your annotation data to YOLO format for fine-tuning YOLO. As you know when the annotation is completed you will download the annotation in coco JSON format but to finetune YOLO you need in a specific file structure called YOLO format. So I have simplified this process using a pre-built script of my own. You just have to clone this uh repository I have created for this task. And then for you have to just import uh yolo this cocoa to yolo converter function form yolo fine-tune utilities dot coco yolo converter dot seg converter and then here you have to provide these three crucial information. First is our JSON part. This is the part to annotation.json JSON which you have downloaded from annotation platform. Then you have to provide it with the image directory. It's the directory which contain all the images on which annotation has been performed. So this is the path I providing it. And then an output directory where the directory will be created of your YOLO format. So let's run this. As you can see it is compra conversion is completed. Then let's get to our next step. So our pre-processing is done. Now we get to our main uh problem. Finetuning yolo. To finetuning yolo you have to install ultralix libraries. Then you have to import all the required libraries which are ultraalytics. Let's import ultralytics. As you can see I have imported ultralytics and these are the GPU I'm using for my model training. Let's import YOLO. I have imported YOLO from Ultralytics. now. So see what this step is. As you can see, I'm currently on my this uh part. I have to provide it this data set part to the data set I have created here is called YOLO format. Now the data set part has been set which is this YOLO format. To train our data data set on on YOLO 11 model we have to provide this command line for our training. As you can see this command line has some variables like task mode. I will explain each one line one by one. For task as our task is segmented. So we are using segment and our mode is training as we are training YOLO model. Then data. This is the data set part I have created. And in that data set part I have to provide the yimml file. As you can see when you will create this a yimml file is created. And this contain the location of all the images I I'm using in this fine-tuning. As you can see, I'm using all my images for training. So there all the variables are pointing to our same image. But if you use split, all the variables will provide to different like train will proide to train well to image/well and test will proide to image/ test. So what are next? I'm using YOLO model 11x segmentation for our finetuning 10. So I running it till epox 200 on the image size of 640 and the batch size of 10. So, so the batch size means 10 images are taken per turn per turn and approximate the total data set is passed to the model 200 times and image size in which uh images will be taken in the model. So let's run this. This will take two uh a few times. So let's wait for it. So when your model training will be finished, there will be a run folder created. So in run folder, there will be a segment and then a train. Then in that train you will find your weights. So this is the f fin fine tune model you are going to use for ambulance detection. This is called best.pt pt and there is also a last pt which is the model based on your last epox value. So to inference using this you have to just use this command. So you have to write yolo task equal to segment mode equals to predict model equals to the address to the this model not like our previous case we provide the model to yolo dot / s seg here you can choose your confidence uh equal 0.7 source here I'm providing an sample video and then saving that video I'm showing you without the labels So let's run the inference. As you can see our model is inferencing and finding some ambulance in our video. Let's see our video. Similarly, you can perform inference on various other videos just by changing the variable here. Let's see the result on some other videos. With this, our tutorial on fine-tuning YOLO for emergency vehicle detection is completed. Now you can perform and create your own fine-tuned model for ambulance detection and various other object detection. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Every second counts when it comes to saving lives. But what if traffic jams delay ambulances, costing precious time? What if smart systems could detect emergency vehicles in real time and automatically clear the path for them? Can computer vision help ambulances save lives faster? In most cities, traffic congestion slows down emergency vehicles like ambulances. The delay isn't just inconvenient, it's dangerous. Autonomous vehicles and smart traffic cameras need to identify emergency vehicles instantly to respond appropriately. But most systems lack real-time object detection tuned specifically for ambulances. That's where YOLO, one of the fastest and most powerful object detection models, comes in. In this tutorial, we'll show you how to fine-tune YOLO to detect emergency vehicles like ambulances with high accuracy, helping power smarter self-driving cars and responsive traffic systems. I'll walk you through every step collecting data, annotating images, training the model using YOLO segmentation model and testing on real world scenario. Welcome to our tutorial on finetune YOLO for emergency vehicle detection. So to find your yolo for ambulance detection which is our main motive in this tutorial we need to follow some steps. So let's get to our first step which is creating a data set. To create a data set means you have to collect various images of the object you want to detect using YOLO model. In our case we will collect images of an ambulance. So where you can find the images of the the ambulance you can use Kaggle or various other sites like hugging face data set library. Here I'm using Kaggle. As you can see this is the casual data set I using for our it is the data set which has im images of ambulance. There are around uh 529 images of ambulance in various point of view.",
      "So you just have to download this data set. Then your first step is completed. Let's get to our next step. For our next step, we have to perform annotation on the data set we have downloaded. You can use labellerr platform just like I have did. So you have just to go labellerr and create a data set. I have already created a data set and create a project. Then you just have to create add a class called ambulance and using li platform you can update with just few clicks. As you can see our model has been fully annotated. We are using segmentation here which is also known as polygon annotation. So we just pass submit this. You have to keep repeating this process until until all of your images has been annotated. With this your next step is completed. Let's get to our next step. So for the next step you have to convert your annotation data to YOLO format for fine-tuning YOLO. As you know when the annotation is completed you will download the annotation in coco JSON format but to finetune YOLO you need in a specific file structure called YOLO format. So I have simplified this process using a pre-built script of my own. You just have to clone this uh repository I have created for this task. And then for you have to just import uh yolo this cocoa to yolo converter function form yolo fine-tune utilities dot coco yolo converter dot seg converter and then here you have to provide these three crucial information. First is our JSON part. This is the part to annotation.json JSON which you have downloaded from annotation platform. Then you have to provide it with the image directory. It's the directory which contain all the images",
      "on which annotation has been performed. So this is the path I providing it. And then an output directory where the directory will be created of your YOLO format. So let's run this. As you can see it is compra conversion is completed. Then let's get to our next step. So our pre-processing is done. Now we get to our main uh problem. Finetuning yolo. To finetuning yolo you have to install ultralix libraries. Then you have to import all the required libraries which are ultraalytics. Let's import ultralytics. As you can see I have imported ultralytics and these are the GPU I'm using for my model training. Let's import YOLO. I have imported YOLO from Ultralytics. now. So see what this step is. As you can see, I'm currently on my this uh part. I have to provide it this data set part to the data set I have created here is called YOLO format. Now the data set part has been set which is this YOLO format. To train our data data set on on YOLO 11 model we have to provide this command line for our training. As you can see this command line has some variables like task mode. I will explain each one line one by one. For task as our task is segmented. So we are using segment and our mode is training as we are training YOLO model. Then data. This is the data set part I have created. And in that data set part I have to provide the yimml file. As you can see when you will create this a yimml file is created. And this contain the location of all the images I I'm using in this fine-tuning. As you can see, I'm using all my images for training. So",
      "there all the variables are pointing to our same image. But if you use split, all the variables will provide to different like train will proide to train well to image/well and test will proide to image/ test. So what are next? I'm using YOLO model 11x segmentation for our finetuning 10. So I running it till epox 200 on the image size of 640 and the batch size of 10. So, so the batch size means 10 images are taken per turn per turn and approximate the total data set is passed to the model 200 times and image size in which uh images will be taken in the model. So let's run this. This will take two uh a few times. So let's wait for it. So when your model training will be finished, there will be a run folder created. So in run folder, there will be a segment and then a train. Then in that train you will find your weights. So this is the f fin fine tune model you are going to use for ambulance detection. This is called best.pt pt and there is also a last pt which is the model based on your last epox value. So to inference using this you have to just use this command. So you have to write yolo task equal to segment mode equals to predict model equals to the address to the this model not like our previous case we provide the model to yolo dot / s seg here you can choose your confidence uh equal 0.7 source here I'm providing an sample video and then saving that video I'm showing you without the labels So let's run the inference. As you can see our model is inferencing and finding some ambulance in our",
      "video. Let's see our video. Similarly, you can perform inference on various other videos just by changing the variable here. Let's see the result on some other videos. With this, our tutorial on fine-tuning YOLO for emergency vehicle detection is completed. Now you can perform and create your own fine-tuned model for ambulance detection and various other object detection. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 1281,
    "transcript_chunk_count": 5
  },
  {
    "video_id": "xJJindw3X-Q",
    "title": "Stop Wasting Hours Searching Images - This Filter Feature Does It in Seconds!",
    "description": "We just dropped the most requested feature in Labellerr - Smart File Filtering that will revolutionize how you manage your image datasets!\n\nWhat's New:\n✅ Filter by last updated user\n✅ Filter by objects in images (cars, bikes, people, etc.)\n✅ Filter by classifications\n✅ Filter by custom attributes\n✅ Advanced combination filters\n\nReal Example: Need images with sunny weather, no more than 10 cars, bikes present, 2+ large vehicles, and high traffic? Done in 3 clicks!\n\nWhy This Matters:\n⚡ Find specific images in seconds, not hours\n🎯 Combine multiple filters for laser-precise results\n📊 Review, assign, and export files instantly\n🚀 Speed up your annotation workflow by 10x\n\nThis isn't just a filter - it's deep data control that makes massive datasets manageable.\n\nChapters\n[0:00] Introduction\n[0:23] Example: Filtering Traffic Images\n[0:36] Step 1: Filtering by Object (Bikes and Large Vehicles)\n[0:46] Step 2: Filtering by Classification (High Traffic and Sunny Weather)\n[0:52] Step 3: Filtering by Attribute (Not 10 Cars)\n[0:58] Results: 7 Matching Files\n[1:12] Verification of Conditions\n[1:22] Deep Control Over Data\n[1:25] Combining Multiple Filter Options\n[1:31] Reviewing and Assigning Files\n[1:37] Creating an Export\n[1:43] Importance of the Feature\n[1:44] Conclusion and Next Steps\n\n \nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#DataAnnotation #ImageFiltering #AI #MachineLearning #Labellerr\n#ComputerVision #DataManagement #ImageSearch #MLOps #DataLabeling #AITools #Productivity #TechDemo #DataScience #AnnotationTools\n#SmartImageFiltering #DatasetManagement #ObjectDetection #ImageClassification #AnnotationWorkflow #MLDataPrep #AIAnnotation #TrafficDataset #WeatherFiltering #VehicleDetection\n#Tutorial #Demo #NewFeature #ProductUpdate #TechTips #AINews #DataTools #ImageProcessing #SmartSearch #FileManagement",
    "video_url": "https://www.youtube.com/watch?v=xJJindw3X-Q",
    "embed_url": "https://www.youtube.com/embed/xJJindw3X-Q",
    "duration": 116,
    "view_count": 36,
    "upload_date": "20250725",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Hey everyone, we have updated our file filtering section. Let me show you the changes we've made. First of all, let me navigate to the file section in my project. Then you can now filter by last updated by and also by annotations like filtering by objects that are present in the images, the classifications and even attributes. See here I have images of traffic and say I want to get the images that are in sunny weather should not have 10 cars and have bikes and large vehicles more than or equal to two with high traffic. Here's how we can do it. First I need bike and large vehicle more than two. So I will use filtering by object. Next I want high traffic and sunny weather. I will filter by classification. I also want there should not be 10 cars. So I will use filtering by attribute. Now you can see I got seven files that are matching the conditions and let me show you these and you can verify if all the conditions are met or not. Bikes are more than two. High traffic is there. Weather is sunny. Number of cars is not 10. So yeah, these are correct. It gives you deep control over the data. You can combine multiple filter options from the same filter or you can add another filter if you want. Next, I can review any file and perform tasks like review then assign it to someone or create an export. This makes my file browsing very fast. It's quite handy and one of the most critical features in our tool. That's it. For more feature demos, you can visit our YouTube channel. Book a demo with labellerr to know more about annotations. Link in the description.",
    "transcript_chunks": [
      "Hey everyone, we have updated our file filtering section. Let me show you the changes we've made. First of all, let me navigate to the file section in my project. Then you can now filter by last updated by and also by annotations like filtering by objects that are present in the images, the classifications and even attributes. See here I have images of traffic and say I want to get the images that are in sunny weather should not have 10 cars and have bikes and large vehicles more than or equal to two with high traffic. Here's how we can do it. First I need bike and large vehicle more than two. So I will use filtering by object. Next I want high traffic and sunny weather. I will filter by classification. I also want there should not be 10 cars. So I will use filtering by attribute. Now you can see I got seven files that are matching the conditions and let me show you these and you can verify if all the conditions are met or not. Bikes are more than two. High traffic is there. Weather is sunny. Number of cars is not 10. So yeah, these are correct. It gives you deep control over the data. You can combine multiple filter options from the same filter or you can add another filter if you want. Next, I can review any file and perform tasks like review then assign it to someone or create an export. This makes my file browsing very fast. It's quite handy and one of the most critical features in our tool. That's it. For more feature demos, you can visit our YouTube channel. Book a demo with labellerr to know more about annotations. Link in the description."
    ],
    "transcript_word_count": 297,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "Q2-ZH9w3bgo",
    "title": "Fine-Tune YOLO FOR PPE Detection | CV in Manufacturing",
    "description": "Want to learn how to enforce workplace safety with computer vision? In this tutorial, we show you how to fine-tune a YOLO model from scratch to detect Personal Protective Equipment (PPE) like helmets and vests.\n\nPerfect for beginners and professionals in AI, machine learning, or industrial safety automation. Learn dataset creation, annotation, training, and evaluation — all in one video.\n\nResources:\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/fine-tune-yolo-PPE-detection.ipynb\nLabellerr’s Official GitHub: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n\nChapters\n\n0:00 - Introduction: Computer Vision for Safety & Overview of Tutorial\n0:18 - What is Personal Protective Equipment (PPE) and Its Importance\n0:54 - Fine-Tuning the YOLO Model for PPE Detection: Overview\n1:18 - Step 1: Creating a Custom Dataset\n1:42 - Downloading Videos/Images for Dataset Creation\n2:02 - Using Repositories & Scripts to Extract Data\n3:05 - Extracting Frames and Creating the Dataset\n3:12 - Step 2: Image Annotation—Bounding Boxes & Data Labeling\n3:30 - Introduction to Labellerr Platform & Image Annotation Tool\n4:07 - Uploading and Managing Datasets in the Platform\n4:25 - Loading and Reviewing Image Data\n5:02 - Project Creation & Adding Annotation Classes (Helmet & Vest)\n6:11 - Annotation Project Activation and Image-by-Image Labeling\n7:30 - Annotating Multiple Objects (e.g., Multiple People or Equipment per Image)\n8:10 - Completing Annotation for the Dataset\n8:55 - Exporting Annotations in COCO JSON Format\n9:54 - Understanding the Structure of Exported Annotation Files\n10:34 - Converting COCO Format to YOLO Format for Training\n11:10 - Demonstration: Scripted Format Conversion and YOLO Format Explanation\n12:08 - Inspecting the Converted YOLO Labels and Finalizing Data Preparation\n12:54 - Step 3: Fine-Tuning the YOLO Model\n13:05 - Installing Ultralytics and Setting Up Model Training Environment\n13:44 - Configuring YOLO Training with Prepared Data\n14:26 - Initiating and Monitoring the Training Process\n14:50 - Model Outputs: Understanding the Trained Model Files\n15:14 - Running Inference: Testing the Custom Model on Sample Videos\n15:52 - Reviewing Predictions and Generated Results\n16:23 - Running Additional Inferences on New Videos\n16:52 - Conclusion: Empowering Custom PPE Detection Using YOLO\n17:06 - Further Resources & Next Steps\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:     / labellerr  \nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=Q2-ZH9w3bgo",
    "embed_url": "https://www.youtube.com/embed/Q2-ZH9w3bgo",
    "duration": 1046,
    "view_count": 133,
    "upload_date": "20250723",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "machine learning",
      "deep learning",
      "computer vision",
      "ComputerVision",
      "AIAnnotation",
      "YOLOV8",
      "PPEDetection",
      "DataLabeling",
      "MachineLearningTutorial",
      "AISafety",
      "VisionAI",
      "LearnAI",
      "AIWorkflow",
      "AIdevelopment",
      "SmartManufacturing",
      "MLOps",
      "TechTutorial",
      "AIDatasets",
      "IndustrialAI",
      "LearnPython",
      "AIInnovation",
      "ComputerVisionTutorial",
      "make money online",
      "image annotation",
      "image segmentation",
      "data science"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Want to see how computer vision can enforce safety? In this tutorial, we'll walk through how you can fine-tune YOLO to detect safety violation. Hello everyone, welcome to this tutorial. In this tutorial, we are going to fine-tune YOLO model to detect personal protective equipment. So what is personal protective equipment? As you know these equipments which uh which is worn by workers are called personal protective equipment as it help them to protect themsel from any physical injuries and even distinguish them from other individuals. So in this tutorial I am going to explain and perform how you can fine-tune a YOLO model to detect uh these personal equipment on your own from the scratch. As you know YOLO model is only detect 70 classes which belongs to Coco data set. Coco data set doesn't have a personal protective equipment class. So let's see how you can do it. I will tell you in very easy step. First step is to create a data set. Our uh to uh our first step is to create a data set. So where we can get data the data set. To get data set you can check out some uh website like pixels or kaggle where you can get these kind of data set. So or you can just like me create download a video and from that video you can extract uh if frames or images for your data set. So I'm going with the second method. So before that I am uh cloning cloning a g repository to access some required function. So uh I have cloned this G repository where there are some function which help me to create a data set. You can also do it same. So to first I have to create data set. I am using this video as an example. So I just have to provide the address to the video and extract random frame. This will create a direct uh and also mention the amount of images I want. I have around around two or three videos uh in that video repository and I want around 30 images in randomly order from each uh from each video. So I will use extract random frame. So let's run this. So our 30 frames has been extracted and saved to output folder. So our next step is to perform annotation on the data set. So what does annotation mean? Annotation means is to create bounding boxes around the desired object I want to uh uh detect in my image or video. So do this I I have to go to labellerr platform. As you can see this is the labellerr platform where you can perform annotation. First we start by creating a data set. So as you can see I have to select images as I am using images and name the data set. Let's so in next step I have to as you can see these are the images I have to select. So now wait for some times to get data loaded. When the when your image data set is loaded, it will look like this. You can check the images. So these are my images on which I have to perform annotation. It's taking now it's loaded. As you can see, these are the images on which I have to perform annotation. So these are now in our next step I have to create a project. So to uh create a project I have to choose a data set. I am choosing this data set which I have created. Next we start by adding the classes for our annotation. Our first is safety safety helmet which is of bonding box category. Then our next is a vest which is also of bonding box category. And then for next we have to start the next step. We proceed. Our project has been created. When it's its status gets active, we will start our labing process. We will start our labeling. Now we will start labeling image by image. So first we have to annotate the vest or you can choose the helmet. So we have to do this then this. There are two people here. So next is our best We will start like this. And heat. With this our image has been annotated. Then we have to do the same process for the next uh image and repeat that proc till our all the images in our data set has been annotated properly. So I will skip the process as I have already create annotated a data set of my own for this tutorial. Let's go to the as you can see this is my project I have already annotated. I just have to as you can see the whole project is already labelled. So I just have to download my annotation in coco JSON format. So I have to just go to export and create an export. when creating S4 this will be created. So I have just to download in coco JSON format and press download button after which your annotation data annotation of your data set will be downloaded in JSON format. Then you just have to go your annotation data set will be like that. As you can see there are image ID and image other data of images. We just have to go to the as you can see annotation bar. These are annotation. These are the image ID and the annotation of bonding annotation of bonding boxes. You can see these are the bonding boxes. These are X Y and height and height and bread of the bonding boxes. So in our next step our this step is over and in our next step we have to convert our annotation.json into YOLO format. So what is YOLO format? In Euro format you have a image folder or a labels folder and label folder it has the uh class ID name and then x coordinate of the of x coordinate of the most left top box and the y-coordinate of top left box and then height and height and bread of the bonding boxes. In this way you have to store your data. You can simply do it by uh using my script yolo find to utilize dot coco yolo converter dot bonding box converter and input poco to yolo converter I will do that as you can see so there's a of uh the let me so we are not able to now let's see what issue here. Let me try this. So, it's working. As you can see, our YOLO format has been created. These are the images and these are the label. Let me show you the labels of it. These are the class one ID belongs to helmet and class zero, sorry, class one belongs to west and class zero belongs to safety helmet. And these are the bonding boxes. These are the X Y and the height and breadth of the bonding boxes. So in this way you have to store all the annotation of the image with this. Then in our next step we will start the main fine-tuning process of a YOLO model. In our next process of yolo fine-tuning, we will start by installing ultra lytics. So our ultralytics package is installed and we have to import this ultralytics uh lab module. Then from ultralytics we have to import yolo. Then now my present working directory is this yolo fine-tune yolo for PPA detection. I have to provide it with uh data set part. The data set part is this yolo format. So I am providing the yolo yolo format into this data set part. Now let's train our yolo model. So we will start by our task which is detection mode train data set part which is this and then data set yiml. So what is yl? This is yl which is created. It contain the uh parts to our image folder and the all other metadata like which classes is for which uh class name. So zero stand for safety helmet, one stand for vest. So let's run this. As you can see, I am using YOLO 11X model. It's getting downloaded uh installed as you can see. And I will run it around 200 epox on a batch of 30. So it will take some time. So when the training process will be over you will see a runs folder is created. In this run folder there will be a detect folder in which there will train and in that you have your model in which there are two model a best.pt and last. PD best has the by name the best configuration and last city on the last epochs it is trained on. Uh so let's see the inference result of our custom model on a video. So I am providing it with the path to our best model and also with the sample video part and saving that video and I'm showing you without the labels. So let's run this. As you can see our model is running and our result has been saved in predict. So as you can the predict folder has been created. As you can see there is our video. Let's see its inference. Similarly, you can also influence other video just by changing the part to a video like this. Let's run this and check the inference. With this, our YOLO fine-tuning for PP detection is completed. Now you can uh create a fine-tuned model of YOLO to detect PP on your own. >> For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Want to see how computer vision can enforce safety? In this tutorial, we'll walk through how you can fine-tune YOLO to detect safety violation. Hello everyone, welcome to this tutorial. In this tutorial, we are going to fine-tune YOLO model to detect personal protective equipment. So what is personal protective equipment? As you know these equipments which uh which is worn by workers are called personal protective equipment as it help them to protect themsel from any physical injuries and even distinguish them from other individuals. So in this tutorial I am going to explain and perform how you can fine-tune a YOLO model to detect uh these personal equipment on your own from the scratch. As you know YOLO model is only detect 70 classes which belongs to Coco data set. Coco data set doesn't have a personal protective equipment class. So let's see how you can do it. I will tell you in very easy step. First step is to create a data set. Our uh to uh our first step is to create a data set. So where we can get data the data set. To get data set you can check out some uh website like pixels or kaggle where you can get these kind of data set. So or you can just like me create download a video and from that video you can extract uh if frames or images for your data set. So I'm going with the second method. So before that I am uh cloning cloning a g repository to access some required function. So uh I have cloned this G repository where there are some function which help me to create a data set. You can also do it same. So to first I have to create data set. I am",
      "using this video as an example. So I just have to provide the address to the video and extract random frame. This will create a direct uh and also mention the amount of images I want. I have around around two or three videos uh in that video repository and I want around 30 images in randomly order from each uh from each video. So I will use extract random frame. So let's run this. So our 30 frames has been extracted and saved to output folder. So our next step is to perform annotation on the data set. So what does annotation mean? Annotation means is to create bounding boxes around the desired object I want to uh uh detect in my image or video. So do this I I have to go to labellerr platform. As you can see this is the labellerr platform where you can perform annotation. First we start by creating a data set. So as you can see I have to select images as I am using images and name the data set. Let's so in next step I have to as you can see these are the images I have to select. So now wait for some times to get data loaded. When the when your image data set is loaded, it will look like this. You can check the images. So these are my images on which I have to perform annotation. It's taking now it's loaded. As you can see, these are the images on which I have to perform annotation. So these are now in our next step I have to create a project. So to uh create a project I have to choose a data set. I am choosing this data set which I have created. Next we start",
      "by adding the classes for our annotation. Our first is safety safety helmet which is of bonding box category. Then our next is a vest which is also of bonding box category. And then for next we have to start the next step. We proceed. Our project has been created. When it's its status gets active, we will start our labing process. We will start our labeling. Now we will start labeling image by image. So first we have to annotate the vest or you can choose the helmet. So we have to do this then this. There are two people here. So next is our best We will start like this. And heat. With this our image has been annotated. Then we have to do the same process for the next uh image and repeat that proc till our all the images in our data set has been annotated properly. So I will skip the process as I have already create annotated a data set of my own for this tutorial. Let's go to the as you can see this is my project I have already annotated. I just have to as you can see the whole project is already labelled. So I just have to download my annotation in coco JSON format. So I have to just go to export and create an export. when creating S4 this will be created. So I have just to download in coco JSON format and press download button after which your annotation data annotation of your data set will be downloaded in JSON format. Then you just have to go your annotation data set will be like that. As you can see there are image ID and image other data of images. We just have to go to the as you",
      "can see annotation bar. These are annotation. These are the image ID and the annotation of bonding annotation of bonding boxes. You can see these are the bonding boxes. These are X Y and height and height and bread of the bonding boxes. So in our next step our this step is over and in our next step we have to convert our annotation.json into YOLO format. So what is YOLO format? In Euro format you have a image folder or a labels folder and label folder it has the uh class ID name and then x coordinate of the of x coordinate of the most left top box and the y-coordinate of top left box and then height and height and bread of the bonding boxes. In this way you have to store your data. You can simply do it by uh using my script yolo find to utilize dot coco yolo converter dot bonding box converter and input poco to yolo converter I will do that as you can see so there's a of uh the let me so we are not able to now let's see what issue here. Let me try this. So, it's working. As you can see, our YOLO format has been created. These are the images and these are the label. Let me show you the labels of it. These are the class one ID belongs to helmet and class zero, sorry, class one belongs to west and class zero belongs to safety helmet. And these are the bonding boxes. These are the X Y and the height and breadth of the bonding boxes. So in this way you have to store all the annotation of the image with this. Then in our next step we will start the main fine-tuning process of",
      "a YOLO model. In our next process of yolo fine-tuning, we will start by installing ultra lytics. So our ultralytics package is installed and we have to import this ultralytics uh lab module. Then from ultralytics we have to import yolo. Then now my present working directory is this yolo fine-tune yolo for PPA detection. I have to provide it with uh data set part. The data set part is this yolo format. So I am providing the yolo yolo format into this data set part. Now let's train our yolo model. So we will start by our task which is detection mode train data set part which is this and then data set yiml. So what is yl? This is yl which is created. It contain the uh parts to our image folder and the all other metadata like which classes is for which uh class name. So zero stand for safety helmet, one stand for vest. So let's run this. As you can see, I am using YOLO 11X model. It's getting downloaded uh installed as you can see. And I will run it around 200 epox on a batch of 30. So it will take some time. So when the training process will be over you will see a runs folder is created. In this run folder there will be a detect folder in which there will train and in that you have your model in which there are two model a best.pt and last. PD best has the by name the best configuration and last city on the last epochs it is trained on. Uh so let's see the inference result of our custom model on a video. So I am providing it with the path to our best model and also with the sample video",
      "part and saving that video and I'm showing you without the labels. So let's run this. As you can see our model is running and our result has been saved in predict. So as you can the predict folder has been created. As you can see there is our video. Let's see its inference. Similarly, you can also influence other video just by changing the part to a video like this. Let's run this and check the inference. With this, our YOLO fine-tuning for PP detection is completed. Now you can uh create a fine-tuned model of YOLO to detect PP on your own. >> For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 1627,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "BTWXAm3DFmQ",
    "title": "KOSMOS-2 Tutorial: Microsoft's AI That Points to Objects in Images!",
    "description": "Welcome to our comprehensive KOSMOS-2 tutorial! Discover Microsoft's  multimodal AI that can point to exact objects in images and understand visual context like never before.\n\n🎯 What You'll Learn:\n- Complete KOSMOS-2 setup and installation guide\n- How to run Microsoft's multimodal AI locally on your system\n- Understanding grounding capabilities and bounding boxes\n- Visual question answering with location detection\n- Real-world examples and limitations of the model\n\n📖 Research Paper Overview:\nThis tutorial covers the groundbreaking \"KOSMOS-2: Grounding Multimodal Large Language Models to the World\" research paper, showcasing how this AI bridges text and visual understanding through innovative location tokens.\n\n🔧 Technical Implementation:\n- Installing required libraries (PyTorch, Transformers, Pillow)\n- Loading the Microsoft KOSMOS-2 model (6.66GB)\n- Testing on sample images and custom datasets\n- Understanding model limitations and hallucination issues\n\n⚠️ Key Findings:\nOur testing reveals both impressive capabilities and current limitations, including performance differences between training data and new images.\n\n🔗 Useful Resources:\nLabellerr’s GitHub:https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\nNotebook link: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/KOSMOS-2\nResearch paper link: https://arxiv.org/abs/2306.14824\n\n👍 If this tutorial helped you understand multimodal AI and KOSMOS-2, please like and subscribe for more cutting-edge AI content!\n\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n\n#KOSMOS2 #MultimodalAI #MicrosoftAI #ComputerVision #AITutorial #MachineLearning #DeepLearning #ArtificialIntelligence #VisualAI #ObjectDetection #BoundingBoxes #GroundingAI #VisionLanguageModels #AIResearch #MLTutorial #TechTutorial #PythonAI #TransformersAI #ImageProcessing #AIImplementation #TechEducation #AIModels #NeuralNetworks #OpenSource #DataScience #AITech #SmartAI #FutureAI #TechInnovation #AILearning #CodeTutorial #TechReview #AIExplained #Programming #SoftwareDevelopment #TechGuide #AITools #MachineLearningTutorial #TechContent #Innovation #Technology #AIResearch #ComputerScience #TechDemo #AIShowcase #TechBlog #DigitalInnovation #TechTrends #AIAdvancement #EmergingTech",
    "video_url": "https://www.youtube.com/watch?v=BTWXAm3DFmQ",
    "embed_url": "https://www.youtube.com/embed/BTWXAm3DFmQ",
    "duration": 1550,
    "view_count": 50,
    "upload_date": "20250718",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Hello everyone, welcome to computer vision and Apple tutorial. Imagine an AI that doesn't just describe what it see but can point to exact spot in an image. Before this this challenge was very difficult to overcome but Cosmos 2 developed by Microsoft is a multimodel lang large large language model which solved this problem. Cosmos to ability to link word directly to image region through bounding boxes and special location tokens treating them like invisible hyperlink that connect text and pixels. Before Cosmos to vision language model struggle with ambiguity asking where is the cat might return a general capsule but never the precise region. Developer had to build separate object detector and then fuse their output with text model leading to error in vision language task. In this tutorial, we are going to learn about Cosmos 2 through its research paper and also I will show you how you can run it locally on your system. So let's review the research paper of Cosmos 2 called Cosmos to grounding multimodel large language model to the world. So in this uh research paper the author introduced Cosmos flow which is a MLM which is stand for multimodal large language model. What does multimodal means? Means it can take images, audio or vision language task. But Cosmos can only take language, image or vision language. So it's a partial multimodel not a full multimodel by today's standards but it was introduced in 2023. So at that time it was considered a multimodel. So the expression it gives as a result isn't markdown as you can see. So it links in markdown text span and bounding boxes. This will be the text it is talking about and this is the bounding boxes of the object it has detected in the uh image. So where object description are the sequence of location tokens. So this bonding box are of location tokens. So it is trained on grid. A grid is a data set. It it was trained which is used in vision language task. It means grid data set contains a vision with language pairs. We will talk about it in letters of this section. So when cosmos to detect its object it give it give data in this form that object is talking about and then the location tokens here is the location about the top left point of the bonding box and the uh lowest right point of the bonding box. So this provide us the location of the campfire it is talking about in this image as you can see. So and similarly for the snowman it can also understand a general understanding means it can if I am talking about something it it will understand it here it is the snowman let's see some examples like if I give him the input prompt of the left eye of emoji the completion will be the uh the bonding box covering the left side of the emoji which is a heart and similar Similarly the if I input with the question how many cows are here the two cows present in the image and the bonding boxes around the cows have been made. So then question will be what does the sign say and answered the sign say welcome to carb street and there he has there cosmos has marked the uh the letters which is talking about carbury street then it can also understand some context reasoning like why is this animal unusual so it has given a a detailed understanding like turtles are slow and rabbits are fast and it is unusual because turtle can't run in match with rabbit. So it's understand the context of a image in detail and then answer it and also if you give it a question like what is it? So it here it understand this. So a buoyance attached to the boat here it represent this so it understand the what object we are talking about or like for another question what is the biggest difference between bottle one and bottle two so it has the be bottle one and bottle two but there is a problem which is bottle one and bottle two we are talking about I don't know the how it is understand this is bottle one and bottle two it can be this bottle one and this bottle but they are in half so it understand like this bottle the person is talking about. So it understand the bottle and also the difference between them is their labels. So it understands images quite well according to the research paper author. So if you input will be describe the image it will provide us with the uh description of the image and also with the object it is talking about. So here you can see the objects here talking about is marked in the color respective to the objects here in the picture. So let's see more about poss. So in in introduction it is told about is the ground capability can provide grounding capability can provide a more convenient and efficient grounding means here. So connected relationship between text and the object in the image like if I am talking about a snowman. So snowmax should this text should has a relationship between this object in the image. So this is the grounding capability of the cosmos 2. So it in Cosmos 2 is built upon Cosmos one as we know Cosmos 2 is a transform casual language model. It is trained using next word prediction. Yes, if you're using an NP large language model transformer is necessary then it's talk about uh it's grid data set which I talked about in the start a large data a large scale data set of grounded image text pair. So it in this data set consists of a pair between a with a image and its description in a large volume and on this pair the model is trained. So let's see like this is the uh image and this is the description of the image. A dog in a field of flowers. So as you can see it says and understand the dog field and flower part which are nouns and understand the relation with between flowers. So and it compose the bonding boxes for it also key drop. So it understand this is the main point of this whole photo. So let's see. So in the end we obtain approximately 91 million images, 115 million text fans and 137 million associated bonding boxes. So let's go. So as you can see this is the part it is talk about how the loca bonding boxes is represented. It's by boxes and locations which we talked about. This is the format in which result is provided and we it's a markdown format. Then it about it training setup. Our training process involve a batch size of 419K token consisting of 185 tokens from text corpora 215k tokens from original and grounded image caps and pair and 19k token from interle data. So it is saying equivalent to 25 billion tokens. So it's a large very AdamW optimizer is used here and 256 V 100 GPUs and training takes around uh one day to complete. So with this large volumes of GPU it it still took one day so it uh very or according to 2023 it will be a large model. So let's see uh after or after model is trained we perform instruct tuning to better align false position. So it after training on grid data it also train on another data set data set of visual language instruction data set lava instructor and large langu language only instruction data set and unnatural instruction plan two so it's data set it's quite large let's see what it's talking about so this is the way grounded llm it's so a man in a blue hard hat and Orange West and it mark the Orange West Twitter link. I provided the address to the uh address to the part of it is talking about then something these are the capabilities. These are the result it has to store. So in validation test split these are the things. Let's see what so here it is. So it can also understand the context of the im image like if you I g in the prompt like the frontmost cow to the right of the other cow. So understand which cow I'm talking about. Similarly here it I here it want the middle of giraffe in the middle. So it mark the giraffe in the middle. So it's the zero sort evaluation. It's few sort evaluation. So let's see another thing. So these are all the things the auto plays 2 has achieved. We will see it further in our implementation. So anything else? So let's see what grid data set it has. As you can see this is the grid data sets sample image of grid. So this is a plate in which food is presented. So grid grid data set the image and the uh language has been in pair. So this object is related to this pair of text. Similarly the aluminium tray a small uh bowl filled with creamy light green. So as you can see these are the images from gr data set on which the cosmos flow is trained. So it it is it must be good on understanding the ground where the where the images are in the in in the image we are searching means where the objects are in the image. So this is also a few examples of great data set. So let's move to its implementation and see what the author claims are true or not. Now let's start by implementing Cosmos 2. To implement Cosmos 2, you have to install some libraries which are do transformers, pillow and accelerator. So these are the libraries you will need to implement Cosmos 2. Let's install them. Now libraries has been installed. Then we import these libraries. Get to importing these libraries. As you can see I am importing image from pillow request transformers. I am importing auto processor and cosmos to for conditional generation and these two. Let's import them. Now I'm creating a helper function. This helper function has the sole purpose to visualize the image I'm taking as an example for this tutorial. So I have just given the source like URL and the part of the image I'm using and it will display me nothing else. So let's run this. Now the most important code the checkpoint I'm using Microsoft Cosmos to patch 14 2024. So this is the model I'm going to use. So I have to create two variables for this a model and a processor using cosmos to for conditional integration. Uh I creating this model variable and a processor for using auto processor. Now let's now as you can see the model is downloading. It's around 6.66 GB. It's a large model according to 2023 but now it's a very small model you can say. As you can know, Pix 12 and other models are very large compared to this. So our model has been downloaded pretty much. Let's take a sample image. This is the sample image provided by P Cosmos 2. So in the in this image the model is trained very well. This this is the image on which we seen in research paper. This is a snowman warming itself with a campfire. So let's run this Cosmos 2. I have to give a prompt. As you know in the research paper I mentioned that grounding is used to implement the grounding feature of Cosmos to where you will get the address or hyperlink to the uh object image and then you have to follow this by a processor provide these the prompt to the text and image and then you have to perform this then these are generated test process tests which are shown here. Let's run this. As you can see the clean up and extract for is equals to false like so the process data will have the all the tags of the object it has detected you will see it now but you can turn it to true to clean up and extract the information in separate uh manner like here you can see as you can see when the cleanup and extract is false the result is given in with all the all These tags you can see the grounding is refers and the image of a snowman and the place and its location. This is the location detected by the object uh detected of the object by the model as you can see and it's giving us a general description of the image warming a snowman warming himself by there's another object called fire then fire and then this. So in simple it's saying and it is very difficult to read. So you should use true and take the uh and take the other variables of location in this format which make it easier to use it further. So the model is saying an image of a snowman warming itself by a fire and the location of a snowman is here. As you can see this is the burning box of the fire snowman at the end of the burning box of the fire. You can see now let's create this function this whole script in a function. So this is a function you should create which where arguments will be image a prompt and clean up extract. If you want true like if you want result in that format you can use true. If you want result in that you can use false. So let's run this. Let's take this image again. So let's run this question which I have read previously an image of it is giving me a same as that. So now let's ask question related to the image like what will happen to the snowman. It's a question in which if a model has a context to the image and its understanding then it will give us a proper answer. So what will happen to the snowman? Snowman will melt away in the in the heat of fire. So it understand what a snowman is and snowman will melt in heat of fire. As you know I am not providing it with grounding. So the other result is like this. So the this box is empty. If I provide with uh as you can see grounding let's do the same here grounding let's see as you can see the snowman object is detected but it is not detected a fire object which I don't know why because if the sentence has both snowman and both objects would be have been present here. So this is the fault of cosmos to model. Let's get to another prompt. What is happening in this image? Let's get a general description. Uh so it's giving us a more detailed description of the image. The the image features a snowman wearing a hat sitting in front of fire. The snowman is surrounded by snow. The fire is burning brightly. The sea is set in snowy forest with the snowman and the fireplace providing warmth and comfort. So it's understand the image and provided a general detailed description of the image. So, so I have created this script as running this code line by line is too much tend uh hard task and time consuming. So, we will just ask a set of question in a list and run this. So, what my first question is what is the color of the snowman? Second is pro grounding. What is the color of the snowman's hat? And I need the location of the snowman hat. What is snowman doing in the image? Is something bad's going to happen in this image? It's a context space. If the model understand it will understand the snowman will melt in the heat of fire in the heat created by fire. What will happen to the snowman? What will happen in snowman melts? Let's see our first what is the color of snowman? The color of snowman is white. It's true. Then what is the color of snowman? The snowman cap is blue. Let's see. Uh it's around blue or gray. Uh then it's giving me the snowman cap's location. So this is the it's true. Now what is snowman doing the snowman sitting by the fire in this one? It's true. Is something bad going to happen in the image? Yes. But it's not pro what will happen? So it's it's limitation. What will happen to snowman? What will happen? Snowman will melt away in the heat of fire. So it's uh understanding what will happen to snowman. It's performing quite well. But this image is the data set on which belongs to data set on which the model is set. So it has a great understanding of that image. So what will happen in human male? Uh uh yes yes what will happen? You you are seeing that it's it's hallucinating. If a human male it will cause a lot of damage to environment so many known poor their ability to melt snow which is why as you can see the model is hallucinating right now when the question begins too complex like what will happen in snowman melt. It's a very complex question according to this model. So it starts hallucinating and giving us a random gibberous as you can see it led to flooding errors and error environmentally. So I don't know why this model might need such useless information. Now let's take a image which is outside from this data set training data set. So as you can see this is the image of a image of two elephant holding their trunks. So let's ask them question. Question one is an image of I want description. Which animal are in this image? And what are the elephants doing in this image? These are pretty standard questions. Let's see what happened. So as you can see the modus test wrong information. Two lions. It is there are no lines in this image as you can see. So it's the it is wrong. It's saying a image of two lions standing next to each other in a zoo close. Zoo enclosed part is true but the animal it has detected is for false. Which animals are in the in there are two animals. What's the two animal? You have to give me in description there are two elephants. I have also the question what are elephants doing in the image. Two elephants are standing close to the possibility in a zoo closer. They are both facing same direction. They same direction is wrong. They are facing each other. Uh with one element slightly to the left, it's not waring us accurate information about the image. Uh let's take another image. So this is a image of a telephone board in front of Big Ben. It's a pretty standard image. There are not much things going on here other than the person is standing here. Let's see. Uh I want a general description of this image. Where is this image taken? The location. So it will be London, London, UK. What is the tallest object in this image? So Big Ben will be the tallest. And then any person in the image I am talking about this person. Then where is this? Where is the person in the image? I want the location. So let's see what will happen. So an image of tall building and a clock tower in London, England. It has already provided me the location of the image. So it's showing. So tall building. So here is the tall building it is talking about and the big ban and clock tower is the big ban here. So it's it's working right here. Then where is the image in London, England? So it's true. Tallest object in the tallest clock tower. It's talking about big ban but it's right clock tower. Any question is yes. The person is the person is a cityscape with a blue sky. A building a building with a clock as the model is hallucinating here. It's creating random gibberish sentences. The this image the object is there. So the location will also be block. So this model is not understanding where is the question mean it has not detected the question yet. Let's take another and our final sample image. So this is a funny image of a boy which faces smashed in k. Let's run these two question. An image of where is the wristwatch. So this is the wristwatch I'm talking about. So let's run the model. So, an image of a man smashing a cake with a sledgehammer on his head. I don't understand where it's saying a sledgehammer. I think the model understanding the this hand is a sledgehammer. So, it's wrong here. A man and a sledgehammer. Where is the wristwatch? Wristwatch is located in the middle of the table. The description is quite wrong here but the location expiring is of the middle of the table. It's around this part. So it's must be this best watch but it's not providing us with the right description. So this is all from me today. Now you understand how you can implement Cosmos 2. So experiment it yourself and see the result. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Hello everyone, welcome to computer vision and Apple tutorial. Imagine an AI that doesn't just describe what it see but can point to exact spot in an image. Before this this challenge was very difficult to overcome but Cosmos 2 developed by Microsoft is a multimodel lang large large language model which solved this problem. Cosmos to ability to link word directly to image region through bounding boxes and special location tokens treating them like invisible hyperlink that connect text and pixels. Before Cosmos to vision language model struggle with ambiguity asking where is the cat might return a general capsule but never the precise region. Developer had to build separate object detector and then fuse their output with text model leading to error in vision language task. In this tutorial, we are going to learn about Cosmos 2 through its research paper and also I will show you how you can run it locally on your system. So let's review the research paper of Cosmos 2 called Cosmos to grounding multimodel large language model to the world. So in this uh research paper the author introduced Cosmos flow which is a MLM which is stand for multimodal large language model. What does multimodal means? Means it can take images, audio or vision language task. But Cosmos can only take language, image or vision language. So it's a partial multimodel not a full multimodel by today's standards but it was introduced in 2023. So at that time it was considered a multimodel. So the expression it gives as a result isn't markdown as you can see. So it links in markdown text span and bounding boxes. This will be the text it is talking about and this is the bounding boxes of the object it has detected in the uh",
      "image. So where object description are the sequence of location tokens. So this bonding box are of location tokens. So it is trained on grid. A grid is a data set. It it was trained which is used in vision language task. It means grid data set contains a vision with language pairs. We will talk about it in letters of this section. So when cosmos to detect its object it give it give data in this form that object is talking about and then the location tokens here is the location about the top left point of the bonding box and the uh lowest right point of the bonding box. So this provide us the location of the campfire it is talking about in this image as you can see. So and similarly for the snowman it can also understand a general understanding means it can if I am talking about something it it will understand it here it is the snowman let's see some examples like if I give him the input prompt of the left eye of emoji the completion will be the uh the bonding box covering the left side of the emoji which is a heart and similar Similarly the if I input with the question how many cows are here the two cows present in the image and the bonding boxes around the cows have been made. So then question will be what does the sign say and answered the sign say welcome to carb street and there he has there cosmos has marked the uh the letters which is talking about carbury street then it can also understand some context reasoning like why is this animal unusual so it has given a a detailed understanding like turtles are slow and rabbits are fast",
      "and it is unusual because turtle can't run in match with rabbit. So it's understand the context of a image in detail and then answer it and also if you give it a question like what is it? So it here it understand this. So a buoyance attached to the boat here it represent this so it understand the what object we are talking about or like for another question what is the biggest difference between bottle one and bottle two so it has the be bottle one and bottle two but there is a problem which is bottle one and bottle two we are talking about I don't know the how it is understand this is bottle one and bottle two it can be this bottle one and this bottle but they are in half so it understand like this bottle the person is talking about. So it understand the bottle and also the difference between them is their labels. So it understands images quite well according to the research paper author. So if you input will be describe the image it will provide us with the uh description of the image and also with the object it is talking about. So here you can see the objects here talking about is marked in the color respective to the objects here in the picture. So let's see more about poss. So in in introduction it is told about is the ground capability can provide grounding capability can provide a more convenient and efficient grounding means here. So connected relationship between text and the object in the image like if I am talking about a snowman. So snowmax should this text should has a relationship between this object in the image. So this is the grounding capability of the cosmos",
      "2. So it in Cosmos 2 is built upon Cosmos one as we know Cosmos 2 is a transform casual language model. It is trained using next word prediction. Yes, if you're using an NP large language model transformer is necessary then it's talk about uh it's grid data set which I talked about in the start a large data a large scale data set of grounded image text pair. So it in this data set consists of a pair between a with a image and its description in a large volume and on this pair the model is trained. So let's see like this is the uh image and this is the description of the image. A dog in a field of flowers. So as you can see it says and understand the dog field and flower part which are nouns and understand the relation with between flowers. So and it compose the bonding boxes for it also key drop. So it understand this is the main point of this whole photo. So let's see. So in the end we obtain approximately 91 million images, 115 million text fans and 137 million associated bonding boxes. So let's go. So as you can see this is the part it is talk about how the loca bonding boxes is represented. It's by boxes and locations which we talked about. This is the format in which result is provided and we it's a markdown format. Then it about it training setup. Our training process involve a batch size of 419K token consisting of 185 tokens from text corpora 215k tokens from original and grounded image caps and pair and 19k token from interle data. So it is saying equivalent to 25 billion tokens. So it's a large very AdamW optimizer is used",
      "here and 256 V 100 GPUs and training takes around uh one day to complete. So with this large volumes of GPU it it still took one day so it uh very or according to 2023 it will be a large model. So let's see uh after or after model is trained we perform instruct tuning to better align false position. So it after training on grid data it also train on another data set data set of visual language instruction data set lava instructor and large langu language only instruction data set and unnatural instruction plan two so it's data set it's quite large let's see what it's talking about so this is the way grounded llm it's so a man in a blue hard hat and Orange West and it mark the Orange West Twitter link. I provided the address to the uh address to the part of it is talking about then something these are the capabilities. These are the result it has to store. So in validation test split these are the things. Let's see what so here it is. So it can also understand the context of the im image like if you I g in the prompt like the frontmost cow to the right of the other cow. So understand which cow I'm talking about. Similarly here it I here it want the middle of giraffe in the middle. So it mark the giraffe in the middle. So it's the zero sort evaluation. It's few sort evaluation. So let's see another thing. So these are all the things the auto plays 2 has achieved. We will see it further in our implementation. So anything else? So let's see what grid data set it has. As you can see this is the grid data sets",
      "sample image of grid. So this is a plate in which food is presented. So grid grid data set the image and the uh language has been in pair. So this object is related to this pair of text. Similarly the aluminium tray a small uh bowl filled with creamy light green. So as you can see these are the images from gr data set on which the cosmos flow is trained. So it it is it must be good on understanding the ground where the where the images are in the in in the image we are searching means where the objects are in the image. So this is also a few examples of great data set. So let's move to its implementation and see what the author claims are true or not. Now let's start by implementing Cosmos 2. To implement Cosmos 2, you have to install some libraries which are do transformers, pillow and accelerator. So these are the libraries you will need to implement Cosmos 2. Let's install them. Now libraries has been installed. Then we import these libraries. Get to importing these libraries. As you can see I am importing image from pillow request transformers. I am importing auto processor and cosmos to for conditional generation and these two. Let's import them. Now I'm creating a helper function. This helper function has the sole purpose to visualize the image I'm taking as an example for this tutorial. So I have just given the source like URL and the part of the image I'm using and it will display me nothing else. So let's run this. Now the most important code the checkpoint I'm using Microsoft Cosmos to patch 14 2024. So this is the model I'm going to use. So I have to create two",
      "variables for this a model and a processor using cosmos to for conditional integration. Uh I creating this model variable and a processor for using auto processor. Now let's now as you can see the model is downloading. It's around 6.66 GB. It's a large model according to 2023 but now it's a very small model you can say. As you can know, Pix 12 and other models are very large compared to this. So our model has been downloaded pretty much. Let's take a sample image. This is the sample image provided by P Cosmos 2. So in the in this image the model is trained very well. This this is the image on which we seen in research paper. This is a snowman warming itself with a campfire. So let's run this Cosmos 2. I have to give a prompt. As you know in the research paper I mentioned that grounding is used to implement the grounding feature of Cosmos to where you will get the address or hyperlink to the uh object image and then you have to follow this by a processor provide these the prompt to the text and image and then you have to perform this then these are generated test process tests which are shown here. Let's run this. As you can see the clean up and extract for is equals to false like so the process data will have the all the tags of the object it has detected you will see it now but you can turn it to true to clean up and extract the information in separate uh manner like here you can see as you can see when the cleanup and extract is false the result is given in with all the all These tags you can see",
      "the grounding is refers and the image of a snowman and the place and its location. This is the location detected by the object uh detected of the object by the model as you can see and it's giving us a general description of the image warming a snowman warming himself by there's another object called fire then fire and then this. So in simple it's saying and it is very difficult to read. So you should use true and take the uh and take the other variables of location in this format which make it easier to use it further. So the model is saying an image of a snowman warming itself by a fire and the location of a snowman is here. As you can see this is the burning box of the fire snowman at the end of the burning box of the fire. You can see now let's create this function this whole script in a function. So this is a function you should create which where arguments will be image a prompt and clean up extract. If you want true like if you want result in that format you can use true. If you want result in that you can use false. So let's run this. Let's take this image again. So let's run this question which I have read previously an image of it is giving me a same as that. So now let's ask question related to the image like what will happen to the snowman. It's a question in which if a model has a context to the image and its understanding then it will give us a proper answer. So what will happen to the snowman? Snowman will melt away in the in the heat of fire. So it understand what",
      "a snowman is and snowman will melt in heat of fire. As you know I am not providing it with grounding. So the other result is like this. So the this box is empty. If I provide with uh as you can see grounding let's do the same here grounding let's see as you can see the snowman object is detected but it is not detected a fire object which I don't know why because if the sentence has both snowman and both objects would be have been present here. So this is the fault of cosmos to model. Let's get to another prompt. What is happening in this image? Let's get a general description. Uh so it's giving us a more detailed description of the image. The the image features a snowman wearing a hat sitting in front of fire. The snowman is surrounded by snow. The fire is burning brightly. The sea is set in snowy forest with the snowman and the fireplace providing warmth and comfort. So it's understand the image and provided a general detailed description of the image. So, so I have created this script as running this code line by line is too much tend uh hard task and time consuming. So, we will just ask a set of question in a list and run this. So, what my first question is what is the color of the snowman? Second is pro grounding. What is the color of the snowman's hat? And I need the location of the snowman hat. What is snowman doing in the image? Is something bad's going to happen in this image? It's a context space. If the model understand it will understand the snowman will melt in the heat of fire in the heat created by fire. What will",
      "happen to the snowman? What will happen in snowman melts? Let's see our first what is the color of snowman? The color of snowman is white. It's true. Then what is the color of snowman? The snowman cap is blue. Let's see. Uh it's around blue or gray. Uh then it's giving me the snowman cap's location. So this is the it's true. Now what is snowman doing the snowman sitting by the fire in this one? It's true. Is something bad going to happen in the image? Yes. But it's not pro what will happen? So it's it's limitation. What will happen to snowman? What will happen? Snowman will melt away in the heat of fire. So it's uh understanding what will happen to snowman. It's performing quite well. But this image is the data set on which belongs to data set on which the model is set. So it has a great understanding of that image. So what will happen in human male? Uh uh yes yes what will happen? You you are seeing that it's it's hallucinating. If a human male it will cause a lot of damage to environment so many known poor their ability to melt snow which is why as you can see the model is hallucinating right now when the question begins too complex like what will happen in snowman melt. It's a very complex question according to this model. So it starts hallucinating and giving us a random gibberous as you can see it led to flooding errors and error environmentally. So I don't know why this model might need such useless information. Now let's take a image which is outside from this data set training data set. So as you can see this is the image of a image of two",
      "elephant holding their trunks. So let's ask them question. Question one is an image of I want description. Which animal are in this image? And what are the elephants doing in this image? These are pretty standard questions. Let's see what happened. So as you can see the modus test wrong information. Two lions. It is there are no lines in this image as you can see. So it's the it is wrong. It's saying a image of two lions standing next to each other in a zoo close. Zoo enclosed part is true but the animal it has detected is for false. Which animals are in the in there are two animals. What's the two animal? You have to give me in description there are two elephants. I have also the question what are elephants doing in the image. Two elephants are standing close to the possibility in a zoo closer. They are both facing same direction. They same direction is wrong. They are facing each other. Uh with one element slightly to the left, it's not waring us accurate information about the image. Uh let's take another image. So this is a image of a telephone board in front of Big Ben. It's a pretty standard image. There are not much things going on here other than the person is standing here. Let's see. Uh I want a general description of this image. Where is this image taken? The location. So it will be London, London, UK. What is the tallest object in this image? So Big Ben will be the tallest. And then any person in the image I am talking about this person. Then where is this? Where is the person in the image? I want the location. So let's see what will happen. So an",
      "image of tall building and a clock tower in London, England. It has already provided me the location of the image. So it's showing. So tall building. So here is the tall building it is talking about and the big ban and clock tower is the big ban here. So it's it's working right here. Then where is the image in London, England? So it's true. Tallest object in the tallest clock tower. It's talking about big ban but it's right clock tower. Any question is yes. The person is the person is a cityscape with a blue sky. A building a building with a clock as the model is hallucinating here. It's creating random gibberish sentences. The this image the object is there. So the location will also be block. So this model is not understanding where is the question mean it has not detected the question yet. Let's take another and our final sample image. So this is a funny image of a boy which faces smashed in k. Let's run these two question. An image of where is the wristwatch. So this is the wristwatch I'm talking about. So let's run the model. So, an image of a man smashing a cake with a sledgehammer on his head. I don't understand where it's saying a sledgehammer. I think the model understanding the this hand is a sledgehammer. So, it's wrong here. A man and a sledgehammer. Where is the wristwatch? Wristwatch is located in the middle of the table. The description is quite wrong here but the location expiring is of the middle of the table. It's around this part. So it's must be this best watch but it's not providing us with the right description. So this is all from me today. Now you understand how",
      "you can implement Cosmos 2. So experiment it yourself and see the result. For more computer vision or machine learning related tasks, you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 3635,
    "transcript_chunk_count": 13
  },
  {
    "video_id": "ajDb1Obtwmg",
    "title": "BLIP Explained: A Unified Vision Language Model",
    "description": "Unlock the power of Vision-Language Models (VLMs) with this complete walkthrough of BLIP—a breakthrough AI model by Salesforce. In this video, we dive deep into how BLIP combines image understanding with natural language generation.\n\nLearn how BLIP’s unified architecture enables image captioning, visual question answering (VQA), and image-text retrieval. We also demonstrate how to implement BLIP using HuggingFace Transformers in Python.\n\nWhether you're into computer vision, deep learning, or multimodal AI, this tutorial is your one-stop guide!\n\nLabellerr’s GitHub: GitHub - Labellerr/Hands-On-Learning-in-Computer-Vision: Hands-On Learning in Computer Vision\n\n🔍 Topics Covered:\n- What is BLIP and why it matters\n- Key innovations: MED architecture & CAPFIL filtering\n- Real use cases of VQA and captioning\n- Hands-on coding with BLIP from HuggingFace\n- Model limitations and strengths\n\n🔗 Resources:\nBlog: BLIP Explained: Use It For VQA & Captioning\nNotebook: Hands-On-Learning-in-Computer-Vision/Model Notebooks/BLIP at main · Labellerr/Hands-On-Learning-in-Computer-Vision\n\nChapters\n[0:00] Introduction to Labeller's SAM2 Tool\n[0:12] Uploading a Football Video for Tracking\n[0:24] Selecting Objects to Track (Ball and Player)\n[0:35] Using the Magic Brush and SAM2 Model\n[0:48] Adding Point Prompts for Segmentation\n[1:01] Visualizing Tracked Objects in the Timeline\n[1:11] Tracking the Ball with SAM2\n[1:18] Tracking the Player with SAM2\n[1:42] Previewing Seamless Object Tracking\n[1:50] Handling Occlusions and Out-of-Frame Objects\n[2:09] Conclusion: Easy Object Tracking with SAM2\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n \n\n#BLIP #VisionLanguageModels #AI #MachineLearning #ComputerVision #VisualQuestionAnswering #ImageCaptioning #DeepLearning #MultimodalAI #SalesforceAI #HuggingFace #BLIPModel #Transformers #VQATutorial #ImageToText #AIModels #ImageUnderstanding #AIDemo #PythonAI #AIResearch #NeuralNetworks #ArtificialIntelligence #DataScience #BLIPTutorial #ImageProcessing",
    "video_url": "https://www.youtube.com/watch?v=ajDb1Obtwmg",
    "embed_url": "https://www.youtube.com/embed/ajDb1Obtwmg",
    "duration": 1397,
    "view_count": 146,
    "upload_date": "20250715",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "cvat guide",
      "labeling software",
      "image tagging",
      "BLIP",
      "Machinelearning",
      "VisionLanguageModel",
      "AI",
      "ComputerVision",
      "ArtificialIntelligence",
      "DeepLearning",
      "AIResearch",
      "DataScience",
      "Allmplementation",
      "imagelabeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hey everyone and welcome back to the channel. Have you ever wondered how AI can look at the picture and not only understand what's in it but also describe it in a detailed sentence or even answer your question about the image. This is possible because something called called vision language models. And today we are going to deep uh dive into a really important one that helped save the entire field of vision language model. We are talking about blip which is made by Salesforce which stand for bootstrapping language image pre-training. Blip. Blip was a gamecher when it came out in 2022 because it created a unified way for a single AI model to get good at both understanding image and generating text about them. This was a huge step forward and helped build the foundation for many of the powerful models we see today. So in this video we are going to break down about blip and h teach you how you can implement blip on your own. First we should see its research paper. Let's get into it. Let's review the research paper of blip. It stands for bootstrapping language image pre-training for unified vision language understanding and generation. So what is blip? Blip at its core is a framework for pre-training an AI model on a massive amount of image and text at the same time. The goal was to create a single unified model that could handle wide variety of tasks that involve both vision and language. So what were the problems that gave part to blip creation? Uh before blip most model were specialist. You had some that were great at understanding based task like matching a sentence to the correct image out of a big file. We call this image text retrieval. Then you had other models that were great at generation based task like writing a caption for a photo. The problem was a model that was good at one usually not so good at other. It's like having a good great listener who couldn't tell a good story or a great storyteller who could uh was not a great good listener. This created a divide in the field. So what solution were presented by labor? Uh the researcher at Salesforce came up with two brilliant uh solution to tackle this. First they created a new model architecture they call multimodel mixture of encoders and decoder called me. This is a fancy way of saying they built a flexible model that can switch between different modes depending on task. Second, to deal with the fact that a lot of image text data scrap on the web is messy and inaccurate, they invented a technique called captioning and filtering or cap field. This is a clever way for the model to clean up clean up its own training data. So let's break down each of the this at the heart of the blip. It's a unified architecture. Think of it as a Swiss an it has different rules but they are all part of the same handle. The ME architecture can operate in three ways. Let's see. Uh unimodel encoder. In this mode it looked at the image and text separately. It learns to create representation or numerical summaries for both so that the representation for a dog picture is very similar to a representation for a word a photo of a dog. Second is image ground text encoder for understanding task. This mode encodes a text while paying close attention to details in image. This helped the model figure out if a sentence truly matches the image content in a fine grain way. Third, image grounded text decoder for a generation task like image capsing. This mode uses the image as a guide to generate a new relevant sentence word by words. To get this to work, this model train out three different objective at once. image text contrastive loss. So what is it? Uh this pushes the model to make the representation of matching image text pair more similar and non-matching pair more different image text matching loss. This is a fine grain check. Uh it teaches the model to determine if a specific piece of text is a perfect match for an image or just loosely related. Language model loss. LM. This is this is what powers the text generator. It teaches the model to predict the next word in a sentence given the image and the word that came before. So the final is uh cap field. Now for the second uh caps cap will stand for captioning and filtering. The internet is full of image with caption but a lot of them are low quality or just plain wrong. Training a model on this noise data can really limits its performance. Cap field uh solve this with a two-step process. the captioner first they take the pre-trained blip uh model and use it to generate brand new synthetic caption for all the web image the filter then the the filter then the model switches the uh switches hat and acts as a filter it's look at both the original web text and the newly generated caption for each image and decided if the text is a good match for the image if it's noisy or irrelevant caps and it gets thrown out. By doing this blip create a massive new data set that is much cleaner and higher quality. The model literally bootstrap itself to get better data to learn from. So what are the benefits of the blip architecture? So from technical standpoint uh standpoint the the advantage are clear. You get one unified model for both understanding and generation. It's more computationally efficient because different functions share most of the same models parameter. The cap fil method is a game changer for improving data quality and it's scalable meaning its performance get even better as you feed it more data. Practically this means researcher get a versatile, effective and efficient model that sets new state-of-the-art performance across a ton of benchmarks from image retrieval to visual question answering. Of course, there is no model which is perfect. Blip has its limitation too. On technical side, it struggle with task where the visual concept were very different from it saw in its web based training data. When dealing with videos, it just treat the frame as a bag of image without truly understanding the sequence of event or time. There is also a risk of confirmation bias because the same base model is used for both captioning and filtering. It might be too lenient on its own mistake letting some bad caps and slip flow practically training blip it still require a ton of computational power. Its performance depend heavily on having access to large scale data set. So in my conclusion blip was a thoughtful and practical evolution in vision language models. It wasn't necessarily a huge uh great model, but it created a foundation for later more modern and powerful VLM. So like the clip fil method was a clever solution to the persistent issue of noisy web data in unified architecture was a forwardthinking move that pointed towards the more general purpose AI model that are common today. While it may have been a bit conservative in its ambition impact was substantial. It push performance benchmark higher and provide a stable well-gineered foundation that influence a lot of follow-up research in this area. So with this our review on blip is over. Let's get to its implementation. Let's see how you can implement blip on your own. So to implement blip first you have to install some required libraries which are torque transformers pillow accelerator. Let's run this. Now let's import these libraries which are torque transformers. We have to import autoprocessor auto model for visual question answering request from pillow. We have to import image and other these things. Let run this. So next step is to create a helper function. This helper function is solely for this notebook as it help me to visualize the image I'm going to use for the notebook. So let's run this uh code. Now our main function implementing blip. First we have to create a checkpoint. This is the model address in hugging phase for blip. VQA means visual question answering base means this model will help me in visual related question answering. So and then I am creating a processor variable and providing it with this uh model model name and a model uh model variable which also with this checkpoint and also some error variables which is torque dot float float 16 and device map auto. Now I creating a image variable and storing the uh uh image in it using the URL which I have taken using parameter. And next the question the question is also a parameter in this function. Then as a I am providing these two variables to the processor for the model to perform inference and then that input provided to the model to generate the output that output it again decoded to provide the answer and now the answer is is returned by the end of this function. Let's run this. So our function has been executed safely. Now let's take a sample image. This sample image from the hugging face data set library. So as you can see this is a image of a cat in a snowy area which is a blurry image. As you can see there is not so much information but there are some information like the area the image is is taken in a snowy place which is confirmed. So let's provide blip with our question what is the weather in this image and the URL of that uh image. So let's run this. As you can see our model is getting download. You can see the model safe answers size is around 1.5 GB. Now as you can see the answer came out is snowy which is right for this image. Now let's take another image this image. So a group of seeps on a hilly area in a grassy grassy hilly area to be specific. Let's ask the very simple question of how many animals are in this image and provide the URL of that image in our function. Let's see the result three which is true. As you can see there are three sips in this image. Now I have created a set of list which contain various question related to that image which are what is the weather in this image. The previous asked question, how many animal in this image which is this question uh which animal is in the image. Uh so it is a sheep. You can see what type of terrain in the image. As you can see it will be hilly and any flower in the image. If a model detects a flower, it will say true or if not false. And then which time of day it is. So it's I think it's a morning or afternoon but it's not night. We are sure of that. Let's run this. I have for each each person I have run blip and provided with the URL. Now first person what is the weather in this? It's sunny. It's true. It's And right. How many animal in this image? Three, which is it. Which animal is in this image? Sheep, which is true. What type of terrain in the image? Hilly. And it's surely hilly area. And then any flower in the image? No, the model doesn't find any flower. If they say I can't also find any flower. Uh so which time of day? It is afternoon. So it's it's uh it's thinking that the image were taken taken during an afternoon which is right because we can't be sure it's day or afternoon. Let's take another uh sample image. [Music] So this sample image uh as you can see this image is of a poster on a wall. Let's zoom out. As you can see this is a wallside image of a room where there are multiple posters some video album or a video glare device and a body plant and uh what furniture you can say. So let's ask some visual question on it like number of poster in this image like it will be three if model taken this as a poster it is wrong because it's not a poster is a video cast video cassette you can say next is name of the device in this image so what this device it is and next is on right side poster what is written on it like so in right side this is the right side so this song reminds me of you is written and any plant in this image uh it should be true. Yeah, I can see this is the plant. Let's run this. Our first person name of the poster number of poster in this image tree which is true. Uh name of the device in this image record player which is true. Uh on the right side poster what is written on it? Music it's here in the middle middle poster it is written music but on the right side there is something else written. So this is wrong as you can see uh any plant in this image. Yes. So it's right. So the model was wrong in this person which has shown its limitation because it was a earlier foundation with a language model. So the it has its limitation but it was a great model at its time. I'm talking about 2022. Let's take another image. [Music] So this image is of a coin on a table or a cardboard. What can you see? So let's see how visual uh question answering is good. The visual question answering model is a base model. If there was a large model, we should try it also. Let's see the questions. Number of coins in this image. There are around uh 13 coins. And what is the color of coins in this image? Silver value written on the coin. Major majority of coin have one value. Uh and which currency does the coin belong to? It belongs to rupee. And which currency is written on the coin? So it's uh rupee also. Let's run this. So the first answer is wrong. There are 12 coin. It's showing it's wrong as there are 13 coin. Uh so it's showing its limitation right now. What is the color of coin? It is in image. Silver it's true. Value return on the coin one it's right. Which currency does the coin belong to? British. I don't know how you can say British it's easily Indian. If it's meant a country which currency is written on the coin pounds I think the model is hallucinating or anything else you can say because it is not a single coin which is belong to pound. Yeah all the coins are rupees. So these are the limitation of the uh this model. Let's get to another image. So this image is a very beautiful image of a van in along a beach where two people are on the van. So let's ask some simple question like which vehicle is in the image which is a van. What is the color of the vehicle? Blue. What is the brand of the vehicle? As you can see VW means Volkswagen. uh number of person in the image I think there are only two no more or where is the which place where is the person in the image so on the top of the manuh which place is in the image it's it's related to beach so the beach answers it will be right so what time of day it is it's around morning or afternoon so what is the van plate vehicle ID so I want to make retrieve the the value which is 7498. As you can see, it's the plate numbered of the van. Let's run this. Which vehicle it is? Van which is right. What is the color of the vehicle? Blue. What is the brand of vehicle? Volkswagen. It's true. Number of opportunity image for a Ford. It's failed miserably. There are only two people in this image. I don't know which other two people it's talking about. So it's wrong there. Where is the person in the image? Top of the man. It's right. Which place is in the image? Beach. It's right. What time of day it is in the image? Afternoon. So according to him it's afternoon. So it's true. You can say what is the van plate vehicle ID? VW. So it's not extracting data from the plate ID but it is extracting the logo of the vehicle. So it's saying the van ID is VW which is a symbol of Volkswagen. Let's change it and make it more specific. Let's see what happened. So you can see it's processing our questions will be last we require. So so it's still the VW. So the model is unable to grasp the OCR of this plate which show that it is not you know OCR value. So it has its limitation with this uh it shows that the bullet model was a revolutionary foundation model of VLM but it has limitation but its approach to problem solving is are unique and the unified architecture was very great at sol solving multiple tasks like visual question answering or image captioning which can you perform by just by changing the model uh name of Salesforce uh Salesforce blip week to sales for image captioning base. There is a model present on hugging face. So with this our tutorial on blip is over. For more computer vision or machine learning related tasks you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "[Music] Hey everyone and welcome back to the channel. Have you ever wondered how AI can look at the picture and not only understand what's in it but also describe it in a detailed sentence or even answer your question about the image. This is possible because something called called vision language models. And today we are going to deep uh dive into a really important one that helped save the entire field of vision language model. We are talking about blip which is made by Salesforce which stand for bootstrapping language image pre-training. Blip. Blip was a gamecher when it came out in 2022 because it created a unified way for a single AI model to get good at both understanding image and generating text about them. This was a huge step forward and helped build the foundation for many of the powerful models we see today. So in this video we are going to break down about blip and h teach you how you can implement blip on your own. First we should see its research paper. Let's get into it. Let's review the research paper of blip. It stands for bootstrapping language image pre-training for unified vision language understanding and generation. So what is blip? Blip at its core is a framework for pre-training an AI model on a massive amount of image and text at the same time. The goal was to create a single unified model that could handle wide variety of tasks that involve both vision and language. So what were the problems that gave part to blip creation? Uh before blip most model were specialist. You had some that were great at understanding based task like matching a sentence to the correct image out of a big file. We call this image text",
      "retrieval. Then you had other models that were great at generation based task like writing a caption for a photo. The problem was a model that was good at one usually not so good at other. It's like having a good great listener who couldn't tell a good story or a great storyteller who could uh was not a great good listener. This created a divide in the field. So what solution were presented by labor? Uh the researcher at Salesforce came up with two brilliant uh solution to tackle this. First they created a new model architecture they call multimodel mixture of encoders and decoder called me. This is a fancy way of saying they built a flexible model that can switch between different modes depending on task. Second, to deal with the fact that a lot of image text data scrap on the web is messy and inaccurate, they invented a technique called captioning and filtering or cap field. This is a clever way for the model to clean up clean up its own training data. So let's break down each of the this at the heart of the blip. It's a unified architecture. Think of it as a Swiss an it has different rules but they are all part of the same handle. The ME architecture can operate in three ways. Let's see. Uh unimodel encoder. In this mode it looked at the image and text separately. It learns to create representation or numerical summaries for both so that the representation for a dog picture is very similar to a representation for a word a photo of a dog. Second is image ground text encoder for understanding task. This mode encodes a text while paying close attention to details in image. This helped the model figure out",
      "if a sentence truly matches the image content in a fine grain way. Third, image grounded text decoder for a generation task like image capsing. This mode uses the image as a guide to generate a new relevant sentence word by words. To get this to work, this model train out three different objective at once. image text contrastive loss. So what is it? Uh this pushes the model to make the representation of matching image text pair more similar and non-matching pair more different image text matching loss. This is a fine grain check. Uh it teaches the model to determine if a specific piece of text is a perfect match for an image or just loosely related. Language model loss. LM. This is this is what powers the text generator. It teaches the model to predict the next word in a sentence given the image and the word that came before. So the final is uh cap field. Now for the second uh caps cap will stand for captioning and filtering. The internet is full of image with caption but a lot of them are low quality or just plain wrong. Training a model on this noise data can really limits its performance. Cap field uh solve this with a two-step process. the captioner first they take the pre-trained blip uh model and use it to generate brand new synthetic caption for all the web image the filter then the the filter then the model switches the uh switches hat and acts as a filter it's look at both the original web text and the newly generated caption for each image and decided if the text is a good match for the image if it's noisy or irrelevant caps and it gets thrown out. By doing this blip",
      "create a massive new data set that is much cleaner and higher quality. The model literally bootstrap itself to get better data to learn from. So what are the benefits of the blip architecture? So from technical standpoint uh standpoint the the advantage are clear. You get one unified model for both understanding and generation. It's more computationally efficient because different functions share most of the same models parameter. The cap fil method is a game changer for improving data quality and it's scalable meaning its performance get even better as you feed it more data. Practically this means researcher get a versatile, effective and efficient model that sets new state-of-the-art performance across a ton of benchmarks from image retrieval to visual question answering. Of course, there is no model which is perfect. Blip has its limitation too. On technical side, it struggle with task where the visual concept were very different from it saw in its web based training data. When dealing with videos, it just treat the frame as a bag of image without truly understanding the sequence of event or time. There is also a risk of confirmation bias because the same base model is used for both captioning and filtering. It might be too lenient on its own mistake letting some bad caps and slip flow practically training blip it still require a ton of computational power. Its performance depend heavily on having access to large scale data set. So in my conclusion blip was a thoughtful and practical evolution in vision language models. It wasn't necessarily a huge uh great model, but it created a foundation for later more modern and powerful VLM. So like the clip fil method was a clever solution to the persistent issue of noisy web data in unified architecture was",
      "a forwardthinking move that pointed towards the more general purpose AI model that are common today. While it may have been a bit conservative in its ambition impact was substantial. It push performance benchmark higher and provide a stable well-gineered foundation that influence a lot of follow-up research in this area. So with this our review on blip is over. Let's get to its implementation. Let's see how you can implement blip on your own. So to implement blip first you have to install some required libraries which are torque transformers pillow accelerator. Let's run this. Now let's import these libraries which are torque transformers. We have to import autoprocessor auto model for visual question answering request from pillow. We have to import image and other these things. Let run this. So next step is to create a helper function. This helper function is solely for this notebook as it help me to visualize the image I'm going to use for the notebook. So let's run this uh code. Now our main function implementing blip. First we have to create a checkpoint. This is the model address in hugging phase for blip. VQA means visual question answering base means this model will help me in visual related question answering. So and then I am creating a processor variable and providing it with this uh model model name and a model uh model variable which also with this checkpoint and also some error variables which is torque dot float float 16 and device map auto. Now I creating a image variable and storing the uh uh image in it using the URL which I have taken using parameter. And next the question the question is also a parameter in this function. Then as a I am providing these two variables to",
      "the processor for the model to perform inference and then that input provided to the model to generate the output that output it again decoded to provide the answer and now the answer is is returned by the end of this function. Let's run this. So our function has been executed safely. Now let's take a sample image. This sample image from the hugging face data set library. So as you can see this is a image of a cat in a snowy area which is a blurry image. As you can see there is not so much information but there are some information like the area the image is is taken in a snowy place which is confirmed. So let's provide blip with our question what is the weather in this image and the URL of that uh image. So let's run this. As you can see our model is getting download. You can see the model safe answers size is around 1.5 GB. Now as you can see the answer came out is snowy which is right for this image. Now let's take another image this image. So a group of seeps on a hilly area in a grassy grassy hilly area to be specific. Let's ask the very simple question of how many animals are in this image and provide the URL of that image in our function. Let's see the result three which is true. As you can see there are three sips in this image. Now I have created a set of list which contain various question related to that image which are what is the weather in this image. The previous asked question, how many animal in this image which is this question uh which animal is in the image. Uh so it is",
      "a sheep. You can see what type of terrain in the image. As you can see it will be hilly and any flower in the image. If a model detects a flower, it will say true or if not false. And then which time of day it is. So it's I think it's a morning or afternoon but it's not night. We are sure of that. Let's run this. I have for each each person I have run blip and provided with the URL. Now first person what is the weather in this? It's sunny. It's true. It's And right. How many animal in this image? Three, which is it. Which animal is in this image? Sheep, which is true. What type of terrain in the image? Hilly. And it's surely hilly area. And then any flower in the image? No, the model doesn't find any flower. If they say I can't also find any flower. Uh so which time of day? It is afternoon. So it's it's uh it's thinking that the image were taken taken during an afternoon which is right because we can't be sure it's day or afternoon. Let's take another uh sample image. [Music] So this sample image uh as you can see this image is of a poster on a wall. Let's zoom out. As you can see this is a wallside image of a room where there are multiple posters some video album or a video glare device and a body plant and uh what furniture you can say. So let's ask some visual question on it like number of poster in this image like it will be three if model taken this as a poster it is wrong because it's not a poster is a video cast video cassette you can say",
      "next is name of the device in this image so what this device it is and next is on right side poster what is written on it like so in right side this is the right side so this song reminds me of you is written and any plant in this image uh it should be true. Yeah, I can see this is the plant. Let's run this. Our first person name of the poster number of poster in this image tree which is true. Uh name of the device in this image record player which is true. Uh on the right side poster what is written on it? Music it's here in the middle middle poster it is written music but on the right side there is something else written. So this is wrong as you can see uh any plant in this image. Yes. So it's right. So the model was wrong in this person which has shown its limitation because it was a earlier foundation with a language model. So the it has its limitation but it was a great model at its time. I'm talking about 2022. Let's take another image. [Music] So this image is of a coin on a table or a cardboard. What can you see? So let's see how visual uh question answering is good. The visual question answering model is a base model. If there was a large model, we should try it also. Let's see the questions. Number of coins in this image. There are around uh 13 coins. And what is the color of coins in this image? Silver value written on the coin. Major majority of coin have one value. Uh and which currency does the coin belong to? It belongs to rupee. And which currency is written",
      "on the coin? So it's uh rupee also. Let's run this. So the first answer is wrong. There are 12 coin. It's showing it's wrong as there are 13 coin. Uh so it's showing its limitation right now. What is the color of coin? It is in image. Silver it's true. Value return on the coin one it's right. Which currency does the coin belong to? British. I don't know how you can say British it's easily Indian. If it's meant a country which currency is written on the coin pounds I think the model is hallucinating or anything else you can say because it is not a single coin which is belong to pound. Yeah all the coins are rupees. So these are the limitation of the uh this model. Let's get to another image. So this image is a very beautiful image of a van in along a beach where two people are on the van. So let's ask some simple question like which vehicle is in the image which is a van. What is the color of the vehicle? Blue. What is the brand of the vehicle? As you can see VW means Volkswagen. uh number of person in the image I think there are only two no more or where is the which place where is the person in the image so on the top of the manuh which place is in the image it's it's related to beach so the beach answers it will be right so what time of day it is it's around morning or afternoon so what is the van plate vehicle ID so I want to make retrieve the the value which is 7498. As you can see, it's the plate numbered of the van. Let's run this. Which vehicle it",
      "is? Van which is right. What is the color of the vehicle? Blue. What is the brand of vehicle? Volkswagen. It's true. Number of opportunity image for a Ford. It's failed miserably. There are only two people in this image. I don't know which other two people it's talking about. So it's wrong there. Where is the person in the image? Top of the man. It's right. Which place is in the image? Beach. It's right. What time of day it is in the image? Afternoon. So according to him it's afternoon. So it's true. You can say what is the van plate vehicle ID? VW. So it's not extracting data from the plate ID but it is extracting the logo of the vehicle. So it's saying the van ID is VW which is a symbol of Volkswagen. Let's change it and make it more specific. Let's see what happened. So you can see it's processing our questions will be last we require. So so it's still the VW. So the model is unable to grasp the OCR of this plate which show that it is not you know OCR value. So it has its limitation with this uh it shows that the bullet model was a revolutionary foundation model of VLM but it has limitation but its approach to problem solving is are unique and the unified architecture was very great at sol solving multiple tasks like visual question answering or image captioning which can you perform by just by changing the model uh name of Salesforce uh Salesforce blip week to sales for image captioning base. There is a model present on hugging face. So with this our tutorial on blip is over. For more computer vision or machine learning related tasks you can check our GitHub where",
      "you will find everything in one place."
    ],
    "transcript_word_count": 3007,
    "transcript_chunk_count": 11
  },
  {
    "video_id": "bORBNxLLTC0",
    "title": "Real-Time Object Tracking with OC-SORT",
    "description": "Want to learn real-time multi-object tracking using OC-SORT? In this tutorial, we break down the OC-SORT paper, explain how it improves over SORT, and walk you through practical implementation using YOLO, BoxMOT, and sample videos. From tracking people and cars to planes and overlapping objects, see OC-SORT in action!\n\nResources:\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/OC-SORT\nBlog: https://www.labellerr.com/blog/oc-sort/\nExplore our GitHub for more computer vision and machine learning projects.\n\n🔗 GitHub: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision\n📌 Topics Covered:\n\n- OC-SORT Paper Overview\n- Observation-Centric Design\n- Real-Time Tracking Demo\n- YOLO + BoxMOT Integration\n- Handling Occlusion & Complex Motion\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:     / labellerr  \nTwitter: https://x.com/Labellerr1\n\n\n#OCSORT #ObjectTracking #MultiObjectTracking #YOLOv8 #BoxMOT #ComputerVision #MachineLearning #AIVideoTracking #PythonAI #RealTimeTracking #SurveillanceAI #SelfDrivingCars #MLProjects #SportsAnalytics #AIResearch #DeepLearningProjects #OpenSourceAI #TrackingAlgorithm #VideoAnalytics #Labellerr",
    "video_url": "https://www.youtube.com/watch?v=bORBNxLLTC0",
    "embed_url": "https://www.youtube.com/embed/bORBNxLLTC0",
    "duration": 475,
    "view_count": 287,
    "upload_date": "20250709",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "machine learning",
      "computer vision",
      "annotation software",
      "data visualization",
      "deep learning",
      "training data",
      "quality assurance",
      "data processing",
      "visual recognition",
      "image tagging",
      "automatic annotation",
      "image classification",
      "image tagging software",
      "ai training",
      "automated labeling",
      "labeling software",
      "data markup",
      "image segmentation",
      "dataset creation",
      "ml training",
      "annotation workflows"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Want to learn object tracking in real time using OC Sort? Watch this tutorial and learn how you can track multiple objects in real time with few lines of code. [Music] Hey everyone, welcome back to the channel. Today we're diving into an exciting new approach in computer vision called observation ccentric sort or OC sort for short. If you're interested in tracking multiple objects and videos like people in a crowd, cars on a street or athletes on a field, this is a mustwatch. We are currently at the official GitHub repo of OC Sort. We'll start by looking at the key ideas and problems this method solves by reviewing the research paper that introduced Oak Sort. And finally, we'll get ready to jump into the code implementation together. What is OC sort all about? OC sort stands for observation ccentric sort. It's a new method for tracking multiple objects in videos. The main idea is to focus more on what the camera actually sees, called observations, instead of just relying on predictions about where objects might be. What previous problem were the authors facing before introducing the topic? Older trackers like sort were struggling with a few major issues. One, linear motion assumption. They relied on common filters that assume objects move in straight lines, which isn't realistic in dynamic environments. Two, error accumulation during occlusion. When objects are temporarily hidden, trackers made guesses about their position, which led to compounding errors. Three, estimationcentric design. The system trusted its predictions more than the actual observations from the camera, leading to identity switches and poor tracking accuracy. What's new about OC sort? OC sort changes things up by being observationcentric. That means it pays more attention to the real detections from the video. Observation ccentric re-update OU. When an object reappears after being hidden, OC sort uses its actual observed position to correct past prediction errors. It builds a virtual trajectory between the last known and current position to recalibrate tracking. Observationcentric momentum, OCM. This leverages the object's movement direction based on real observed data. It helps the tracker better associate new detections with the correct object, reducing identity switches. What are the benefits? Robust to occlusion. It can recover from objects being blocked or hidden. Handles complex motion. Works well even when objects don't move in straight lines. Fast and simple. Runs in real time and is easy to use. Works across different data sets. Performs well without needing lots of tweaks. What are the limitations? Association using IOU. It matches objects based on box overlap which can fail with fastmoving objects or low frame rate videos. Needs parameter tuning. Some components require careful tuning for optimal performance. No appearance features. It doesn't use visual cues to distinguish objects which can lead to identity switches in crowded scenes. My honest take OC sort is a smart upgrade to the classic sort tracker. It's practical, easy to use, and fixes some big problems with older methods. But it could be even better if it used appearance cues like color or texture to help tell objects apart, especially when things get crowded or messy. That's a quick overview of observation ccentric sort and the research behind it. Now, let's move to its code implementation to see how you can perform it on your own. To implement OC sort, we start by installing libraries which are BoxMo and Ultralytics. After that, I've created a helper function for this notebook to visualize the sample video I'm going to use here. Now I'm going to use this sample video of multiple moving people to implement tracking using OC sort. To implement OC sort using box we just have to use this command line where tracking method is OC sort provided with YOLO model REID model name and path to video. You can also provide it with target classes you want to track. Here I am tracking people. So provided zero according to YOLO's Coco indexing. Now run this to see the inference. For next I'm going to track cars. For that I'm using this video. And now let's run OC sort and check the inference. Now let's track multiple planes using o sort. This video is going to be used for that. Now let's perform tracking. Next, we're going to take a video where multiple people appear in small size, which makes tracking difficult to perform. Now, check OC sort here. For next, I'm going to see how well OC sort performs on athletes who are running. Let's run tracking on this video and see the result. For last, we will see how well OC sort performs on objects which overlap a lot. For this, I'm using this video of a horse. Now let's test OC sort here. With this our tutorial on OC sort is over. For more computer vision or machine learning related tasks you can check our GitHub where you will find everything in one place.",
    "transcript_chunks": [
      "Want to learn object tracking in real time using OC Sort? Watch this tutorial and learn how you can track multiple objects in real time with few lines of code. [Music] Hey everyone, welcome back to the channel. Today we're diving into an exciting new approach in computer vision called observation ccentric sort or OC sort for short. If you're interested in tracking multiple objects and videos like people in a crowd, cars on a street or athletes on a field, this is a mustwatch. We are currently at the official GitHub repo of OC Sort. We'll start by looking at the key ideas and problems this method solves by reviewing the research paper that introduced Oak Sort. And finally, we'll get ready to jump into the code implementation together. What is OC sort all about? OC sort stands for observation ccentric sort. It's a new method for tracking multiple objects in videos. The main idea is to focus more on what the camera actually sees, called observations, instead of just relying on predictions about where objects might be. What previous problem were the authors facing before introducing the topic? Older trackers like sort were struggling with a few major issues. One, linear motion assumption. They relied on common filters that assume objects move in straight lines, which isn't realistic in dynamic environments. Two, error accumulation during occlusion. When objects are temporarily hidden, trackers made guesses about their position, which led to compounding errors. Three, estimationcentric design. The system trusted its predictions more than the actual observations from the camera, leading to identity switches and poor tracking accuracy. What's new about OC sort? OC sort changes things up by being observationcentric. That means it pays more attention to the real detections from the video. Observation ccentric re-update OU. When an",
      "object reappears after being hidden, OC sort uses its actual observed position to correct past prediction errors. It builds a virtual trajectory between the last known and current position to recalibrate tracking. Observationcentric momentum, OCM. This leverages the object's movement direction based on real observed data. It helps the tracker better associate new detections with the correct object, reducing identity switches. What are the benefits? Robust to occlusion. It can recover from objects being blocked or hidden. Handles complex motion. Works well even when objects don't move in straight lines. Fast and simple. Runs in real time and is easy to use. Works across different data sets. Performs well without needing lots of tweaks. What are the limitations? Association using IOU. It matches objects based on box overlap which can fail with fastmoving objects or low frame rate videos. Needs parameter tuning. Some components require careful tuning for optimal performance. No appearance features. It doesn't use visual cues to distinguish objects which can lead to identity switches in crowded scenes. My honest take OC sort is a smart upgrade to the classic sort tracker. It's practical, easy to use, and fixes some big problems with older methods. But it could be even better if it used appearance cues like color or texture to help tell objects apart, especially when things get crowded or messy. That's a quick overview of observation ccentric sort and the research behind it. Now, let's move to its code implementation to see how you can perform it on your own. To implement OC sort, we start by installing libraries which are BoxMo and Ultralytics. After that, I've created a helper function for this notebook to visualize the sample video I'm going to use here. Now I'm going to use this sample video of multiple moving people",
      "to implement tracking using OC sort. To implement OC sort using box we just have to use this command line where tracking method is OC sort provided with YOLO model REID model name and path to video. You can also provide it with target classes you want to track. Here I am tracking people. So provided zero according to YOLO's Coco indexing. Now run this to see the inference. For next I'm going to track cars. For that I'm using this video. And now let's run OC sort and check the inference. Now let's track multiple planes using o sort. This video is going to be used for that. Now let's perform tracking. Next, we're going to take a video where multiple people appear in small size, which makes tracking difficult to perform. Now, check OC sort here. For next, I'm going to see how well OC sort performs on athletes who are running. Let's run tracking on this video and see the result. For last, we will see how well OC sort performs on objects which overlap a lot. For this, I'm using this video of a horse. Now let's test OC sort here. With this our tutorial on OC sort is over. For more computer vision or machine learning related tasks you can check our GitHub where you will find everything in one place."
    ],
    "transcript_word_count": 826,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "O6WVrS-apN4",
    "title": "Search Images Instantly Using Natural Language | Labellerr’s Natural Language Search Explained",
    "description": "Tired of endlessly scrolling through your Google Drive or cloud storage just to find one photo? With Labellerr’s new Natural Language Search, you can search images using phrases like \"cake photo\" or \"birthday celebration\", just like you would on Google!\nWhether it's personal images, work files, or huge photo archives, Labellerr makes it simple to search and organize photos without remembering filenames or tags. Perfect for creators, teams, and businesses!\n\n✅ Try it now and stop wasting time.\n🔍 AI-powered visual search is finally here.\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=O6WVrS-apN4",
    "embed_url": "https://www.youtube.com/embed/O6WVrS-apN4",
    "duration": 129,
    "view_count": 26,
    "upload_date": "20250707",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Where is that picture of my friend on his birthday? I can't remember the file name. My drive has all my images, which is so inorganized, and it's too hard to organize. Anyway, I have like thousands or more photos in my Google Drive or database of all kinds like vacations, birthdays, or workrelated. And every time I want to find my photo, I end up scrolling forever. What if I could just type what I remember like cake photo just like Google search but on my database data it will make my searching much easier but sadly Google like search do not work in drive or any database here it's only search by keyword which does not solve my problem if there was a way I could use this type of search in my database searching and organizing images would become so much easier is there any yes there is now with labellerr's new natural language search I can search and organize any images in my drive or database with just ease. Now I just have to go to labellerrs platform and create a workspace. Then in that workspace I just have to create a data set by clicking on create data set. Then I can upload my images from anywhere. It can be Google Drive or local storage or any other cloud platform as labellerr supports all of them for my ease. After images are selected just create a data set and wait when it is uploading. When uploading is done, then I only have to go to data sets and open the data set I created. And then in my data set, choose natural language search to search my desired image. Now I can use Google like search on my images like cake or cake on face to be specific to easily find my image. Here it is, the photo I've been searching for so long. Similarly, you can use this to organize and search any type of data. For example, searching for a specific color object like red car or something else. Its use case is tremendous. It feels like magic, but it's real. Now, just like me, you can also organize your database or any cloud storage very easily using Libler's new feature.",
    "transcript_chunks": [
      "Where is that picture of my friend on his birthday? I can't remember the file name. My drive has all my images, which is so inorganized, and it's too hard to organize. Anyway, I have like thousands or more photos in my Google Drive or database of all kinds like vacations, birthdays, or workrelated. And every time I want to find my photo, I end up scrolling forever. What if I could just type what I remember like cake photo just like Google search but on my database data it will make my searching much easier but sadly Google like search do not work in drive or any database here it's only search by keyword which does not solve my problem if there was a way I could use this type of search in my database searching and organizing images would become so much easier is there any yes there is now with labellerr's new natural language search I can search and organize any images in my drive or database with just ease. Now I just have to go to labellerrs platform and create a workspace. Then in that workspace I just have to create a data set by clicking on create data set. Then I can upload my images from anywhere. It can be Google Drive or local storage or any other cloud platform as labellerr supports all of them for my ease. After images are selected just create a data set and wait when it is uploading. When uploading is done, then I only have to go to data sets and open the data set I created. And then in my data set, choose natural language search to search my desired image. Now I can use Google like search on my images like cake or cake on face",
      "to be specific to easily find my image. Here it is, the photo I've been searching for so long. Similarly, you can use this to organize and search any type of data. For example, searching for a specific color object like red car or something else. Its use case is tremendous. It feels like magic, but it's real. Now, just like me, you can also organize your database or any cloud storage very easily using Libler's new feature."
    ],
    "transcript_word_count": 378,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "hlcFdiuz_HI",
    "title": "How I Fixed My Biggest Annotation Nightmare with Labellerr | Industrial AI for Greyscale Images",
    "description": "As an ML engineer, I faced one of my toughest challenges — annotating complex greyscale images from a water treatment plant. Manual efforts failed, and even top models like SAM didn’t work well.\n\nIn this video, I’ll share how I discovered Labellerr, and how its industrial-grade annotation workflow turned hours of frustration into minutes of precision. From AI-assisted labeling to smart QA tools — here’s how we made it work.\n\n👉 Try Labellerr’s free pilot or book a demo today:\nhttps://www.labellerr.com/book-a-demo\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n#DataAnnotation #MachineLearning #AIAnnotation #Labellerr #GreyscaleImage #IndustrialAI #ComputerVision #AIAutomation #MLWorkflow #ImageSegmentation #CLIPModel #HumanInTheLoop #SAMmodel #WaterTreatmentAI #LabelingTools",
    "video_url": "https://www.youtube.com/watch?v=hlcFdiuz_HI",
    "embed_url": "https://www.youtube.com/embed/hlcFdiuz_HI",
    "duration": 126,
    "view_count": 40,
    "upload_date": "20250705",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "I'm an ML engineer. I got a project from our customer who want to annotate grayscale images of a water treatment plant captured by high-speed cameras. The images had pipes, sand filters, valve actuators, and control panels. They needed every component labeled in their images so they could train an AI system to monitor the plant in real time. I first tried the best segmentation model available, SAM. My model mixed up the sand filters and pipes. It merged actuators into the tanks. RGB images give more data, but here I had only grayscale. Even manual labeling slowed me down and still left mistakes. I needed a smarter, faster method. So, I started browsing and I found a blog from labellerr. So, I looked up the website. Here is what I found. labellerr merges AI speed with human expertise. They have built a workflow tuned for industrial images, AI assisted pre-labeling. They use a custom model to propose rough masks for each class. Magic editor. I click on the sand filters and trim away pipe overlaps in seconds. Clip mode. One click isolates each valve actuator even when it sits right against a filter. Polygon eraser. I remove reflections or flange shadows that the AI mismasked. Human in the loop. Our experts review tricky spots like matching tag numbers to the correct actuator, so nothing slips through. Looking at the results, annotation dropped from 2 hours to 3 to 4 minutes per image. There were barely any overlapping errors. Label accuracy increased up to 99%. labellerr offers this industrial workflow starting at just $499 per month. No hidden fees. Your data stays secure under ST2 standards. Visit labellerr.com/pricing to learn more. If you run a plant and wrestle with complex images, click the link below to book a demo today. Let's make industrial AI work",
    "transcript_chunks": [
      "I'm an ML engineer. I got a project from our customer who want to annotate grayscale images of a water treatment plant captured by high-speed cameras. The images had pipes, sand filters, valve actuators, and control panels. They needed every component labeled in their images so they could train an AI system to monitor the plant in real time. I first tried the best segmentation model available, SAM. My model mixed up the sand filters and pipes. It merged actuators into the tanks. RGB images give more data, but here I had only grayscale. Even manual labeling slowed me down and still left mistakes. I needed a smarter, faster method. So, I started browsing and I found a blog from labellerr. So, I looked up the website. Here is what I found. labellerr merges AI speed with human expertise. They have built a workflow tuned for industrial images, AI assisted pre-labeling. They use a custom model to propose rough masks for each class. Magic editor. I click on the sand filters and trim away pipe overlaps in seconds. Clip mode. One click isolates each valve actuator even when it sits right against a filter. Polygon eraser. I remove reflections or flange shadows that the AI mismasked. Human in the loop. Our experts review tricky spots like matching tag numbers to the correct actuator, so nothing slips through. Looking at the results, annotation dropped from 2 hours to 3 to 4 minutes per image. There were barely any overlapping errors. Label accuracy increased up to 99%. labellerr offers this industrial workflow starting at just $499 per month. No hidden fees. Your data stays secure under ST2 standards. Visit labellerr.com/pricing to learn more. If you run a plant and wrestle with complex images, click the link below to book a demo today.",
      "Let's make industrial AI work"
    ],
    "transcript_word_count": 305,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "nFQj6Qdxbus",
    "title": "Why BoT-SORT is #1 on MOTChallenge: Better Associations, Fewer ID Switches?",
    "description": "Discover how BoT-SORT enables highly accurate object tracking across challenging scenarios like crowds, athletes, and aerial views.\n\nIn this tutorial, you’ll learn how to implement BoT-SORT, understand its strengths over DeepSORT, and use it in real-time applications. Whether you're working with pedestrian detection or aerial surveillance, this guide gives you everything you need to get started with robust multi-object tracking using AI.\n\nResources\n\nBlog: https://www.labellerr.com/blog/bot-sort-tracking/\n\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/BotSORT\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#BoTSORT #ObjectTracking #MultiObjectTracking #TrackingAI #ComputerVision #AIDetection #PedestrianTracking #DeepSORT #YOLOv8 #MOTChallenge #RealTimeTracking #AIinSurveillance #TrackingAlgorithm #PythonTracking #AIproject #OpenCV #RobustTracking",
    "video_url": "https://www.youtube.com/watch?v=nFQj6Qdxbus",
    "embed_url": "https://www.youtube.com/embed/nFQj6Qdxbus",
    "duration": 1154,
    "view_count": 169,
    "upload_date": "20250702",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone, welcome back. Today we are diving into bots sort a famous tracking algorithm for multiple object tracking in video sequence. If you ever worked with surveillance footage, autonomous driving data or crowd data, you know how tricky it can be to keep track of individual, especially when people overlap, cameras move, or the scene get clustered. Bots tackles all of these challenges head on, making your tracking more accurate and reliable. So imagine a situation where you have a video of a busy street and people keep crossing each other. Traditional trackers like sort or deep sort can lose track of who is who when the people walk behind each other. That's called occision or when one object blocks another object. Plus if you have if you have a camera which is moving away say it will be a drone or a handle camera. So throw which throw of the motion model. BSoft solved this issue by combining smart motion prediction appearance feature and a technique called camera motion compensation which stabilize your video. So the tracker see only the uh object moving not the whole scene. So before moving into its implementation, let's review its research paper. Bot sort research paper focuses on multiple object tracking specifically developing an advanced tracking by detection system. Further object tracking in video sequences. Multiple object tracking or MOT is the challenge of detecting and tracking all objects in a scene while maintaining unique identifier for each object throughout their trajectories. This technology is absolutely fundamental for application like autonomous driving uh videos uh surveillance and traffic monitoring system. uh previously the problem author were facing is stated here is the author identifies several critical limitation that were plagued existing sort like tracking algorithm. First uh there were serious calman filter state vector issues like most existing method uses deep sort approach of estimating aspect ratio instead of direct bid estimation. This led to inaccuracy in bonding box dimension and suboptimal localization essentially. So the tracking boxes did not fit the object properly. Second, the CA camera motion sensitivity was a major problem. IOU based tracking approach heavily relied on predicted bonding box quality which failed dramatically in dynamic camera scenarios like when camera move it causes significant shift in bonding boxes location resulting in low overlap between predicted and detected boxes. This ultimately led to increased ID situ switch such and false negative meaning object would lose their identities or disappear entirely from tracking. Third is there was a fundamental uh existing system tradeoff like we have to choose between accuracy and identity preservation. uh using IOU typically achieve better which is a metric for multiple object tracking while reidentification approaches achieve higher IDF one metric but you could not have both at the same time so it was a trade-off situation between accuracy and identity preservation so author novel approach uh the author developed Bshot and Bshaw reidentification with three groundbreaking innovation. First it introduced enhanced calman filter like they completely redesign the state vector to directly estimate width and height using uh the format instead of the aspect ratio. This provide much more accurate bonding box prediction that accur actually fit the object being tracked. Second one is camera motion compensation or CMC. This they implemented a conventional image registration using enhanced correlation coefficient for to to correct the camera motion. This make the tracker robust to dynamic camera scenarios like if the camera is tilted or moving in another angle uh in every frame. So this module would handle these cases so the tracking identities don't switch or and be preserved. Third is it introduce a novel IOU reidentification fusion instead of traditional weighted sum approach that other method use. They develop a completely new method. They rejected low similarity candidates and uses the minimum between IOU distance and adjusted cosine distance. uh this clever fusion strategy is what allows them to achieve high performance in both motion and appearance tracking. So how the new method fix previous problem for improve bonding box accuracy the direct height estimation in their enhanced canon filter produce significantly more accurate bonding boxes. Uh they actually show this visually in figure three. Uh let me show like you can see this uh of their paper. This difference is quite striking for camera motion. Robust CMC module accounts for rigid camera movement by estimating background motion and correcting calvin filter prediction accordingly. This that dramatically reduces ID switches ID switches scenarios solving one of the biggest practical problems in real world tracking for unified performance. They their IOU and reidentification fuser method combines motion and appearance information achieving high uh performance in both MODA metrics and IDF1 metrics. So uh we have all talked about its benefits but there are some uh uh limitation also like computational overhead camera motion calculation can be time consuming for large image through the through the order note this negligible compared to detector in uh inference time in most practical scenarios. There is also the issue of scene dependencies like in a scene where high density of dynamic objects camera motion estimation may fail due to insufficient background key point for reliable estimation. Like if I am tracking an object in a wall. So there is not much information to get what this situ frame orientation is. So tractor may fall here. So there is also a problem of motion estimation failures like incorrect camera motion estimation can lead to unexpected tracker behavior in complex scenarios creating a potential uh single point of failure. So in my approach while the technical contribution are solid and the performance are impressive the systems complexity may limit adoption in production environment whereas the approach feels like a incremental but much more significant improvement rather than a paradigm shift which is a actually appropriate for a mature like object tracking. uh because in objecting there are various other algorithm which are better like bite track for example. So there you have it a comprehensive breakdown of beauty short revolutionary approach to multiple object tracking. We have seen how they track the fundamental pro tackle the fundamental problem in a tracking system. Uh now let's move to its code implementation to see how you can perform it on your own and start tracking object. Now let's see how you can implement bot sort in this notebook. I will tell you two ways you can perform bot sort. First is using ultralytics as its default tracking algorithm is bot sort and second one is using pip library of box mod. It's a python library you can use to implement various tracking algorithm. So it also you can be used in implementing b sort. So let's start by seeing our python version. I'm using python 3.10. Now let's install the uh required libraries there which is box mode and ultralytics. These are the version I am using here. So let's uh run this helper function. This helper function is used in displaying the sample uh video I'm going to uh use in this tutorial. Like uh there are various videos I am going to use. Uh so I have created this function for better uh visualization. Now let's run this function. Now let's see the our first sample uh video. This is the sample video I'm going to use first. This is a sample video containing various objects which are people here and they are going in various direction. Uh the total count of people is very hard to see. So let's see how our tracking algorithm perform. So for ultra I'm using ultralytics first. So to track using ultralytics you just have to import yolo from ultralytics and provide the model name yolo as you can see I'm using yolo 12x and to perform tracking you just have to uh use it track function provide the address image uh video part and then uh truly persistent for videos and also So input the target classes. Now run this code. As you see the inference result of BSot tracking, it has failed to detect some uh objects in this scene. This was happened due to the uh YOLO 12X limitation in object tracking but the tracking trajectories were clear and there were no identity switches in the video. Now let's move to another uh video. Here I'm using pip method. So let's this is the sample video I am going to use. This is a video where there is a camera movement. It's not uh very much uh uh and you can see but the camera is moving. So it's a good video you you can see how bots sword works. So to use bots using box mod we just have to provide with this command line. So we have to just provide box mod track source and parts to video then provide the tracking method which is the algorithm I'm going to use here. it is bot mod bot sort and then the yolo model which I'm going to use then the reidentification model model I'm going to use then the target classes so let's run this code As you see the result here, you see the inference was good and the tracking method were perfectly implemented as there was there were no identity switches and object detection was happening quite good. So there was no object left behind. Now let's take another example where we detect card. This is the sample image I'm going to use for our next tracking. Here you can see this is a video of a highway where multiple cars are going forward. This is a very hard image to track as objects are getting smaller with each frame. Now let's see the inference result of tracking of B sort here. So overall the tracking was very much good but there were some planes where objects were not detected even they were present. So the tracking algorithm doesn't uh identity switches it. So it was a good tracking implementation. Now let's track athlete. This is a sample image of athlete running on a track. As you can see uh this is a very sample sports uh video which is very good for tracking algorithm to be tested. Now let's see how bot sortter works. So the tracking algorithm works greatly here. The tracking was perfect. There were no identity switches and there were no frames where there were no object detected. So it was a great example for B sort implementation whereas worked perfectly but there are uh will be a limitation. We will see it further. Let's test another watch out another video of a plane or we can say multiple planes. This is a sample video I'm going to use to track uh plane using B bot sort. As you can see these plane are following the curve part and uh it's these objects are first comes from small pixel to larger pixel. So let's see how bots track these objects. B sword work here very much great or perfectly you can say there were no identity switches and tracking was very much smooth. Object detection was very much perfect. But these are all the benefits of B sort. Let's see the limitation of B sort. In this video, I'm going to use it. Uh let's track some birds you using Bot. This is a sample image of birds flying across sky or above the ocean. You can whatever you can see. But in this MA video you will see there is some uh tracking issue will which will be faced by uh B short. Let's see its inference on this video. As you seen in the tracking inference of bot sort there was various issue with tracking like the identity switches or object detection failure and many things. So this so there is a clear limitation of bot sort due to a dependency on object tracking which is a significant limiter for its trajectory uh tracking. So with this our tutorial on bot sort is over. For more on machine learning you can check our GitHub where you will find various material like notebook blogs and videos. Check it now.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome back. Today we are diving into bots sort a famous tracking algorithm for multiple object tracking in video sequence. If you ever worked with surveillance footage, autonomous driving data or crowd data, you know how tricky it can be to keep track of individual, especially when people overlap, cameras move, or the scene get clustered. Bots tackles all of these challenges head on, making your tracking more accurate and reliable. So imagine a situation where you have a video of a busy street and people keep crossing each other. Traditional trackers like sort or deep sort can lose track of who is who when the people walk behind each other. That's called occision or when one object blocks another object. Plus if you have if you have a camera which is moving away say it will be a drone or a handle camera. So throw which throw of the motion model. BSoft solved this issue by combining smart motion prediction appearance feature and a technique called camera motion compensation which stabilize your video. So the tracker see only the uh object moving not the whole scene. So before moving into its implementation, let's review its research paper. Bot sort research paper focuses on multiple object tracking specifically developing an advanced tracking by detection system. Further object tracking in video sequences. Multiple object tracking or MOT is the challenge of detecting and tracking all objects in a scene while maintaining unique identifier for each object throughout their trajectories. This technology is absolutely fundamental for application like autonomous driving uh videos uh surveillance and traffic monitoring system. uh previously the problem author were facing is stated here is the author identifies several critical limitation that were plagued existing sort like tracking algorithm. First uh there were serious calman",
      "filter state vector issues like most existing method uses deep sort approach of estimating aspect ratio instead of direct bid estimation. This led to inaccuracy in bonding box dimension and suboptimal localization essentially. So the tracking boxes did not fit the object properly. Second, the CA camera motion sensitivity was a major problem. IOU based tracking approach heavily relied on predicted bonding box quality which failed dramatically in dynamic camera scenarios like when camera move it causes significant shift in bonding boxes location resulting in low overlap between predicted and detected boxes. This ultimately led to increased ID situ switch such and false negative meaning object would lose their identities or disappear entirely from tracking. Third is there was a fundamental uh existing system tradeoff like we have to choose between accuracy and identity preservation. uh using IOU typically achieve better which is a metric for multiple object tracking while reidentification approaches achieve higher IDF one metric but you could not have both at the same time so it was a trade-off situation between accuracy and identity preservation so author novel approach uh the author developed Bshot and Bshaw reidentification with three groundbreaking innovation. First it introduced enhanced calman filter like they completely redesign the state vector to directly estimate width and height using uh the format instead of the aspect ratio. This provide much more accurate bonding box prediction that accur actually fit the object being tracked. Second one is camera motion compensation or CMC. This they implemented a conventional image registration using enhanced correlation coefficient for to to correct the camera motion. This make the tracker robust to dynamic camera scenarios like if the camera is tilted or moving in another angle uh in every frame. So this module would handle these cases so the tracking identities don't",
      "switch or and be preserved. Third is it introduce a novel IOU reidentification fusion instead of traditional weighted sum approach that other method use. They develop a completely new method. They rejected low similarity candidates and uses the minimum between IOU distance and adjusted cosine distance. uh this clever fusion strategy is what allows them to achieve high performance in both motion and appearance tracking. So how the new method fix previous problem for improve bonding box accuracy the direct height estimation in their enhanced canon filter produce significantly more accurate bonding boxes. Uh they actually show this visually in figure three. Uh let me show like you can see this uh of their paper. This difference is quite striking for camera motion. Robust CMC module accounts for rigid camera movement by estimating background motion and correcting calvin filter prediction accordingly. This that dramatically reduces ID switches ID switches scenarios solving one of the biggest practical problems in real world tracking for unified performance. They their IOU and reidentification fuser method combines motion and appearance information achieving high uh performance in both MODA metrics and IDF1 metrics. So uh we have all talked about its benefits but there are some uh uh limitation also like computational overhead camera motion calculation can be time consuming for large image through the through the order note this negligible compared to detector in uh inference time in most practical scenarios. There is also the issue of scene dependencies like in a scene where high density of dynamic objects camera motion estimation may fail due to insufficient background key point for reliable estimation. Like if I am tracking an object in a wall. So there is not much information to get what this situ frame orientation is. So tractor may fall here. So there is",
      "also a problem of motion estimation failures like incorrect camera motion estimation can lead to unexpected tracker behavior in complex scenarios creating a potential uh single point of failure. So in my approach while the technical contribution are solid and the performance are impressive the systems complexity may limit adoption in production environment whereas the approach feels like a incremental but much more significant improvement rather than a paradigm shift which is a actually appropriate for a mature like object tracking. uh because in objecting there are various other algorithm which are better like bite track for example. So there you have it a comprehensive breakdown of beauty short revolutionary approach to multiple object tracking. We have seen how they track the fundamental pro tackle the fundamental problem in a tracking system. Uh now let's move to its code implementation to see how you can perform it on your own and start tracking object. Now let's see how you can implement bot sort in this notebook. I will tell you two ways you can perform bot sort. First is using ultralytics as its default tracking algorithm is bot sort and second one is using pip library of box mod. It's a python library you can use to implement various tracking algorithm. So it also you can be used in implementing b sort. So let's start by seeing our python version. I'm using python 3.10. Now let's install the uh required libraries there which is box mode and ultralytics. These are the version I am using here. So let's uh run this helper function. This helper function is used in displaying the sample uh video I'm going to uh use in this tutorial. Like uh there are various videos I am going to use. Uh so I have created this function",
      "for better uh visualization. Now let's run this function. Now let's see the our first sample uh video. This is the sample video I'm going to use first. This is a sample video containing various objects which are people here and they are going in various direction. Uh the total count of people is very hard to see. So let's see how our tracking algorithm perform. So for ultra I'm using ultralytics first. So to track using ultralytics you just have to import yolo from ultralytics and provide the model name yolo as you can see I'm using yolo 12x and to perform tracking you just have to uh use it track function provide the address image uh video part and then uh truly persistent for videos and also So input the target classes. Now run this code. As you see the inference result of BSot tracking, it has failed to detect some uh objects in this scene. This was happened due to the uh YOLO 12X limitation in object tracking but the tracking trajectories were clear and there were no identity switches in the video. Now let's move to another uh video. Here I'm using pip method. So let's this is the sample video I am going to use. This is a video where there is a camera movement. It's not uh very much uh uh and you can see but the camera is moving. So it's a good video you you can see how bots sword works. So to use bots using box mod we just have to provide with this command line. So we have to just provide box mod track source and parts to video then provide the tracking method which is the algorithm I'm going to use here. it is bot mod bot sort and",
      "then the yolo model which I'm going to use then the reidentification model model I'm going to use then the target classes so let's run this code As you see the result here, you see the inference was good and the tracking method were perfectly implemented as there was there were no identity switches and object detection was happening quite good. So there was no object left behind. Now let's take another example where we detect card. This is the sample image I'm going to use for our next tracking. Here you can see this is a video of a highway where multiple cars are going forward. This is a very hard image to track as objects are getting smaller with each frame. Now let's see the inference result of tracking of B sort here. So overall the tracking was very much good but there were some planes where objects were not detected even they were present. So the tracking algorithm doesn't uh identity switches it. So it was a good tracking implementation. Now let's track athlete. This is a sample image of athlete running on a track. As you can see uh this is a very sample sports uh video which is very good for tracking algorithm to be tested. Now let's see how bot sortter works. So the tracking algorithm works greatly here. The tracking was perfect. There were no identity switches and there were no frames where there were no object detected. So it was a great example for B sort implementation whereas worked perfectly but there are uh will be a limitation. We will see it further. Let's test another watch out another video of a plane or we can say multiple planes. This is a sample video I'm going to use to track uh plane",
      "using B bot sort. As you can see these plane are following the curve part and uh it's these objects are first comes from small pixel to larger pixel. So let's see how bots track these objects. B sword work here very much great or perfectly you can say there were no identity switches and tracking was very much smooth. Object detection was very much perfect. But these are all the benefits of B sort. Let's see the limitation of B sort. In this video, I'm going to use it. Uh let's track some birds you using Bot. This is a sample image of birds flying across sky or above the ocean. You can whatever you can see. But in this MA video you will see there is some uh tracking issue will which will be faced by uh B short. Let's see its inference on this video. As you seen in the tracking inference of bot sort there was various issue with tracking like the identity switches or object detection failure and many things. So this so there is a clear limitation of bot sort due to a dependency on object tracking which is a significant limiter for its trajectory uh tracking. So with this our tutorial on bot sort is over. For more on machine learning you can check our GitHub where you will find various material like notebook blogs and videos. Check it now."
    ],
    "transcript_word_count": 2037,
    "transcript_chunk_count": 7
  },
  {
    "video_id": "MWi3BaAdw4g",
    "title": "DeepSORT Object Tracking Explained",
    "description": "Learn DeepSORT object tracking from theory to implementation using YOLO and DeepSORT-Realtime. This tutorial covers the original research paper, explains tracking by detection, and shows how to use a deep learning embedder for visual memory. We’ll also explore edge cases, performance benchmarks, and apply DeepSORT on people, planes, and vehicles with real video demos. Ideal for AI, computer vision, and surveillance projects.\n\nResources:\n\nBLOG: https://www.labellerr.com/blog/deepsort-real-time-object-tracking-guide/\n\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/DeepSORT\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n#DeepSORT #ObjectTracking #ComputerVision #YOLOv8 #DeepLearning #SurveillanceAI #MultiObjectTracking #KalmanFilter #HungarianAlgorithm #TrackingByDetection #AIimplementation #PythonAI #OpenCV #RealTimeTracking #CNN #VisualTracking #YOLOwithDeepSORT #MOTChallenge #EdgeCaseTracking #MachineLearning #TutorialForBeginners #AIprojects #SecurityCameraAI #AutonomousVision #AIResearch",
    "video_url": "https://www.youtube.com/watch?v=MWi3BaAdw4g",
    "embed_url": "https://www.youtube.com/embed/MWi3BaAdw4g",
    "duration": 1144,
    "view_count": 168,
    "upload_date": "20250630",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone, welcome back to my tutorial. Today we are diving into something really exciting and important in the world of computer vision deep sort which stand for simple online and realtime tracking with a deep association metric. So what exactly deep sort? Well, imagine you are watching a crowded street from a security camera. You see people walking around, some disappearing behind camera and other coming back into view. Now your job is to keep track of each person to know what the person who disappeared behind that car is the same person who came out on the other side. That's essentially what multiple object tracking is all about. It's like uh giving each object in a video its own unique ID and making sure we don't lose track of who is who. So this is incredibly important in so many real world application. Think about surveillance system in airports. Autonomous car that needs to track credits uh people and other vehicles or even sports analytics where we track player through a throughout a game. The ability of constantly identifying and follow object is crucial for these system to work properly. Now before deep sort came along researcher were struggling with a major problem. The earlier tracking method especially the original sort algorithm had this annoying habit of losing track of object. Imagine you are following someone in a crowd they disappear behind a pillar for few seconds and when you when they came back out you think they are completely different person that exactly what was happening. These system were exper experiencing what we call identity switches where they lose track of who is who. Today we are going to break down the research paper that solved this problem and revolutionized object tracking. We will understand the clever technique and that the author used look at their result and at the end I will show you how to implement this yourself. All right, let's dive into the research paper. The first thing we need to understand is the context of this worker. Multiple object tracking has been around for quite some time, but it really took off with the rise of tracking by detection method. Now, what does tracking by detection mean? Think of it like instead of trying to track objects from the very first frame, we first detect all object in each frame and then we try to connect these detection across frame. It's like taking snapshot of a room every second and then trying to figure out which person in snapshot 2 is the same as the person in snapshot one. Before deep sort, there was the original sort algorithm introduced in 2016. Sort was actually pretty good. It was fast, simple, and worked in real time. But it had one major weakness. The biggest issue with sort was frequent identity switches. Let me explain this with a simple example. Imagine you are trying to track two people walking in a hallway. Imagine person A is wearing a red shirt, person B is wearing a blue shirt. They are walking towards each other and for a movement they overlap. one person temporarily block the other from camera view. Now when they separate and both become visible again sort would sometimes get confused and think that the person in red shirt is actually person B and the person in blue shirt is person A. It's basically strapped their identities. This happened because S only looked at motion in formaker. It tried to predict uh where object would be based on their previous movement but it completely ignored what the objects actually look like. The technical term for this is that the sort relied solely on bounding boxes overlap using something called intersection over union or IOU. Think of IOU as a measure of how much rectangles overlap. If they uh overlap a lot the system assume they are the same objects but when object get occluded that means blog from view this approach fails miserably. So so how did the deep sort of author solve this? They came up with a brilliant idea. Why don't we teach the system to remember what object look like? This is where the deep in deep sort come from. They use deep learning to give the tracking system a visual memory. Their approach combined two types of information. First motion information. They kept the good part of sort the calman filter and the Hungarian algorithm. Now a calvin filter is like smart predictor that say based on where this object was how fast it was moving here where I think it would be next. The Hungarian algorithm is like a matchmaker that pair us prediction with actual detection in most optimal way. Second, appearance information. This is the game changer. They added a CNN that that is a type of deep learning model that's really good at understanding image. This CNN looks at each objected object and create what we call an appearance descriptor. Think of it as a unique fingerprint for how that object look. The CNN they use is pretty sophisticated. It's wide uh residual network or a type of neural network architecture that really good at learning complex pattern. It helps to describe what the object look like. Maybe some numbers represent color, other represent shape, texture, and so on. Here's a clever part. For each object being triggered, the system remembers the last 100 appearance descriptor. It's like having a photo album for each person you are tracking. When a new detection comes in, the system look at the new detection and compared it with all the photo album to see which one matches best. So how this fix the previous problem? The combination of motion and appearance information is like having both a GPS and your eyes when you are trying to find someone in a crowd. The GPS motion prediction tells you roughly where the uh they should be but your eyes appearance feature confirm that you are looking at the right person. When object get occluded remember that mean blocked from view the motion prediction became uh unreliable but the appearance feature remains con consistent so the object reappears the system can say here I remember what this p person look like and currently maintain their identity. The author also introduced something called a matching cascade algorithm. Instead of trying to solve everything at one once, they prioritize track based on their age. Think of it like if you have been successfully tracking someone for a long time. They're probably more confident about who they are compared to someone you just started tracking. So the system gives priority to these more established track. Let's see the numbers. The author tested their method on something called MOT 16 benchmark. This is like standardized test for tracking algorithm. They use several metrics to measure performance. Deep sort achieve 61.4% MOTA compared to SS 59.8%. In identity switches, this was the big win. Remember this was the main problem they were trying to solve. Deep sort of reduce identity switches by 45%. From uh now how what about processing speed? Despite all this additional complexity, Deepshot still runs at 40 Hz means it can process 40 frames per second which is more than enough for realtime application. The benefits are pretty clear. First, you get much more reliable tracking. 45% fewer identity situation means your system is much less likely to get confused about who is who. Second, it handles application much better. Third, it maintain realtime performance so you can actually use it in practical application. Uh limitation, let's be honest about limitation two. First, you need a decent GPU to run this in real time because of CNN component. Second, it does produce more false positive. Sometimes it's a bit too easier to maintain track. Third, the CNN was trained specifically on people. So, it might not work on uh other tracking objects like car without retraining. So from my perspective from a practical standpoint deep sort is more of an intelligent evolution rather than a revolutionary breakthrough. The author were smart. They took what worked from sort and added the missing piece visible memory which solved a real problem that uh engineers were facing. This 45% reduction in identity switches is impressive and addresses a real world need. However, the approach is fundamentally incremental. It combine existing technique rather than introducing completely new algorithm. But sometime that exactly what the field need practical solution that works. And there you have it. We have covered the complete deep sort research paper and problem it solved. Uh deep sort that sometimes the best solution comes from combining different approaches intelligently by adding wizard memory to memory based tracking. They created a system that much more robust and reliable. Uh the key takeaway here is the that appearance information is crucial for robust tracking. When motion prediction fail having a visual memory of what object look like can save the day. So uh in next part of this tutorial we are going to dive into the actual code implementation. Let's see how you can implement deep sort on your own. First we start by installing required libraries. uh we have to install ultralytic for object detection as we are going to use yolo and real uh deep sort real time it's a library through which you can perform deep sort with few code so I we just have to run this code now now in deep sort I'm using uh an embedder which is a reidentification ation means a CNN which help in identification of object. So this is I'm going to use the collector get now our next step I'm creating a helper function. This helper function help me uh download sample video which which I am going to use in this tutorial and visualize it in the notebook. Now let's come to our main function deep sort. First we import dolo from ultralytics and then from deep sort real time from deep sort tracker we import deep sort. Then we import cv2 o and a random to generate random color for each bounding boxes. Now in deep sort algorithm I'm providing with a model yolo 11 next. Then uh here as you can see I initialize deep sort tracker. This tracker has a maximum age argument means how much frame it will look before predicting the next frame. How much objects are required to start the tracking means and minimum number of objects after tracking started is two. and then embed which is a reidentification identity which helps me extract feature from object to keep track of it uh which is uh click vit I'm using and these are uh arguments you will needed for deep sort now as you can see I'm you taking frames by frame and then providing frames to our model and then for each detection I am appending then value to detection list and then this is the main code for deep salt using deep using deep tracker I updating tracker based on its detection and frame and taking that detection tracker is confirm I am drawing bounding boxes using this part of code and then I am providing with each ID The ID is already provided by the tracker algorithm. Then I am saving this into our uh into a video format. Now let's run this code. Let's track people first. This is the sample video I'm going to implement in this tutorial. Uh these this is a video of two people jogging jogging. So it's a really simple video and a very easy tracking video for a tractor. Let's implement deep sorting. You have seen the result of deep sort on this video. As you see it work very great on these type of video where object is moving in a linear path. Now let's take an example of another video. Let's try playing. Now let's see the uh sample video I'm using. This is a a video of a multiple planes flying across sky. As you can see, these are traveling in a curved part which make it harder for an object tracker to track it. Let's see the result of deep sort on it. As you can see, I'm providing with the part of the video and the output name and the target class which is four. Four is the target class of plane in YOLO. Let's run this code. As you have now seen the result of deep south on a plane, it was a pretty pretty good result as you know that the plane has been uh very much clearly detected and it was the right identity provided to each object. Now let's see the edge cases of deep thought. This is a video I'm going to use. Uh in this video you can see the car is moving in a straight line but the problem with uh it is that car which is the object is becoming bigger by every frame which make it difficult for tracker to detect uh like calvin filter to predict is the same as previous objects. Uh you will see what will happen in this type of video. Let's see the influence of deepot on this video. As you seen in the inference video, the result was not pretty much good. The objective tracking was very bad and the identity kept switching. This was a edge cases or the limitation of carman filter which has been used in uh deep sort. It also failed to recognize the same object which is moving forward in different frame. Uh due to uh the limitation of feature extraction with this our tutorial on deep sort is over. To learn more you can check our git repository where you can find e every models different notebook. its blog and YouTube video related to it. There you can find every models related to computer vision in greater detail and how you can implement it yourself.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome back to my tutorial. Today we are diving into something really exciting and important in the world of computer vision deep sort which stand for simple online and realtime tracking with a deep association metric. So what exactly deep sort? Well, imagine you are watching a crowded street from a security camera. You see people walking around, some disappearing behind camera and other coming back into view. Now your job is to keep track of each person to know what the person who disappeared behind that car is the same person who came out on the other side. That's essentially what multiple object tracking is all about. It's like uh giving each object in a video its own unique ID and making sure we don't lose track of who is who. So this is incredibly important in so many real world application. Think about surveillance system in airports. Autonomous car that needs to track credits uh people and other vehicles or even sports analytics where we track player through a throughout a game. The ability of constantly identifying and follow object is crucial for these system to work properly. Now before deep sort came along researcher were struggling with a major problem. The earlier tracking method especially the original sort algorithm had this annoying habit of losing track of object. Imagine you are following someone in a crowd they disappear behind a pillar for few seconds and when you when they came back out you think they are completely different person that exactly what was happening. These system were exper experiencing what we call identity switches where they lose track of who is who. Today we are going to break down the research paper that solved this problem and revolutionized object tracking. We will understand the",
      "clever technique and that the author used look at their result and at the end I will show you how to implement this yourself. All right, let's dive into the research paper. The first thing we need to understand is the context of this worker. Multiple object tracking has been around for quite some time, but it really took off with the rise of tracking by detection method. Now, what does tracking by detection mean? Think of it like instead of trying to track objects from the very first frame, we first detect all object in each frame and then we try to connect these detection across frame. It's like taking snapshot of a room every second and then trying to figure out which person in snapshot 2 is the same as the person in snapshot one. Before deep sort, there was the original sort algorithm introduced in 2016. Sort was actually pretty good. It was fast, simple, and worked in real time. But it had one major weakness. The biggest issue with sort was frequent identity switches. Let me explain this with a simple example. Imagine you are trying to track two people walking in a hallway. Imagine person A is wearing a red shirt, person B is wearing a blue shirt. They are walking towards each other and for a movement they overlap. one person temporarily block the other from camera view. Now when they separate and both become visible again sort would sometimes get confused and think that the person in red shirt is actually person B and the person in blue shirt is person A. It's basically strapped their identities. This happened because S only looked at motion in formaker. It tried to predict uh where object would be based on their previous movement but it",
      "completely ignored what the objects actually look like. The technical term for this is that the sort relied solely on bounding boxes overlap using something called intersection over union or IOU. Think of IOU as a measure of how much rectangles overlap. If they uh overlap a lot the system assume they are the same objects but when object get occluded that means blog from view this approach fails miserably. So so how did the deep sort of author solve this? They came up with a brilliant idea. Why don't we teach the system to remember what object look like? This is where the deep in deep sort come from. They use deep learning to give the tracking system a visual memory. Their approach combined two types of information. First motion information. They kept the good part of sort the calman filter and the Hungarian algorithm. Now a calvin filter is like smart predictor that say based on where this object was how fast it was moving here where I think it would be next. The Hungarian algorithm is like a matchmaker that pair us prediction with actual detection in most optimal way. Second, appearance information. This is the game changer. They added a CNN that that is a type of deep learning model that's really good at understanding image. This CNN looks at each objected object and create what we call an appearance descriptor. Think of it as a unique fingerprint for how that object look. The CNN they use is pretty sophisticated. It's wide uh residual network or a type of neural network architecture that really good at learning complex pattern. It helps to describe what the object look like. Maybe some numbers represent color, other represent shape, texture, and so on. Here's a clever part. For each",
      "object being triggered, the system remembers the last 100 appearance descriptor. It's like having a photo album for each person you are tracking. When a new detection comes in, the system look at the new detection and compared it with all the photo album to see which one matches best. So how this fix the previous problem? The combination of motion and appearance information is like having both a GPS and your eyes when you are trying to find someone in a crowd. The GPS motion prediction tells you roughly where the uh they should be but your eyes appearance feature confirm that you are looking at the right person. When object get occluded remember that mean blocked from view the motion prediction became uh unreliable but the appearance feature remains con consistent so the object reappears the system can say here I remember what this p person look like and currently maintain their identity. The author also introduced something called a matching cascade algorithm. Instead of trying to solve everything at one once, they prioritize track based on their age. Think of it like if you have been successfully tracking someone for a long time. They're probably more confident about who they are compared to someone you just started tracking. So the system gives priority to these more established track. Let's see the numbers. The author tested their method on something called MOT 16 benchmark. This is like standardized test for tracking algorithm. They use several metrics to measure performance. Deep sort achieve 61.4% MOTA compared to SS 59.8%. In identity switches, this was the big win. Remember this was the main problem they were trying to solve. Deep sort of reduce identity switches by 45%. From uh now how what about processing speed? Despite all this additional complexity,",
      "Deepshot still runs at 40 Hz means it can process 40 frames per second which is more than enough for realtime application. The benefits are pretty clear. First, you get much more reliable tracking. 45% fewer identity situation means your system is much less likely to get confused about who is who. Second, it handles application much better. Third, it maintain realtime performance so you can actually use it in practical application. Uh limitation, let's be honest about limitation two. First, you need a decent GPU to run this in real time because of CNN component. Second, it does produce more false positive. Sometimes it's a bit too easier to maintain track. Third, the CNN was trained specifically on people. So, it might not work on uh other tracking objects like car without retraining. So from my perspective from a practical standpoint deep sort is more of an intelligent evolution rather than a revolutionary breakthrough. The author were smart. They took what worked from sort and added the missing piece visible memory which solved a real problem that uh engineers were facing. This 45% reduction in identity switches is impressive and addresses a real world need. However, the approach is fundamentally incremental. It combine existing technique rather than introducing completely new algorithm. But sometime that exactly what the field need practical solution that works. And there you have it. We have covered the complete deep sort research paper and problem it solved. Uh deep sort that sometimes the best solution comes from combining different approaches intelligently by adding wizard memory to memory based tracking. They created a system that much more robust and reliable. Uh the key takeaway here is the that appearance information is crucial for robust tracking. When motion prediction fail having a visual memory of what object",
      "look like can save the day. So uh in next part of this tutorial we are going to dive into the actual code implementation. Let's see how you can implement deep sort on your own. First we start by installing required libraries. uh we have to install ultralytic for object detection as we are going to use yolo and real uh deep sort real time it's a library through which you can perform deep sort with few code so I we just have to run this code now now in deep sort I'm using uh an embedder which is a reidentification ation means a CNN which help in identification of object. So this is I'm going to use the collector get now our next step I'm creating a helper function. This helper function help me uh download sample video which which I am going to use in this tutorial and visualize it in the notebook. Now let's come to our main function deep sort. First we import dolo from ultralytics and then from deep sort real time from deep sort tracker we import deep sort. Then we import cv2 o and a random to generate random color for each bounding boxes. Now in deep sort algorithm I'm providing with a model yolo 11 next. Then uh here as you can see I initialize deep sort tracker. This tracker has a maximum age argument means how much frame it will look before predicting the next frame. How much objects are required to start the tracking means and minimum number of objects after tracking started is two. and then embed which is a reidentification identity which helps me extract feature from object to keep track of it uh which is uh click vit I'm using and these are uh arguments you will",
      "needed for deep sort now as you can see I'm you taking frames by frame and then providing frames to our model and then for each detection I am appending then value to detection list and then this is the main code for deep salt using deep using deep tracker I updating tracker based on its detection and frame and taking that detection tracker is confirm I am drawing bounding boxes using this part of code and then I am providing with each ID The ID is already provided by the tracker algorithm. Then I am saving this into our uh into a video format. Now let's run this code. Let's track people first. This is the sample video I'm going to implement in this tutorial. Uh these this is a video of two people jogging jogging. So it's a really simple video and a very easy tracking video for a tractor. Let's implement deep sorting. You have seen the result of deep sort on this video. As you see it work very great on these type of video where object is moving in a linear path. Now let's take an example of another video. Let's try playing. Now let's see the uh sample video I'm using. This is a a video of a multiple planes flying across sky. As you can see, these are traveling in a curved part which make it harder for an object tracker to track it. Let's see the result of deep sort on it. As you can see, I'm providing with the part of the video and the output name and the target class which is four. Four is the target class of plane in YOLO. Let's run this code. As you have now seen the result of deep south on a plane, it",
      "was a pretty pretty good result as you know that the plane has been uh very much clearly detected and it was the right identity provided to each object. Now let's see the edge cases of deep thought. This is a video I'm going to use. Uh in this video you can see the car is moving in a straight line but the problem with uh it is that car which is the object is becoming bigger by every frame which make it difficult for tracker to detect uh like calvin filter to predict is the same as previous objects. Uh you will see what will happen in this type of video. Let's see the influence of deepot on this video. As you seen in the inference video, the result was not pretty much good. The objective tracking was very bad and the identity kept switching. This was a edge cases or the limitation of carman filter which has been used in uh deep sort. It also failed to recognize the same object which is moving forward in different frame. Uh due to uh the limitation of feature extraction with this our tutorial on deep sort is over. To learn more you can check our git repository where you can find e every models different notebook. its blog and YouTube video related to it. There you can find every models related to computer vision in greater detail and how you can implement it yourself."
    ],
    "transcript_word_count": 2344,
    "transcript_chunk_count": 8
  },
  {
    "video_id": "64Ncymcfpsk",
    "title": "Track Crowds in Real-Time with FairMOT",
    "description": "Learn how FairMOT revolutionizes real-time multi-object tracking with a balanced detection and re-identification approach.\n\nIn this tutorial, we explain the key concepts, challenges, and implementation of FairMOT using its official GitHub repository. Perfect for AI and computer vision learners, this guide helps you understand the strengths and limitations of the FairMOT model with hands-on walkthroughs.\n\nResources:\nBLOG: https://www.labellerr.com/blog/implementing-fairmot-tutorial/\n\nFixes Scripts: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/FairMot\nOfficial GitHub: https://github.com/ifzhang/FairMOT\n\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n\n#FairMOT #MultiObjectTracking #ComputerVision #ObjectDetection #AI #MachineLearning #DeepLearning #RealTimeTracking #TrackingAlgorithm #YOLO #MOTChallenge #PythonAI #VisionAI #GitHubProject #AIimplementation #AnchorFreeDetection #ReIdentification #PeopleTracking #AIResearch #MOTTutorial",
    "video_url": "https://www.youtube.com/watch?v=64Ncymcfpsk",
    "embed_url": "https://www.youtube.com/embed/64Ncymcfpsk",
    "duration": 990,
    "view_count": 191,
    "upload_date": "20250629",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone, welcome back. Today we are going to see an amazing tracking method called pair mode. I am currently on its official GitHub repository page for pair mode and as you can see it's quite popular with over 4100 stars. So this repository uh contains the official implementation of research paper titled fair mod on the fairness of detection and reidentification in multiple object tracking. What make this project special is that it provide a simple yet effective approach to multiple object tracking which is the task of detecting multiple objects in a video and tracking them across frame. The repository includes all the code you need to train and test the pair model and even provide pre-tained models that that achieve state-ofthe-art result on several benchmark data set. Before uh diving into the code implementation, let's understand what this research paper is about and why it's important in the field of computer vision. I will break down the key concept in a simple term so everyone can follow along. So fair mode focus on multiple object tracking or MOT for short. MOT is a computer vision task where we need to detect multiple objects in a video and track them as they move across different frames. Think of it like watching a busy street and keeping track of all the people walking by. Remembering who is who even when they across part or get temporarily hidden behind something. The core contribution of fair mode is a novel approach that tackles the author call unfairness issue between two important task. Object detection means finding where objects are are and reidentification or re ID for short which is recognizing the same object across different frames. Uh previous method tend to favor the detection task over the task leading to a problem in tracking performance. Previous problems author were facing was like before pair mod researcher were dealing with several challenges in multiple object tracking. First there were uh two main approaches to two-step method. This used to this used separate models for detection and reid making the whole system computationally expensive and too slow for realtime application. Imagine running two different computer programs simultaneously. It takes a lot of processing power. The and another was the one sort tracker approach. This tried to combine both task in a single network but had several unfairness issue uh like anchorbased detection cause problem. Uh these tracker use predefined boxes called anchor. Uh let me see yes anchors to detect object. But this created ambiguity for reidentification when multiple object appear in one anchor or when one object span multiple anchor like this. Uh it's like trying to identify people using a grid on your camera. Sometime a person might be split between grid cell or multiple people might be in one cell or another issue was also feature sharing conflict like uh detection needs high level feature to classify object like is this a person or a car while reidentification needs more detailed feature to distinguish between similar objects like is this a person A or a person B. this computing needs create uh created a conflict in shared network. So the author author proposed a novel approach uh in fair mode. Fair mode introduces several innovative design choices to address these issues like uh anchor free design. Instead of using predefined anchor boxes, Fairmont detect object as points their center and then predicts their size. This eliminates the ambiguity problem of anchorbased method. Uh it's like trying to uh trying to identify people by their center position rather than trying to fit them into a predefined boxes. Uh another suggestion author uh gave in Fairmont is homogeneous branch architecture. Like Fairmont uses two parallel branches for similar structure of one for detection uh and one for reidentification. This treat both task equally rather than making one subordinate to other. So, so how the new method fix the previous problem? Uh, fair mode effectively address the previous issue in several way like uh like uh the anchor free approach eliminates identity ambiguity by extracting uh reidentification feature only at object center. This is a much cleaner than messy anchorbased approach. uh the homogeneous branch design treats uh detection and reidentification equally removing the bias towards detection and these are the result of the fair mode in MOT data set. So uh fair mode offers several key advantage like state-of-the-art accuracy. It ranked first among multiple object tracking challenge benchmark. It also uh reduces in identity switches like up to 63% fewer identity switches compared to previous method. Another was its realtime performance. 30fps inference enables practical deployment in real world scenarios. It also reduces the computational uh requirement like the single network approach reduces redundant computer comput computation compared to two-step method. So despite its strength, fair mode does have some limitation like fair mode has these limitation like multiple task multitask optimization remains complex and may require careful tuning for different scenarios like all tracking by detection method. Performance is fundamentally limited by detection quality. uh and it also has a limitation like some parameter needs to fine-tune for different domains while improved uh handling prolong or extreme occision like when an object are hidden behind other object remains challenging for fair mode currently. So in my viewpoint there are two key point I understand for fair mod like first it is strength fair represent a well executed engineering solution that systematically address known problem in one sort tracking the anchor free approach is a novel and experimental validation is comprehensive but it significantly reduction in identity switches which is approximately 63% demonstrate real practical value but it's second my second approach to fair mode is like its technical concern like while effective and is somewhat incremental incremental uh essentially fair mode is like combining existing technique which is centeret multitasking learning uncertainity loss uh rather than introducing fundamentally new concept the fair fair fairness framing while catchy is more about balance optimization rather than novel theoretical approach. So in conclusion uh fair mode approach is clever solution to the fairness problem in multi-object tracking that achieve a state-of-the-art result while maintaining realtime performance. Now let's move to its code implementation to see how you can perform it on your own. We will explore the GitHub repository in detail and set up the environment and run some demos to see fair mode in action. center. With this our tutorial on fair mode is over. You can check our blog on fair mode. Track crowds in real time with fair mode. A detailed tutorial. In this blog, I have explained how you can implement fair mode on uh any video especially for people tracking in crowded area. So check it out.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome back. Today we are going to see an amazing tracking method called pair mode. I am currently on its official GitHub repository page for pair mode and as you can see it's quite popular with over 4100 stars. So this repository uh contains the official implementation of research paper titled fair mod on the fairness of detection and reidentification in multiple object tracking. What make this project special is that it provide a simple yet effective approach to multiple object tracking which is the task of detecting multiple objects in a video and tracking them across frame. The repository includes all the code you need to train and test the pair model and even provide pre-tained models that that achieve state-ofthe-art result on several benchmark data set. Before uh diving into the code implementation, let's understand what this research paper is about and why it's important in the field of computer vision. I will break down the key concept in a simple term so everyone can follow along. So fair mode focus on multiple object tracking or MOT for short. MOT is a computer vision task where we need to detect multiple objects in a video and track them as they move across different frames. Think of it like watching a busy street and keeping track of all the people walking by. Remembering who is who even when they across part or get temporarily hidden behind something. The core contribution of fair mode is a novel approach that tackles the author call unfairness issue between two important task. Object detection means finding where objects are are and reidentification or re ID for short which is recognizing the same object across different frames. Uh previous method tend to favor the detection task over the task leading to",
      "a problem in tracking performance. Previous problems author were facing was like before pair mod researcher were dealing with several challenges in multiple object tracking. First there were uh two main approaches to two-step method. This used to this used separate models for detection and reid making the whole system computationally expensive and too slow for realtime application. Imagine running two different computer programs simultaneously. It takes a lot of processing power. The and another was the one sort tracker approach. This tried to combine both task in a single network but had several unfairness issue uh like anchorbased detection cause problem. Uh these tracker use predefined boxes called anchor. Uh let me see yes anchors to detect object. But this created ambiguity for reidentification when multiple object appear in one anchor or when one object span multiple anchor like this. Uh it's like trying to identify people using a grid on your camera. Sometime a person might be split between grid cell or multiple people might be in one cell or another issue was also feature sharing conflict like uh detection needs high level feature to classify object like is this a person or a car while reidentification needs more detailed feature to distinguish between similar objects like is this a person A or a person B. this computing needs create uh created a conflict in shared network. So the author author proposed a novel approach uh in fair mode. Fair mode introduces several innovative design choices to address these issues like uh anchor free design. Instead of using predefined anchor boxes, Fairmont detect object as points their center and then predicts their size. This eliminates the ambiguity problem of anchorbased method. Uh it's like trying to uh trying to identify people by their center position rather than trying",
      "to fit them into a predefined boxes. Uh another suggestion author uh gave in Fairmont is homogeneous branch architecture. Like Fairmont uses two parallel branches for similar structure of one for detection uh and one for reidentification. This treat both task equally rather than making one subordinate to other. So, so how the new method fix the previous problem? Uh, fair mode effectively address the previous issue in several way like uh like uh the anchor free approach eliminates identity ambiguity by extracting uh reidentification feature only at object center. This is a much cleaner than messy anchorbased approach. uh the homogeneous branch design treats uh detection and reidentification equally removing the bias towards detection and these are the result of the fair mode in MOT data set. So uh fair mode offers several key advantage like state-of-the-art accuracy. It ranked first among multiple object tracking challenge benchmark. It also uh reduces in identity switches like up to 63% fewer identity switches compared to previous method. Another was its realtime performance. 30fps inference enables practical deployment in real world scenarios. It also reduces the computational uh requirement like the single network approach reduces redundant computer comput computation compared to two-step method. So despite its strength, fair mode does have some limitation like fair mode has these limitation like multiple task multitask optimization remains complex and may require careful tuning for different scenarios like all tracking by detection method. Performance is fundamentally limited by detection quality. uh and it also has a limitation like some parameter needs to fine-tune for different domains while improved uh handling prolong or extreme occision like when an object are hidden behind other object remains challenging for fair mode currently. So in my viewpoint there are two key point I understand for fair mod like first",
      "it is strength fair represent a well executed engineering solution that systematically address known problem in one sort tracking the anchor free approach is a novel and experimental validation is comprehensive but it significantly reduction in identity switches which is approximately 63% demonstrate real practical value but it's second my second approach to fair mode is like its technical concern like while effective and is somewhat incremental incremental uh essentially fair mode is like combining existing technique which is centeret multitasking learning uncertainity loss uh rather than introducing fundamentally new concept the fair fair fairness framing while catchy is more about balance optimization rather than novel theoretical approach. So in conclusion uh fair mode approach is clever solution to the fairness problem in multi-object tracking that achieve a state-of-the-art result while maintaining realtime performance. Now let's move to its code implementation to see how you can perform it on your own. We will explore the GitHub repository in detail and set up the environment and run some demos to see fair mode in action. center. With this our tutorial on fair mode is over. You can check our blog on fair mode. Track crowds in real time with fair mode. A detailed tutorial. In this blog, I have explained how you can implement fair mode on uh any video especially for people tracking in crowded area. So check it out."
    ],
    "transcript_word_count": 1130,
    "transcript_chunk_count": 4
  },
  {
    "video_id": "hviocgahbpc",
    "title": "StrongSORT Object Tracking Tutorial with YOLOv7",
    "description": "Learn how to implement the advanced StrongSORT multi-object tracking algorithm with YOLOv7 and OSNet for real-time video analytics.\n\nIn this tutorial, we cover StrongSORT’s improvements over DeepSORT, including AFLink and GSI modules, and guide you step-by-step on setting it up, running it on your own videos, and tracking specific classes like people or vehicles. Perfect for surveillance, sports, and research applications.\n\nCheck the official GitHub repo and start tracking today!\n\nResources:\n- Blog: https://www.labellerr.com/blog/objects-tracking-using-strongsort/\n- Official GitHub: https://github.com/dyhBUPT/StrongSORT\n- Github with YOLO object detection: https://github.com/mikel-brostrom/Yolov7_StrongSORT_OSNet\n- Research Paper: https://arxiv.org/abs/2202.13514\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#StrongSORT #YOLOv7 #ObjectTracking #MultiObjectTracking #DeepSORT #ComputerVision #TrackingAlgorithm #PythonTracking #SurveillanceAI #VideoAnalytics #YOLO #AFLink #GSI #MachineLearning #AI #OpenSourceAI #SecurityCameraAI #VisualTracking #CVResearch #RealtimeTracking",
    "video_url": "https://www.youtube.com/watch?v=hviocgahbpc",
    "embed_url": "https://www.youtube.com/embed/hviocgahbpc",
    "duration": 1054,
    "view_count": 180,
    "upload_date": "20250625",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "Watch how you can track people moving through a crowd or athletes sprinting down the track. And you can also learn a track planes soaring through the sky by the end of this tutorial. Hello everyone, welcome back. Today I'm going to implement you one of the best tracking algorithm in multitracking, strong sort. Let's start by taking a quick look at its official GitHub repo. You can find it at this where you will see active development and ton of updates and everything you need to get started with this tracker. There are even package and demo versions available if you want to try it with different detection models or in your own project. So what is strong sort? In simple terms, strong sort is a upgraded version of the classic deep sort tracker. It it does this by upgrading it to detect objects, how it tells them apart and how it connects the dot state move through each frame. on on top of that it adds two clever modules AF link and gsi that help it handle tricky situation like mist detection or identity switches. Uh now let's dive into a quick review of strong sword research paper uh called strong sort make deep sword great again. So what exactly is strong sort? Imagine you are watching a security footage and you want the computer to keep track of every person even as they move around, disappear behind object or come back into view. Strong sort is a new algorithm that does exactly that but but way better than before. It takes classic deep sort trackers and with smart detection, better recognition and more reliable tracking. Plus it has two modules called AFlink and GSI to handle those tricky movements when tracking usually fails. We will learn about these two later in video. So in this uh research paper the the the previous algorithm has two major flaws like the author saw that the that older trackers like deep shot struggle with two big issues like sometimes the object would get split into multiple tracks if it disappeared and came back. That's called missing association. Like for example picture someone walking behind a pillar. the tracker loses them and when they reappear starts a brand new track. Now you have got two tracks for the same person which is not a good tracking. Other times the tracker would just miss an object for a few frames and breaking the tracking chain. That's missing detection. This breaks the flow and make it tough to follow the same person smoothly. And the third problem, older tracker use outdated detector and simple models in crowded scene. They got confused fast. So strong sword uses a novel approach to fix this issue. First, it uses a modern object detector called YOLO X. This thing is sharp and rarely misses a percept. Next, it swaps out the feature extractor from ONET, which is better at telling people apart even if they look similar for or for predictive movements. Stockshot uses an improved mo motion model that handles sudden moves and occasion much better. And instead of just tracks in a basic way, it uses smarter strategies to keep identifying straight identity straight. But the real game changer are two new modules called AF and GSI. AF link connects sort tracklets. Tracklets is basically a sort tracks between five or six frame generally into full tracks without even needing to know what the person look like. It just uses motion and position. So it's fast and reliable. And another one is gsi. GSI steps in when tracker misses someone for a few frames. It uses math called Gaussian process regression for the for to fill in the gaps and keep track smooth. It uh strong sort has limitation like it does require more computing power than the simplest trackers. So if you need to run it on a tiny device, you might hit a wall. Uh also like most tracker, it still depend on how good the object detector is. If the detector fails, tracking will do in really crowded scenes or when people are hidden a lot. Even strong sort get confused. And while those new module are designed to work anywhere, sometimes you will need to tune them for different situation. But the benefit of strong sword is far greater. Like it handled miss uh missing detection and identity switches way better than before. It sets a new standard for comparing tracking algorithm which helps everyone in the field field and the new modules are plug and play. Just drop them into other tracker. No retraining needed and most importantly it keeps track of multiple object even in tough uh scenarios. So strong sort is a big leap forward for tracking people and objects in video. It fixes old uh problems, add clever new tools and sets a new bar for the field. Uh if you are working in video analytics security or anything that needs to follow object over time, you should definitely check it out. To implement strong sword easily, you just need to come to its original repository. There is a repository address given below where you can implement a strong sort with YOLO with very few step. You just have to click this which will open this repository. As you can see this repository show how you can implement strong sort with YOLO 7 with ONET very easily. These steps you have to follow. So you can implement strong sword on your own. Let's follow these steps. First you have to clone this. Before cloning you have to make a repository for it. I will make a repository called strong sword. Then I also created a cond environment where you can uh install all the requirement. It is very handy in handling uh mini environment. So I have already created an environment for strong. So we I have just to activate it. Now I will go to that directory I have created. As you can see now I am the directory. As you can see it's empty right now. Let's clone it. Clone the uh git repo. So our first step is clear. So our next step will be to install the requirements library. So let's check. As you can see, this is the G repo we have cloned into our local system. Let's go to this repo. Now let's see what are the files in our repo. These are the files which are in our report. Let's install the requirements. I'm going to use pip for it. This is the command you have to use to install the requirements. As you can see all the requirements have been downloaded. Uh now our next step will be to which command line is to pro uh use to inference uh strong sort on our video. As you can see in the repo, there are various way you can print using strong sort in various ways like you can directly track uh on a webcam, image video directory, globe, YouTube video uh and ETA or and also you can change your models you want to use like YOLO 7 or a YOLO 7X or any other YOLO model. You can also add uh strong strong sword with OSNet which is very great for object tracking when you require very high level of accuracy as these are the feature extraction which help identify frames by frames which object are same. So you can check this out. There is also a method you can only track specific classes like using this you can track only uh cats and dogs using cocoa data set as in copo data set 16 and 17 uh numbers are for cats and dog. So please check it out. I am going to implement the best uh scenario for uh to implement strong sort like I am using the best command you can use to implement strong sort on your own. Let's check it out to perform a strong sort on a sample video. I have selected some sample video. This is a sample video which is uh uses in every multiple object tracking as a standard. As you can see this is a video containing lots of people going in various direction. I have chosen some other videos like uh like this video of athletes running on a track. I have chosen some videos like of a plane. This is a sample video of a plane. As you can see there are the total of five plane. Let's see how you can implement uh strong sword on these videos. To implement strong sword we first have to see which file I'm going to use. The file I'm going to use is call track. As you can see, this is the file I am going to use. So, first you have to write python track py then provide the source you are going to use. the sources uh as the video I'm going to use the video is in date uh data dot uh mp4 then you have to provide the yolo weights you are going to use I'm using yolo Hello 77 V7 PT then the sort uh uh strong sort like I using a feature extraction called ONET which you can provide using this you can you provide using this oet This will help in uh our strong s tracking make our uh tracking accuracy much more uh smoother. So I'm going to save that video. Uh to save video I'm going to use this. Then for classes I have to choose the people class. The people class in YOLO is as Coco data set is zero. Now we have written the command line. Let's run this command line. As you can see it's running the code. As you can see various peoples are getting track use in this video. [Music] So as you can see our tracking is done and the result has been saved in runs tracks XP. As you can see a new folder has been created called runs and this is the let's see what our result. Similarly you can inference using strong phone or various other videos just by changing the uh source video and the classes you need to track. Let's see other inferencing result. With this our tutorial on strong sort tracking is over. In this tutorial you have learned how you can implement strong sort on your own on local devices. To learn more you can check our blog on strong sort. In this blog we have tell you about how you can implement strong sword with much more accuracy and efficiency. Please check it out.",
    "transcript_chunks": [
      "Watch how you can track people moving through a crowd or athletes sprinting down the track. And you can also learn a track planes soaring through the sky by the end of this tutorial. Hello everyone, welcome back. Today I'm going to implement you one of the best tracking algorithm in multitracking, strong sort. Let's start by taking a quick look at its official GitHub repo. You can find it at this where you will see active development and ton of updates and everything you need to get started with this tracker. There are even package and demo versions available if you want to try it with different detection models or in your own project. So what is strong sort? In simple terms, strong sort is a upgraded version of the classic deep sort tracker. It it does this by upgrading it to detect objects, how it tells them apart and how it connects the dot state move through each frame. on on top of that it adds two clever modules AF link and gsi that help it handle tricky situation like mist detection or identity switches. Uh now let's dive into a quick review of strong sword research paper uh called strong sort make deep sword great again. So what exactly is strong sort? Imagine you are watching a security footage and you want the computer to keep track of every person even as they move around, disappear behind object or come back into view. Strong sort is a new algorithm that does exactly that but but way better than before. It takes classic deep sort trackers and with smart detection, better recognition and more reliable tracking. Plus it has two modules called AFlink and GSI to handle those tricky movements when tracking usually fails. We will learn about",
      "these two later in video. So in this uh research paper the the the previous algorithm has two major flaws like the author saw that the that older trackers like deep shot struggle with two big issues like sometimes the object would get split into multiple tracks if it disappeared and came back. That's called missing association. Like for example picture someone walking behind a pillar. the tracker loses them and when they reappear starts a brand new track. Now you have got two tracks for the same person which is not a good tracking. Other times the tracker would just miss an object for a few frames and breaking the tracking chain. That's missing detection. This breaks the flow and make it tough to follow the same person smoothly. And the third problem, older tracker use outdated detector and simple models in crowded scene. They got confused fast. So strong sword uses a novel approach to fix this issue. First, it uses a modern object detector called YOLO X. This thing is sharp and rarely misses a percept. Next, it swaps out the feature extractor from ONET, which is better at telling people apart even if they look similar for or for predictive movements. Stockshot uses an improved mo motion model that handles sudden moves and occasion much better. And instead of just tracks in a basic way, it uses smarter strategies to keep identifying straight identity straight. But the real game changer are two new modules called AF and GSI. AF link connects sort tracklets. Tracklets is basically a sort tracks between five or six frame generally into full tracks without even needing to know what the person look like. It just uses motion and position. So it's fast and reliable. And another one is gsi. GSI steps",
      "in when tracker misses someone for a few frames. It uses math called Gaussian process regression for the for to fill in the gaps and keep track smooth. It uh strong sort has limitation like it does require more computing power than the simplest trackers. So if you need to run it on a tiny device, you might hit a wall. Uh also like most tracker, it still depend on how good the object detector is. If the detector fails, tracking will do in really crowded scenes or when people are hidden a lot. Even strong sort get confused. And while those new module are designed to work anywhere, sometimes you will need to tune them for different situation. But the benefit of strong sword is far greater. Like it handled miss uh missing detection and identity switches way better than before. It sets a new standard for comparing tracking algorithm which helps everyone in the field field and the new modules are plug and play. Just drop them into other tracker. No retraining needed and most importantly it keeps track of multiple object even in tough uh scenarios. So strong sort is a big leap forward for tracking people and objects in video. It fixes old uh problems, add clever new tools and sets a new bar for the field. Uh if you are working in video analytics security or anything that needs to follow object over time, you should definitely check it out. To implement strong sword easily, you just need to come to its original repository. There is a repository address given below where you can implement a strong sort with YOLO with very few step. You just have to click this which will open this repository. As you can see this repository show how you can",
      "implement strong sort with YOLO 7 with ONET very easily. These steps you have to follow. So you can implement strong sword on your own. Let's follow these steps. First you have to clone this. Before cloning you have to make a repository for it. I will make a repository called strong sword. Then I also created a cond environment where you can uh install all the requirement. It is very handy in handling uh mini environment. So I have already created an environment for strong. So we I have just to activate it. Now I will go to that directory I have created. As you can see now I am the directory. As you can see it's empty right now. Let's clone it. Clone the uh git repo. So our first step is clear. So our next step will be to install the requirements library. So let's check. As you can see, this is the G repo we have cloned into our local system. Let's go to this repo. Now let's see what are the files in our repo. These are the files which are in our report. Let's install the requirements. I'm going to use pip for it. This is the command you have to use to install the requirements. As you can see all the requirements have been downloaded. Uh now our next step will be to which command line is to pro uh use to inference uh strong sort on our video. As you can see in the repo, there are various way you can print using strong sort in various ways like you can directly track uh on a webcam, image video directory, globe, YouTube video uh and ETA or and also you can change your models you want to use like YOLO 7 or",
      "a YOLO 7X or any other YOLO model. You can also add uh strong strong sword with OSNet which is very great for object tracking when you require very high level of accuracy as these are the feature extraction which help identify frames by frames which object are same. So you can check this out. There is also a method you can only track specific classes like using this you can track only uh cats and dogs using cocoa data set as in copo data set 16 and 17 uh numbers are for cats and dog. So please check it out. I am going to implement the best uh scenario for uh to implement strong sort like I am using the best command you can use to implement strong sort on your own. Let's check it out to perform a strong sort on a sample video. I have selected some sample video. This is a sample video which is uh uses in every multiple object tracking as a standard. As you can see this is a video containing lots of people going in various direction. I have chosen some other videos like uh like this video of athletes running on a track. I have chosen some videos like of a plane. This is a sample video of a plane. As you can see there are the total of five plane. Let's see how you can implement uh strong sword on these videos. To implement strong sword we first have to see which file I'm going to use. The file I'm going to use is call track. As you can see, this is the file I am going to use. So, first you have to write python track py then provide the source you are going to use. the sources uh",
      "as the video I'm going to use the video is in date uh data dot uh mp4 then you have to provide the yolo weights you are going to use I'm using yolo Hello 77 V7 PT then the sort uh uh strong sort like I using a feature extraction called ONET which you can provide using this you can you provide using this oet This will help in uh our strong s tracking make our uh tracking accuracy much more uh smoother. So I'm going to save that video. Uh to save video I'm going to use this. Then for classes I have to choose the people class. The people class in YOLO is as Coco data set is zero. Now we have written the command line. Let's run this command line. As you can see it's running the code. As you can see various peoples are getting track use in this video. [Music] So as you can see our tracking is done and the result has been saved in runs tracks XP. As you can see a new folder has been created called runs and this is the let's see what our result. Similarly you can inference using strong phone or various other videos just by changing the uh source video and the classes you need to track. Let's see other inferencing result. With this our tutorial on strong sort tracking is over. In this tutorial you have learned how you can implement strong sort on your own on local devices. To learn more you can check our blog on strong sort. In this blog we have tell you about how you can implement strong sword with much more accuracy and efficiency. Please check it out."
    ],
    "transcript_word_count": 1788,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "0gJjJ2P08GE",
    "title": "ByteTrack Tracking Tutorial: Perform Tracking with YOLO",
    "description": "In this step‑by‑step ByteTrack tutorial, you’ll learn how ByteTrack’s two‑step matching algorithm recovers high and low‑confidence detections for robust multi‑object tracking.\n\nWe’ll cover the research behind ByteTrack, installation of Ultralytics and OpenCV, setting up YOLO models, and implementing ByteTrack in Python. Follow along as we demo tracking people, vehicles, and more, even through heavy occlusions.\n\nResources:\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/ByteTrack\nBlog: https://www.labellerr.com/blog/how-to-implement-bytetrack/\nResearch Paper: https://arxiv.org/abs/2110.06864\n\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#ByteTrack #MultiObjectTracking #MOT #ComputerVision #YOLO #OpenCV #Ultralytics #PythonTutorial #TrackingAlgorithm #DeepLearning #ObjectDetection #KalmanFilter #VideoAnalytics #AI #MachineLearning #ObjectTracking #MultiObjectTracking #ComputerVision #YOLO #Ultralytics #MachineLearning #DeepLearning #AI #VideoAnalytics #TrackingAlgorithm #Python #OpenCV #SecurityCameras #SelfDrivingCars #SportsAnalytics #OcclusionHandling #RealTimeTracking #Tutorial #TechEducation",
    "video_url": "https://www.youtube.com/watch?v=0gJjJ2P08GE",
    "embed_url": "https://www.youtube.com/embed/0gJjJ2P08GE",
    "duration": 840,
    "view_count": 358,
    "upload_date": "20250625",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "[Music] Hello everyone, welcome to this tutorial. Today we are going to explore about one of the most famous multiple object tracking method called by track. A simple but super effective way to track multiple objects in video. When by track was first introduced, it quickly became famous because it solved a big problem that other trackers struggle with. missing object when they were hard to see or got blocked by something else. Instead of ignoring these tricky cases, Bitra uses almost every detection it can, making it super reliable even in crowded or messy scenes. So before going into its implementation, we go through its research paper and learn how it works. Let's get started. So the research paper of by track is called by track multiobject tracking by associating every detection box. So what exactly is bite track? By track is a smart algorithm designed for multiple object tracking. That means it can follow a lot of moving things like people, car or animal in a video by drawing boxes around them and keeping their identity straight as they move around. What make by track stand out is its simple idea. Instead of uh only trusting the boxes it's really sure about, it tries to use almost every detection box even the one it it's less less sure about. This help it track better especially when they are hard to see or get blocked by something else. So what problem did older tracker have which by track solve? Now let's talk about the problem older tracking method faced by previously. First, most old tracker would only keep boxes if the computer was very confident that they were real. If the computer wasn't sure, maybe because the bo objects was hidden or blurry, it would just throw these boxes away. This mean that real object would be missed, especially when they were occluded or not easy to see. Occlusion means when an object overlaps another object. So one object is unable to see during viewer's point of view. So it also causes problem like losing track of object when they are disappeared for a movement and then giving them a new ID when they come back making it hard to follow the same object through the whole video. So how does bike track fix this? by trap fix this issue by clever uh two-step process. First, it matches all the high confidence boxes to existing track. This is similar to what old trackers did. Then it takes the boxes with lower confidence and tries to match them to this is the key difference. By not throwing away this these boxes by track can recover object that were hidden or hard to see for a moment. It uses a prediction tool called Calvin filter to guess where each object should be. Maybe it even better at keeping track of them even if they disappear for a few frame. So what's new and special about track is the big innovation by track doesn't w any potential useful information by using both high or low confidence detection. It can track object more reliably especially in crowded or tricky scenes. This approach is simple but it works so well that it's now considered state-ofthe-art. In fact, Bit Track has ranked first among several major tracking benchmark beating of older or more complicated method. But with uh with newer technique there are some limitation. It can still struggle with small objects or in extremely crowded scene where lots of things overlap. One limitation is that it uses fixed threshold to decide what's high or low confidence. This number might need to be adjusted for different video or situation which can be bit tricky and it's mainly designed for tracking one type of object at a time not multiple types together but overall uh by track has some major benefits. It's super accurate running at real time speed even on a big video. It makes far fewer mistakes like losing track of object or m mixing up the their identities. It's simple to use and can be added to other tracking uh system to make them better and it works really well in real world scenario like security camera, self-driving car. Let's see some example. As you can see these are the example in which occlusion happens and and due to by track algorithm its ID remains same. As you can see the yellow triangle is going and then it got oluded but it still do not lose it lose its ID you using by track tracking. Same with hair as the person occluded for some times it uh it's ID became redu red triangle but again it came back to yellow triangle. These are the few examples you can check around. So the bite tracking is a simple but powerful way to track a lots of moving objects in video using every bit of information it can get. If you are interested in computer vision, this is the tracking algorithm you should check out. Let's get to its implementation. Now let's see how you can implement by track. There are various way you can use by track using it gets github or ultralytics. I'm going to show you the easiest way you can implement by track on your video which is the ultralytics method. In this method you will have an object detection algorithm which is yolo and the by track algorithm together in one place. So you don't need to install any other requirements for that. First we need to install some uh libraries which is ultralytics and open CV. Then in our next step I'm going to create a helper function which help me show you some video on which I am going to perform object tracking on. So let's run this function. Now this is the main function which you need to create to perform bite tracking. First we need a model which is here yolo atex model. By track can run on all yolo model. So you can choose any model you like. Then you need to create a function in which you will use the model track method. In that in that argument you have to provide with or video part and the by track do yml name. This is the alphalytics built-in by track configuration and then you just have to loop it through every frame and print out the every object ID it detect and see what happened. Let's run this. Now this is a sample video which we are going to use by drag corner. This video is pretty much a simple video of two people jogging. There are no occlusion or anything else. The video is simple and very clear. So there will be no problem in tracking. Let's perform by track. As you can see in every frame there are two ids are generated. There are two objects. So there are two ids are generated. If there are more object, more ids will be generated. And the the tracking algorithm duty is to keep these ID uh in check. So that if tracking tracker ID one is representing this object. So tracker ID in next frame should also represent this function. And this will happen throughout the video. So the track if that happen tracking algorithm is successfully implemented. If not tracking algorithm is failed to track the object. As you can see throughout the frame both tracker ID remain constantly on the same object which is between this box. As you can see every tracker ID is nearly start with 54 which is representing the this object in the video and it's performing good. Now let's create another function. This is the function I have created using our previous by track function. In this function, I have added some more features like now the video will be annotated and saved in our local file system. As you can see this part will draw bonding boxes around the uh around the object which is with same ID and this will save video in our local file system. So let's run this function. Now let's take a look at our another sample video which we are going to use object tracking on. This is the video we are going to use object tracking on. This is a complex video of some players playing footballs. It has lots of occasion small objects and large object and an object which pattern you can't generally predict it. So let's see how by track perform. As you have seen by track tracking on players next next video should be a different tracking object. So let's take this video. This video is of a playing plane flying across the sky which is a very hard object to be tracked like there are various small objects and you can't see which object is going which direction. Let's see how by track perform in this kind of situation. As you have seen the result of by track tracking on a plane. Let's take a object which is very simple and we will see it in everyday life like this object. This is an uh video of a highway in which multiple car are passing through. This is an everyday tracking object you need in uh a security situation. So let's see how well by track perform on these kind of uh video like like because it is difficult to track a small pixel becoming a large object. Uh let's see how by track works here. As we have seen how by track works in different scenarios. Now you can implement bite track on your own. With this our tutorial on by track implementation is over. To read about more on by track you can check our website in which we have written a blog on how to implement by track for multiobject tracking in which we have explained it more in detail how you can implement by track using both method ultralytics and GitHub. Please check it out.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome to this tutorial. Today we are going to explore about one of the most famous multiple object tracking method called by track. A simple but super effective way to track multiple objects in video. When by track was first introduced, it quickly became famous because it solved a big problem that other trackers struggle with. missing object when they were hard to see or got blocked by something else. Instead of ignoring these tricky cases, Bitra uses almost every detection it can, making it super reliable even in crowded or messy scenes. So before going into its implementation, we go through its research paper and learn how it works. Let's get started. So the research paper of by track is called by track multiobject tracking by associating every detection box. So what exactly is bite track? By track is a smart algorithm designed for multiple object tracking. That means it can follow a lot of moving things like people, car or animal in a video by drawing boxes around them and keeping their identity straight as they move around. What make by track stand out is its simple idea. Instead of uh only trusting the boxes it's really sure about, it tries to use almost every detection box even the one it it's less less sure about. This help it track better especially when they are hard to see or get blocked by something else. So what problem did older tracker have which by track solve? Now let's talk about the problem older tracking method faced by previously. First, most old tracker would only keep boxes if the computer was very confident that they were real. If the computer wasn't sure, maybe because the bo objects was hidden or blurry, it would just throw these boxes",
      "away. This mean that real object would be missed, especially when they were occluded or not easy to see. Occlusion means when an object overlaps another object. So one object is unable to see during viewer's point of view. So it also causes problem like losing track of object when they are disappeared for a movement and then giving them a new ID when they come back making it hard to follow the same object through the whole video. So how does bike track fix this? by trap fix this issue by clever uh two-step process. First, it matches all the high confidence boxes to existing track. This is similar to what old trackers did. Then it takes the boxes with lower confidence and tries to match them to this is the key difference. By not throwing away this these boxes by track can recover object that were hidden or hard to see for a moment. It uses a prediction tool called Calvin filter to guess where each object should be. Maybe it even better at keeping track of them even if they disappear for a few frame. So what's new and special about track is the big innovation by track doesn't w any potential useful information by using both high or low confidence detection. It can track object more reliably especially in crowded or tricky scenes. This approach is simple but it works so well that it's now considered state-ofthe-art. In fact, Bit Track has ranked first among several major tracking benchmark beating of older or more complicated method. But with uh with newer technique there are some limitation. It can still struggle with small objects or in extremely crowded scene where lots of things overlap. One limitation is that it uses fixed threshold to decide what's high or",
      "low confidence. This number might need to be adjusted for different video or situation which can be bit tricky and it's mainly designed for tracking one type of object at a time not multiple types together but overall uh by track has some major benefits. It's super accurate running at real time speed even on a big video. It makes far fewer mistakes like losing track of object or m mixing up the their identities. It's simple to use and can be added to other tracking uh system to make them better and it works really well in real world scenario like security camera, self-driving car. Let's see some example. As you can see these are the example in which occlusion happens and and due to by track algorithm its ID remains same. As you can see the yellow triangle is going and then it got oluded but it still do not lose it lose its ID you using by track tracking. Same with hair as the person occluded for some times it uh it's ID became redu red triangle but again it came back to yellow triangle. These are the few examples you can check around. So the bite tracking is a simple but powerful way to track a lots of moving objects in video using every bit of information it can get. If you are interested in computer vision, this is the tracking algorithm you should check out. Let's get to its implementation. Now let's see how you can implement by track. There are various way you can use by track using it gets github or ultralytics. I'm going to show you the easiest way you can implement by track on your video which is the ultralytics method. In this method you will have an object detection algorithm",
      "which is yolo and the by track algorithm together in one place. So you don't need to install any other requirements for that. First we need to install some uh libraries which is ultralytics and open CV. Then in our next step I'm going to create a helper function which help me show you some video on which I am going to perform object tracking on. So let's run this function. Now this is the main function which you need to create to perform bite tracking. First we need a model which is here yolo atex model. By track can run on all yolo model. So you can choose any model you like. Then you need to create a function in which you will use the model track method. In that in that argument you have to provide with or video part and the by track do yml name. This is the alphalytics built-in by track configuration and then you just have to loop it through every frame and print out the every object ID it detect and see what happened. Let's run this. Now this is a sample video which we are going to use by drag corner. This video is pretty much a simple video of two people jogging. There are no occlusion or anything else. The video is simple and very clear. So there will be no problem in tracking. Let's perform by track. As you can see in every frame there are two ids are generated. There are two objects. So there are two ids are generated. If there are more object, more ids will be generated. And the the tracking algorithm duty is to keep these ID uh in check. So that if tracking tracker ID one is representing this object. So tracker ID in",
      "next frame should also represent this function. And this will happen throughout the video. So the track if that happen tracking algorithm is successfully implemented. If not tracking algorithm is failed to track the object. As you can see throughout the frame both tracker ID remain constantly on the same object which is between this box. As you can see every tracker ID is nearly start with 54 which is representing the this object in the video and it's performing good. Now let's create another function. This is the function I have created using our previous by track function. In this function, I have added some more features like now the video will be annotated and saved in our local file system. As you can see this part will draw bonding boxes around the uh around the object which is with same ID and this will save video in our local file system. So let's run this function. Now let's take a look at our another sample video which we are going to use object tracking on. This is the video we are going to use object tracking on. This is a complex video of some players playing footballs. It has lots of occasion small objects and large object and an object which pattern you can't generally predict it. So let's see how by track perform. As you have seen by track tracking on players next next video should be a different tracking object. So let's take this video. This video is of a playing plane flying across the sky which is a very hard object to be tracked like there are various small objects and you can't see which object is going which direction. Let's see how by track perform in this kind of situation. As you have seen",
      "the result of by track tracking on a plane. Let's take a object which is very simple and we will see it in everyday life like this object. This is an uh video of a highway in which multiple car are passing through. This is an everyday tracking object you need in uh a security situation. So let's see how well by track perform on these kind of uh video like like because it is difficult to track a small pixel becoming a large object. Uh let's see how by track works here. As we have seen how by track works in different scenarios. Now you can implement bite track on your own. With this our tutorial on by track implementation is over. To read about more on by track you can check our website in which we have written a blog on how to implement by track for multiobject tracking in which we have explained it more in detail how you can implement by track using both method ultralytics and GitHub. Please check it out."
    ],
    "transcript_word_count": 1676,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "ZmIRl8FV7SQ",
    "title": "Getting Started with Labellerr 🚀 | Sign Up, Upload Data & Add Users in Minutes!",
    "description": "Hey everyone! 👋\nIn this quick walkthrough, I’ll show you how to get started with Labellerr — from signing in with Google to creating a project, uploading your first dataset, and even inviting users to your team. Whether you’re an ML engineer or project manager, this guide will help you launch your first labeling project with ease!\n\nStart your AI data journey today with Labellerr!\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n#Labellerr #DataAnnotation #MachineLearningTools #ImageLabeling #AnnotationPlatform #AIWorkflow #MLProjects #DatasetManagement #AIAnnotation #TeamCollaboration",
    "video_url": "https://www.youtube.com/watch?v=ZmIRl8FV7SQ",
    "embed_url": "https://www.youtube.com/embed/ZmIRl8FV7SQ",
    "duration": 220,
    "view_count": 37,
    "upload_date": "20250624",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "annotation platforms",
      "labeling software",
      "dataset creation",
      "computer vision",
      "annotation software",
      "annotate images",
      "machine learning",
      "cvat guide",
      "annotation workflow",
      "data preprocessing",
      "image segmentation",
      "image labeling",
      "image markup",
      "annotation methods",
      "segmentation software",
      "image annotation",
      "image processing",
      "data tagging",
      "markup software",
      "deep learning",
      "image tagging",
      "cvat tutorial",
      "ai tagging",
      "segmentation techniques"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "Hey everyone. Today I'll walk you through how to get started with labellerr. From signing up, creating a workspace and project to uploading data sets and adding users to your team. Let's begin by signing in with Google. Once you log in, you'll see the welcome screen. In the side panel, you'll find options like projects and data sets. Since this is a new account, nothing is listed here yet. You can go to the data sets tab to view your own data sets under my data sets or explore publicly available data sets to test the tool. You'll also find the API keys section. You can create one just by clicking the add new key button. Back on the home screen, you'll find useful resources like product documentation to guide you step by step, access to upload data programmatically, blogs for updates and best practices, book a demo option, product updates for the latest features. Let's create a project now. But first, we need to upload a data set. Choose the type of data. I'll pick image and hit next. You can upload from multiple sources. I'll use the local upload option and select some files. Once done, give your data set a name and description and click on create data set. Now, let it finish uploading. Next, you'll define annotation guidelines. Start by adding objects and choosing annotation types for each. Now, switch to the classification tab to add questions. These help the model better understand your data. You can even select a model to assist with these questions. I'll go with Gemini 2.0 Flash. Then click next. Save this setup as a template if you want to reuse it later. Now you can choose how you want to create the project using labellerr AI or human labeling. In this case, I'll select human labeling and click view to open the project dashboard. Let's add a user to this project. Click on settings, then hover over the side panel and choose users. You'll see your current user listed. Now, click create user and fill in their details. assign them a project and define their role like annotator, reviewer, or super admin. To confirm, I'll open the email invite, click on the project link, and I'm taken right to the project dashboard. Now, I'll check the assigned role, and yep, it's super admin, just like we said earlier. That's it. You've now learned how to sign up, create a project, upload a data set, and invite users. Try it yourself and streamline your data labeling process with labellerr. Thanks for watching.",
    "transcript_chunks": [
      "Hey everyone. Today I'll walk you through how to get started with labellerr. From signing up, creating a workspace and project to uploading data sets and adding users to your team. Let's begin by signing in with Google. Once you log in, you'll see the welcome screen. In the side panel, you'll find options like projects and data sets. Since this is a new account, nothing is listed here yet. You can go to the data sets tab to view your own data sets under my data sets or explore publicly available data sets to test the tool. You'll also find the API keys section. You can create one just by clicking the add new key button. Back on the home screen, you'll find useful resources like product documentation to guide you step by step, access to upload data programmatically, blogs for updates and best practices, book a demo option, product updates for the latest features. Let's create a project now. But first, we need to upload a data set. Choose the type of data. I'll pick image and hit next. You can upload from multiple sources. I'll use the local upload option and select some files. Once done, give your data set a name and description and click on create data set. Now, let it finish uploading. Next, you'll define annotation guidelines. Start by adding objects and choosing annotation types for each. Now, switch to the classification tab to add questions. These help the model better understand your data. You can even select a model to assist with these questions. I'll go with Gemini 2.0 Flash. Then click next. Save this setup as a template if you want to reuse it later. Now you can choose how you want to create the project using labellerr AI or human labeling.",
      "In this case, I'll select human labeling and click view to open the project dashboard. Let's add a user to this project. Click on settings, then hover over the side panel and choose users. You'll see your current user listed. Now, click create user and fill in their details. assign them a project and define their role like annotator, reviewer, or super admin. To confirm, I'll open the email invite, click on the project link, and I'm taken right to the project dashboard. Now, I'll check the assigned role, and yep, it's super admin, just like we said earlier. That's it. You've now learned how to sign up, create a project, upload a data set, and invite users. Try it yourself and streamline your data labeling process with labellerr. Thanks for watching."
    ],
    "transcript_word_count": 432,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "M67Ormc1ASc",
    "title": "Track Annotation Quality with Ease! ✅ Introducing Labellerr’s Quality Metrics Dashboard",
    "description": "Hey everyone! 👋\nIn this video, learn how to use Labellerr’s Quality Metrics dashboard to monitor annotator and reviewer performance, track client rejections, and improve overall data labeling quality. View detailed stats, accuracy rates, and even download everything as a CSV.\nStart managing your labeling teams smarter and faster!\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n#DataAnnotation #QualityMetrics #AnnotationTools #AIAnnotation #Labellerr #ReviewDashboard #AnnotationWorkflow #MLTools #AnnotationManagement #LabelingPlatform",
    "video_url": "https://www.youtube.com/watch?v=M67Ormc1ASc",
    "embed_url": "https://www.youtube.com/embed/M67Ormc1ASc",
    "duration": 118,
    "view_count": 24,
    "upload_date": "20250620",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "annotation software",
      "computer vision annotation",
      "annotation platforms",
      "data labeling software",
      "deep learning",
      "image segmentation",
      "image annotation",
      "data science",
      "data tagging",
      "tagging software",
      "training data",
      "dataset creation",
      "image labeling",
      "data annotation tools",
      "dataannotation.tech starter assessment",
      "data annotation tech",
      "artificial intelligence",
      "ai",
      "computer vision",
      "data labeling",
      "data labeling service",
      "image labeling tool"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "Hey everyone, did you know you can check quality metrics on our dashboard? Let me show you how. Open any recent project and you'll see an option called quality metrics. Here you'll find three tabs: annotator, reviewer, and client rejected. Let's go over each one quickly. After setting a date range, this section shows how each annotator is performing, including speed, quality, and accuracy. Shows the user's name and email. Total files annotated by the annotator. Number of files accepted with no changes. Files the reviewer rejected for poor quality. Files the client rejected after review. Files the annotator skipped. Percentage of accepted files out of total annotated. Reviewer tab. This one focuses on how reviewers are performing. Reviewer's name and email. Total files reviewed in the given range. Files accepted during review. Files marked as rejected. Files rejected by the client. Files skipped by the reviewer. shows review accuracy based on accepted reviews. Client rejected tab. This section helps you trace which files the client rejected and why. Unique ID of the rejected file. Annotator who worked on the file. Who checked the file? Client's reason or feedback for rejecting the file. Date the file was rejected. You can also download all this as a CSV. Try it out today and take full control of your annotation quality with labellerr.",
    "transcript_chunks": [
      "Hey everyone, did you know you can check quality metrics on our dashboard? Let me show you how. Open any recent project and you'll see an option called quality metrics. Here you'll find three tabs: annotator, reviewer, and client rejected. Let's go over each one quickly. After setting a date range, this section shows how each annotator is performing, including speed, quality, and accuracy. Shows the user's name and email. Total files annotated by the annotator. Number of files accepted with no changes. Files the reviewer rejected for poor quality. Files the client rejected after review. Files the annotator skipped. Percentage of accepted files out of total annotated. Reviewer tab. This one focuses on how reviewers are performing. Reviewer's name and email. Total files reviewed in the given range. Files accepted during review. Files marked as rejected. Files rejected by the client. Files skipped by the reviewer. shows review accuracy based on accepted reviews. Client rejected tab. This section helps you trace which files the client rejected and why. Unique ID of the rejected file. Annotator who worked on the file. Who checked the file? Client's reason or feedback for rejecting the file. Date the file was rejected. You can also download all this as a CSV. Try it out today and take full control of your annotation quality with labellerr."
    ],
    "transcript_word_count": 221,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "LdbRXYWVyN0",
    "title": "Explore Your Dataset with Labellerr’s AI: Tags, Descriptions, Object Counts & More!",
    "description": "You can now use Labellerr AI’s under your Dataset section! In this video, we explore publicly available datasets, auto-tagging, object counting, image descriptions, and even custom AI prompts, all in just a few clicks. Try it out and boost your annotation workflow today with Labellerr!\n\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#AIAnnotation #ComputerVision #DatasetTools #ImageAnnotation #AutoTagging #MachineLearning #VisionAI #DeepLearning #AutomatedAnnotation #DatasetAnnotation",
    "video_url": "https://www.youtube.com/watch?v=LdbRXYWVyN0",
    "embed_url": "https://www.youtube.com/embed/LdbRXYWVyN0",
    "duration": 106,
    "view_count": 20,
    "upload_date": "20250618",
    "uploader": "Labellerr",
    "tags": [
      "image annotation tool",
      "data annotation tech",
      "dataannotation.tech starter assessment",
      "cvat tool",
      "annotation software",
      "annotation platforms",
      "data annotation jobs",
      "image segmentation",
      "labeling platform",
      "machine learning",
      "quality assurance",
      "computer vision",
      "cvat tutorial",
      "dataset creation",
      "image labeling",
      "image markup",
      "data preprocessing",
      "manual annotation",
      "cvat guide",
      "image annotation",
      "labeling software",
      "image processing",
      "data labeling software",
      "artificial intelligence"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "Hey everyone. Today I'm going to show you an exciting new feature. We've added our labellerr AI in the data set section of our tools. You can see we've added a couple of publicly available data sets here. These are great if you just want to try out our tool without uploading your own data. I'll select the AD20K data set for this demo. You can open it by either clicking view files or the 100 files button on the data set card. Let these load for a second. All right, here's a sample from the original AD20K data set. I'll choose one of the images. Let's go with this car one. Now, here's the cool part. Look for experiment with labellerr AI on the right. Just click on it. You'll see a few helpful options pop up like identifying tags, writing a short description, counting objects, or even using your own custom prompt. Let's start with tags. Since this is a car image, the tags should reflect that. And yep, they look accurate. Next, let's try the description. Here, we get a short, meaningful description based on the image. And you can quickly verify if it's correct. Now, for object count. This gives us a JSON output showing all the objects and their class names. Finally, let's try something custom. I'll ask it to fetch the number plate of the Chevrolet car. And there you go. We get a focused response. I hope you give this feature a try. It's a great way to explore data sets and experiment with powerful AI tools, all built right into labellerr. Thank you for watching and don't forget to check us out at labelair.com. See you next time.",
    "transcript_chunks": [
      "Hey everyone. Today I'm going to show you an exciting new feature. We've added our labellerr AI in the data set section of our tools. You can see we've added a couple of publicly available data sets here. These are great if you just want to try out our tool without uploading your own data. I'll select the AD20K data set for this demo. You can open it by either clicking view files or the 100 files button on the data set card. Let these load for a second. All right, here's a sample from the original AD20K data set. I'll choose one of the images. Let's go with this car one. Now, here's the cool part. Look for experiment with labellerr AI on the right. Just click on it. You'll see a few helpful options pop up like identifying tags, writing a short description, counting objects, or even using your own custom prompt. Let's start with tags. Since this is a car image, the tags should reflect that. And yep, they look accurate. Next, let's try the description. Here, we get a short, meaningful description based on the image. And you can quickly verify if it's correct. Now, for object count. This gives us a JSON output showing all the objects and their class names. Finally, let's try something custom. I'll ask it to fetch the number plate of the Chevrolet car. And there you go. We get a focused response. I hope you give this feature a try. It's a great way to explore data sets and experiment with powerful AI tools, all built right into labellerr. Thank you for watching and don't forget to check us out at labelair.com. See you next time."
    ],
    "transcript_word_count": 286,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "HBk_hkP6k-I",
    "title": "MASA + SAM: Track ANY Video Object Without Training (2025)",
    "description": "Discover how MASA (Matching Anything by Segmenting Anything) revolutionizes object tracking! This breakthrough computer vision technique tracks ANY object in videos without training data – even unseen objects under challenging conditions.\n\nIn this tutorial, you’ll learn:\n- How MASA solves limitations of traditional trackers (no pre-training required!)\n- The role of SAM (Segment Anything Model) in zero-shot segmentation\n- Step-by-step inference demos on real videos (Minions + face tracking)\n- Command-line implementation walkthrough\n\nResources:\nGitHub: https://github.com/siyuanliii/masa\nBlog: https://www.labellerr.com/blog/matching-anything-by-segment-anything/\nResearch Paper: https://arxiv.org/abs/2406.04221\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n\n\n#MASA #ComputerVision #ObjectTracking #ZeroShot #SAM #SegmentAnything #AI #DeepLearning #VideoAnalysis #MachineLearning #Tutorial #Innovation #TechDemo #AITutorial #ComputerVisionTutorial",
    "video_url": "https://www.youtube.com/watch?v=HBk_hkP6k-I",
    "embed_url": "https://www.youtube.com/embed/HBk_hkP6k-I",
    "duration": 1045,
    "view_count": 74,
    "upload_date": "20250617",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "computer vision",
      "data management",
      "annotation software",
      "data annotation tech",
      "annotation workflow",
      "data quality control",
      "dataset creation",
      "image labeling",
      "data annotation tools",
      "training data quality",
      "labeling software",
      "image classification",
      "quality assurance",
      "image segmentation",
      "data processing",
      "data workflow",
      "data validation",
      "data preprocessing",
      "visual data",
      "data labeling software",
      "annotation platform",
      "image annotation"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "[Music] Hey everyone, today we are diving into a exciting topic in computer vision called MASA that stand for matching anything by segmenting anything. If you ever wondered how computer can follow objects and videos even ones they have never seen before, you are in the right place. Let's start with a problem like uh most object tracking systems can only follow things that they have been trained on. Imagine having a glasses that only can see cats but not dogs. That's how old uh tracking system work. They are stuck with whatever they have learned on. But the but the real world is a messy. There are new object, different lighting and unexpected situation everywhere. That's where MASA comes in. So MASA is a new technology that lets computer track any objects in a video no matter what it is and it doesn't need special training for each things. Pretty cool, right? Uh MASA is built uh on a powerful tool called called SAM, the segment editing model. Think of Sam as a super assistant that can outline any object in a picture automatically. Masa take those outline and learns uh to recognize the same object as is move from one frame to another print even if it changes save size and gets partly covered. So before moving into that we will check what its research paper tell let's see its research paper hey let me walk you through the master research paper in simple term first uh first I want to explain the problem the paper address in computer vision when try to track objects in videos. Most system can only follow things they have already seen during training. So if I train a system to track cars, it might completely ignore a dog or a bicycle in the same video. That's big limitation. Now the MASA which stands for matching anything by segmenting anything changes the game. With MASA I can track any objects in video even if the system has never seen it before. I don't need to train it on a special videos for every possible object. Instead, I can use regular image and MASA will still work. Let me tell you how MASA work. MASA uses a tool called SAM or segment editing model. SAM is like a digital pen that automatically draws outline around any object in picture. Once I have these outlines, Masa learns to match them from one video frame to next. So if I see a yellow ball rolling across the screen, Masa help me to follow it even if it changes shapes size or gets partially hidden behind something. The paper introduce a universal adapted. Let me Yes. The the paper introduced a universal adapter. I like to think of this as a plug that fits any socket with this adapter. I can connect MASA to any object detection system. That mean no matter what detection tool I use, MASA can help me track object across frame. Why is MASA important? because it lets me track objects I have never seen before. This is called zero sort tracking. It also work well with different settings like uh outdoor, indoor or in poor lighting. According to paper, MASA even outperform older method that that need lots of uh level video data. The applications of are huge. I can use MASA for self-driving car, security cameras or robotics basically anywhere. I need to follow moving objects since MASA doesn't need special training for every new objects. It's it's much more practical for real world to to sum up the MASA research paper show me how to teach computer to track the uh track anything in video by smartly matching and segmenting object without needing ton of label data. It's a big step uh forward for computer vision and I think it will make object tracking much easier and more reliable. Let's get to its implementation. Listen. We are going to inference MASA on this sample image. This is the standard sample image provided by MASA GitHub. You can see it's a video of Minions which is a popular uh movie. to run inference uh using masa we just have to use this command line as you can see I'm using python providing the uh address to this uh for uh python script and the video link the means video part to this and the output uh uh the output of that video which is the inference annotated video which is demo output which is the full which currently not exist and also providing other configuration details like masa grounding G dynino masa checkpoints which is the most important as it's providing the uh path to the grounding dyno pta then I'm providing the uh text or you can say label of finding yellow minions which is these objects in this and also the threshold level of 0.2 and these are the other required flags. You can also add the flag to add segmentation mask. Now run this As you can see, it's loading this app in model for inferencing. Let's see there is some error. Oh, we forgot to import this module. First I'm using Python Now it's installed. As you can see now it's influencing our video. It will take some time. Now the inferencing is done. You can see there is a demo output and in that the inference result is there. Let's if you see that if the inferencing sometimes inferencing doesn't work as the folder is not present. You can create the demo output folder then start the command line. It will create the result inside the demo output. But sometime it does it's a bug or bug inside that code. You you can uh comment to iso. So let's run the inference uh result. Let's perform massa inference on another video. As we have already performed segmentation mass, let's only perform tracking. This is the video I'm going to use and this is the parts to that video. It's in demo called saf. I'm going to remove this flag called sam mask to just only performs tracking. Now let's run that. As you can see I have already also given the uh label as face. So it all only detect the face. Now inferencing is happen happening. I will fast forward it. Let's see the result. Similarly, you can inference masa on other video. Let's see other examples. With this our tutorial on MASA is over. You can check our blog on our website in which we explain how you can perform MASA in greater detail and what MASA is and its architecture. Please check it out.",
    "transcript_chunks": [
      "[Music] Hey everyone, today we are diving into a exciting topic in computer vision called MASA that stand for matching anything by segmenting anything. If you ever wondered how computer can follow objects and videos even ones they have never seen before, you are in the right place. Let's start with a problem like uh most object tracking systems can only follow things that they have been trained on. Imagine having a glasses that only can see cats but not dogs. That's how old uh tracking system work. They are stuck with whatever they have learned on. But the but the real world is a messy. There are new object, different lighting and unexpected situation everywhere. That's where MASA comes in. So MASA is a new technology that lets computer track any objects in a video no matter what it is and it doesn't need special training for each things. Pretty cool, right? Uh MASA is built uh on a powerful tool called called SAM, the segment editing model. Think of Sam as a super assistant that can outline any object in a picture automatically. Masa take those outline and learns uh to recognize the same object as is move from one frame to another print even if it changes save size and gets partly covered. So before moving into that we will check what its research paper tell let's see its research paper hey let me walk you through the master research paper in simple term first uh first I want to explain the problem the paper address in computer vision when try to track objects in videos. Most system can only follow things they have already seen during training. So if I train a system to track cars, it might completely ignore a dog or a bicycle in",
      "the same video. That's big limitation. Now the MASA which stands for matching anything by segmenting anything changes the game. With MASA I can track any objects in video even if the system has never seen it before. I don't need to train it on a special videos for every possible object. Instead, I can use regular image and MASA will still work. Let me tell you how MASA work. MASA uses a tool called SAM or segment editing model. SAM is like a digital pen that automatically draws outline around any object in picture. Once I have these outlines, Masa learns to match them from one video frame to next. So if I see a yellow ball rolling across the screen, Masa help me to follow it even if it changes shapes size or gets partially hidden behind something. The paper introduce a universal adapted. Let me Yes. The the paper introduced a universal adapter. I like to think of this as a plug that fits any socket with this adapter. I can connect MASA to any object detection system. That mean no matter what detection tool I use, MASA can help me track object across frame. Why is MASA important? because it lets me track objects I have never seen before. This is called zero sort tracking. It also work well with different settings like uh outdoor, indoor or in poor lighting. According to paper, MASA even outperform older method that that need lots of uh level video data. The applications of are huge. I can use MASA for self-driving car, security cameras or robotics basically anywhere. I need to follow moving objects since MASA doesn't need special training for every new objects. It's it's much more practical for real world to to sum up the MASA research",
      "paper show me how to teach computer to track the uh track anything in video by smartly matching and segmenting object without needing ton of label data. It's a big step uh forward for computer vision and I think it will make object tracking much easier and more reliable. Let's get to its implementation. Listen. We are going to inference MASA on this sample image. This is the standard sample image provided by MASA GitHub. You can see it's a video of Minions which is a popular uh movie. to run inference uh using masa we just have to use this command line as you can see I'm using python providing the uh address to this uh for uh python script and the video link the means video part to this and the output uh uh the output of that video which is the inference annotated video which is demo output which is the full which currently not exist and also providing other configuration details like masa grounding G dynino masa checkpoints which is the most important as it's providing the uh path to the grounding dyno pta then I'm providing the uh text or you can say label of finding yellow minions which is these objects in this and also the threshold level of 0.2 and these are the other required flags. You can also add the flag to add segmentation mask. Now run this As you can see, it's loading this app in model for inferencing. Let's see there is some error. Oh, we forgot to import this module. First I'm using Python Now it's installed. As you can see now it's influencing our video. It will take some time. Now the inferencing is done. You can see there is a demo output and in that the inference",
      "result is there. Let's if you see that if the inferencing sometimes inferencing doesn't work as the folder is not present. You can create the demo output folder then start the command line. It will create the result inside the demo output. But sometime it does it's a bug or bug inside that code. You you can uh comment to iso. So let's run the inference uh result. Let's perform massa inference on another video. As we have already performed segmentation mass, let's only perform tracking. This is the video I'm going to use and this is the parts to that video. It's in demo called saf. I'm going to remove this flag called sam mask to just only performs tracking. Now let's run that. As you can see I have already also given the uh label as face. So it all only detect the face. Now inferencing is happen happening. I will fast forward it. Let's see the result. Similarly, you can inference masa on other video. Let's see other examples. With this our tutorial on MASA is over. You can check our blog on our website in which we explain how you can perform MASA in greater detail and what MASA is and its architecture. Please check it out."
    ],
    "transcript_word_count": 1111,
    "transcript_chunk_count": 4
  },
  {
    "video_id": "mVkPXbWxnEg",
    "title": "Vision Agent Using SAM-Description-Based Object Segmentation Agent | Full-Tutorial",
    "description": "Unlock the power of Grounded SAM for advanced image segmentation using simple text prompts! In this tutorial, you'll learn how to combine Grounding DINO and the Segment Anything Model (SAM) to perform open-vocabulary segmentation on any image. We'll review the official research paper, break down how Grounded SAM works in an easy-to-understand way, and show you how to use it for practical computer vision tasks.\n\nWhat you'll learn:\n- What is Grounded SAM and why is it revolutionary?\n- How does the combination of Grounding DINO and SAM enable text-driven segmentation?\n- Real-world examples: segmenting objects like animals, people, and more with just a text description.\n- How to set up and run Grounded SAM on your own images.\n- Tips for integrating Grounded SAM with other AI models for tasks like image editing and annotation.\n\nWhether you're a beginner or an AI enthusiast, this tutorial will help you get started with the latest in computer vision technology. No need for manual annotation—just type what you want to segment, and let Grounded SAM do the rest!\n\nResources:\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/Vision%20Agent\nBlog: https://www.labellerr.com/blog/vision-agent-using-segment-anything/\n\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n\n\n\n\n#GroundedSAM #ImageSegmentation #ComputerVision #AI #MachineLearning #SegmentAnything #GroundingDINO #OpenVocabularySegmentation #AItutorial #VisionAI #DeepLearning #TextToImage #PythonAI #ResearchPaperReview #AIDemo",
    "video_url": "https://www.youtube.com/watch?v=mVkPXbWxnEg",
    "embed_url": "https://www.youtube.com/embed/mVkPXbWxnEg",
    "duration": 1049,
    "view_count": 107,
    "upload_date": "20250609",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "cvat tool",
      "image segmentation",
      "annotation software",
      "image classification",
      "computer vision",
      "image labeling",
      "labeling software",
      "annotation workflow",
      "labeling solutions",
      "computer vision projects",
      "data annotation software",
      "image annotation",
      "dataset creation",
      "data labeling software",
      "image metadata",
      "data preprocessing",
      "metadata management",
      "machine learning",
      "deep learning",
      "data visualization",
      "visual data",
      "data labeling",
      "image tagging",
      "image processing"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "[Music] Hello everyone, welcome to this tutorial. In this tutorial, we are going to create a vision agent using segment editing. The task of a v vision agent is to use natural language to perform segmentation. It is called grounding uh SAM. Before moving into the implementation using code, first we review its official research paper, let's go to research paper. In this section, we will take a look at the official research paper that introduced grounded SAM to understand what this technology actually does and how it works. Don't worry, I will keep it simple and focus on the big picture rather than getting lost in technical details. Let's open its uh research paper. So it it's called grounded SAM assembling open world models for div diverse visual task. So what is grounded SAM? The researcher created this system by combining two existing AI models. Uh think of it like uh combining two specialized tool to create something more powerful. They took uh a model called grounding do which is really good at finding objects in images when you describe them with uh text and they combine it with another model called SAM which stand for segment editing. Here is a cool part. When you uh combine these two model, you will get something that can find objects and create precision outlines around them all based on simple text description that you type in. So instead of having uh uh to click on objects or draw boxes around them manually, you can just tell the system find the dogs uh or segment the card and it does all the works for you. So how does the system work? The the researcher designed grounded SAM to solve really challenging problem in computer vision. They realized that trying to segment object directly from text description is incredibly uh difficult because you need massive amount of high quality training data. So they came up with a clever two-step approach. First first they use grounding dyno to detect object based on text. This model scans the entire image and draws bonding boxes around anything that matches your description. Then they pass these detected boxes to SAM model which create precision level uh mask around each detected object. It's like having one expert find the objects and another expert trace perfect outlines around them. What makes this approach uh special uh what really impressed me about this uh research is how the author designed the system to work with another other AI uh models. They didn't uh uh they didn't just create an isolated tools. They built something that can connect with others vision uh system to accomplish much more complex task. For example, they showed you to combine grounded SAM with image generation model for precision image editing or with captioning model like blip dot automatically uh uh annotate entire image. The performance result they achieve are pretty remarkable. Like you can see the result of grounded SAM. Uh as you can see if you just input the description it will segment that part of image. Like here you can see it's segmenting iron man. Here you can see it's segmenting uh insect. Here you can see it's segmenting a flower only on description. Uh and here you can see it disc uh segmenting uh do puppies and carpet. So the so this model is highly effective in segmentation and also greatly reduce our segmentation uh load with greater efficiency. So you can also add some more models on it like uh uh like grounded MSG for highly controlled image editing like it had detected the left eye of the car cat and uh added something new to it like this you can see uh grounded sam osx it's detect the human segmented and create a 3D pose of human in its plates Uh as you can see grounded S performance result are pretty much good on challenging benchmark called uh seg in w which test uh which uh which test segmentation in real world uh scenarios. Their systems achieve a 48.7 mean average precision score without any specific training on that data set. That's what we call zero sort performance. The model perform very well on task. It was never specifically trained for it. So it's uh it's uh very great for practic. So what makes grounded particularly valuable is that it's open up uh open vocabulary segmentation to anyone who can type a text description. You don't need uh to be a computer with an expert or spend time manually annotating training data. You just describe what you want to segment and the system handles that is. This research paper demonstrate that sometimes the best innovation comes not from building everything from scratch but thoughtfully combining existing tools in new way. So with this our review on research paper is o over. Let's get to its code implementation. Now let let's come to its code implementation. First we will check with GPU. I am using Nvidia L4 GPU as you can see. Now let's set up the environment. I am going to install some important libraries for this vision agent which is OpenCV plot py mlive numpy torque and transformers which version is this as you can see let's install it as I have installed it I will import these libraries now I imported these libraries in our import bit now our first task is to uh the result which we get from grounded title to be in organized manner too. So we created some classes like bonding box. This will store the values of bonding boxes as a class and you can see this first one is x mean y mean x max yax. It will be in organized manner. So which makes our retrieval easier. Also we will create a class for detection result which will store the values like score label box mask etc. This will make uh retrieval of other datas also easy for our vision agent. Let's run this. Now I'm going to create some uh plot uh utilities like from the result I'm going to annotate that result on image. So I have created a function called annotate which will uh as you can see put rectangle box uh boxes on the image uh targeting the object and also put the uh text on it and uh provide us with the return value of the image which is annotated. As you can also to plot that detection and also save that I have created this function plot detection which will annotate use the previously created function on the annotated and uh show us in the notebook uh notebook uh cell let's run this after that to show segmentation in various color I we have to generate various colors. So this function has a duty to generate various colors for segmentation mark from this data set as you can see. And another is to uh al another function is this plot detection plot. It's a UI based plot detection of libraries which makes interaction with image more beautiful. So I with this you can uh see the result more clearly and understand what object it has detected and what segmentation it has performed. Let's run this. Now this function has a duty to convert mask to polygon. The segmentation which we have generated will be converted to polygon format. Means it will generate an outline of the mask which will make our target uh target object det segmentation more easier and based on that uh polygon outline we will create a more refined mask which is using this as you can see. Let's run this now. Our most important function to load the image in our environment. This will this function take the URL as an argument and give uh download that image from the internet or somewhere else and load it in our environment. Next, this function is to get boxes from detection and store it in a form of list. Let's run this. Now this function has the to define mask means when the SAM model has generated the segmentation mask it has to define in a format which will make visualization more easier. Let's run this. Now the most important part of our vision first we have to detect our objects using uh grounding dial. So we create a detect function which uh uses grounding dino using the labels which we which provided by us like you can see it has using the idea research grounding dino tiny and labels which we have given to it to determ detection on images and give us result and uh let's run this Now on the given result we have to perform segmentation as you can see. So this function takes the detection result as an argument also the image and the segment ID means the segmentation model which we are using and perform segmentation and providing us with the detection result. As you can see now this function combines the both uh both into a singular fun uh function like detection and segment and that detection is displayed on the image as you can see sorry this the detection is return as an image and detection result. Let's run this code. Now let's inference our uh uh our vision agent. First we load image using URL. As you can see I am using a image of a plane for my example. I have provided with a label called airplane. Our threshold is 0.3 and I have provided with the model which I am using which is grounding dyno and SAM vit base. Let's run this. Now I am using grounded segmentation and proed with the image URL uh label threshold refinement is true detector ID and segmented ID. So the model is performing its task. It has performed its Let's plot the detection. As you can see, it is showing us the outline of the object it has detected. As you can see, it has done a great job in a detection and performing outline of the object. Let's perform segmentation on it using the previous segmentation outline. It has created a mask on the object as you can see which is very good and highly accurate for that level of model. So let's perform this in a sequence manner. I have created a function which combine all of these uh steps in a single step. So we can perform this uh uh inference on various other image. Let's run this code. Now load our image. Now I I'm using a image of two elephant. Let's perform uh grounded sam on it. I'm giving it the URL and the label called elephant. Let's see what happen. The model is working. It has detected two elephant and outline that uh elephant using a grounding dyno. And in our next step you can see it has created a mask over it. Our first detection is this object which is integrate outline mask using grounded SAM. Another word is this which is also correct. Let's inference our uh model on other image like two shapes like as you can see this is two shapes in grassland. Let's similarly perform vision agent on it. As you can see it has done the same task on it only only on description of se and the image provided to it and has performed segmentation which is quite nice and accurate. Let's uh perform multiple of types of object detection. As you can see this object contains sheeps and dog. So I have given it uh the URL of the image and the labels called sheeps and dog. Let's perform inference. As you can see it has detected all the objects correctly and outlined it. And the segmentation mask is this. It has detected dog two times which is a downside of that vision agent but the segmentation mask is accurate on board. So uh we can check it later. So with this our tutorial on uh vision agent using segment anything is over. To read more about this vision agent, you can check our blog on our website where I have explained it in more detail how you can create your segment uh segmentation with agent in very easy steps. Please check our uh blog on it.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome to this tutorial. In this tutorial, we are going to create a vision agent using segment editing. The task of a v vision agent is to use natural language to perform segmentation. It is called grounding uh SAM. Before moving into the implementation using code, first we review its official research paper, let's go to research paper. In this section, we will take a look at the official research paper that introduced grounded SAM to understand what this technology actually does and how it works. Don't worry, I will keep it simple and focus on the big picture rather than getting lost in technical details. Let's open its uh research paper. So it it's called grounded SAM assembling open world models for div diverse visual task. So what is grounded SAM? The researcher created this system by combining two existing AI models. Uh think of it like uh combining two specialized tool to create something more powerful. They took uh a model called grounding do which is really good at finding objects in images when you describe them with uh text and they combine it with another model called SAM which stand for segment editing. Here is a cool part. When you uh combine these two model, you will get something that can find objects and create precision outlines around them all based on simple text description that you type in. So instead of having uh uh to click on objects or draw boxes around them manually, you can just tell the system find the dogs uh or segment the card and it does all the works for you. So how does the system work? The the researcher designed grounded SAM to solve really challenging problem in computer vision. They realized that trying to segment object",
      "directly from text description is incredibly uh difficult because you need massive amount of high quality training data. So they came up with a clever two-step approach. First first they use grounding dyno to detect object based on text. This model scans the entire image and draws bonding boxes around anything that matches your description. Then they pass these detected boxes to SAM model which create precision level uh mask around each detected object. It's like having one expert find the objects and another expert trace perfect outlines around them. What makes this approach uh special uh what really impressed me about this uh research is how the author designed the system to work with another other AI uh models. They didn't uh uh they didn't just create an isolated tools. They built something that can connect with others vision uh system to accomplish much more complex task. For example, they showed you to combine grounded SAM with image generation model for precision image editing or with captioning model like blip dot automatically uh uh annotate entire image. The performance result they achieve are pretty remarkable. Like you can see the result of grounded SAM. Uh as you can see if you just input the description it will segment that part of image. Like here you can see it's segmenting iron man. Here you can see it's segmenting uh insect. Here you can see it's segmenting a flower only on description. Uh and here you can see it disc uh segmenting uh do puppies and carpet. So the so this model is highly effective in segmentation and also greatly reduce our segmentation uh load with greater efficiency. So you can also add some more models on it like uh uh like grounded MSG for highly controlled image editing like it",
      "had detected the left eye of the car cat and uh added something new to it like this you can see uh grounded sam osx it's detect the human segmented and create a 3D pose of human in its plates Uh as you can see grounded S performance result are pretty much good on challenging benchmark called uh seg in w which test uh which uh which test segmentation in real world uh scenarios. Their systems achieve a 48.7 mean average precision score without any specific training on that data set. That's what we call zero sort performance. The model perform very well on task. It was never specifically trained for it. So it's uh it's uh very great for practic. So what makes grounded particularly valuable is that it's open up uh open vocabulary segmentation to anyone who can type a text description. You don't need uh to be a computer with an expert or spend time manually annotating training data. You just describe what you want to segment and the system handles that is. This research paper demonstrate that sometimes the best innovation comes not from building everything from scratch but thoughtfully combining existing tools in new way. So with this our review on research paper is o over. Let's get to its code implementation. Now let let's come to its code implementation. First we will check with GPU. I am using Nvidia L4 GPU as you can see. Now let's set up the environment. I am going to install some important libraries for this vision agent which is OpenCV plot py mlive numpy torque and transformers which version is this as you can see let's install it as I have installed it I will import these libraries now I imported these libraries in our import bit now",
      "our first task is to uh the result which we get from grounded title to be in organized manner too. So we created some classes like bonding box. This will store the values of bonding boxes as a class and you can see this first one is x mean y mean x max yax. It will be in organized manner. So which makes our retrieval easier. Also we will create a class for detection result which will store the values like score label box mask etc. This will make uh retrieval of other datas also easy for our vision agent. Let's run this. Now I'm going to create some uh plot uh utilities like from the result I'm going to annotate that result on image. So I have created a function called annotate which will uh as you can see put rectangle box uh boxes on the image uh targeting the object and also put the uh text on it and uh provide us with the return value of the image which is annotated. As you can also to plot that detection and also save that I have created this function plot detection which will annotate use the previously created function on the annotated and uh show us in the notebook uh notebook uh cell let's run this after that to show segmentation in various color I we have to generate various colors. So this function has a duty to generate various colors for segmentation mark from this data set as you can see. And another is to uh al another function is this plot detection plot. It's a UI based plot detection of libraries which makes interaction with image more beautiful. So I with this you can uh see the result more clearly and understand what object it has",
      "detected and what segmentation it has performed. Let's run this. Now this function has a duty to convert mask to polygon. The segmentation which we have generated will be converted to polygon format. Means it will generate an outline of the mask which will make our target uh target object det segmentation more easier and based on that uh polygon outline we will create a more refined mask which is using this as you can see. Let's run this now. Our most important function to load the image in our environment. This will this function take the URL as an argument and give uh download that image from the internet or somewhere else and load it in our environment. Next, this function is to get boxes from detection and store it in a form of list. Let's run this. Now this function has the to define mask means when the SAM model has generated the segmentation mask it has to define in a format which will make visualization more easier. Let's run this. Now the most important part of our vision first we have to detect our objects using uh grounding dial. So we create a detect function which uh uses grounding dino using the labels which we which provided by us like you can see it has using the idea research grounding dino tiny and labels which we have given to it to determ detection on images and give us result and uh let's run this Now on the given result we have to perform segmentation as you can see. So this function takes the detection result as an argument also the image and the segment ID means the segmentation model which we are using and perform segmentation and providing us with the detection result. As you can see",
      "now this function combines the both uh both into a singular fun uh function like detection and segment and that detection is displayed on the image as you can see sorry this the detection is return as an image and detection result. Let's run this code. Now let's inference our uh uh our vision agent. First we load image using URL. As you can see I am using a image of a plane for my example. I have provided with a label called airplane. Our threshold is 0.3 and I have provided with the model which I am using which is grounding dyno and SAM vit base. Let's run this. Now I am using grounded segmentation and proed with the image URL uh label threshold refinement is true detector ID and segmented ID. So the model is performing its task. It has performed its Let's plot the detection. As you can see, it is showing us the outline of the object it has detected. As you can see, it has done a great job in a detection and performing outline of the object. Let's perform segmentation on it using the previous segmentation outline. It has created a mask on the object as you can see which is very good and highly accurate for that level of model. So let's perform this in a sequence manner. I have created a function which combine all of these uh steps in a single step. So we can perform this uh uh inference on various other image. Let's run this code. Now load our image. Now I I'm using a image of two elephant. Let's perform uh grounded sam on it. I'm giving it the URL and the label called elephant. Let's see what happen. The model is working. It has detected two elephant",
      "and outline that uh elephant using a grounding dyno. And in our next step you can see it has created a mask over it. Our first detection is this object which is integrate outline mask using grounded SAM. Another word is this which is also correct. Let's inference our uh model on other image like two shapes like as you can see this is two shapes in grassland. Let's similarly perform vision agent on it. As you can see it has done the same task on it only only on description of se and the image provided to it and has performed segmentation which is quite nice and accurate. Let's uh perform multiple of types of object detection. As you can see this object contains sheeps and dog. So I have given it uh the URL of the image and the labels called sheeps and dog. Let's perform inference. As you can see it has detected all the objects correctly and outlined it. And the segmentation mask is this. It has detected dog two times which is a downside of that vision agent but the segmentation mask is accurate on board. So uh we can check it later. So with this our tutorial on uh vision agent using segment anything is over. To read more about this vision agent, you can check our blog on our website where I have explained it in more detail how you can create your segment uh segmentation with agent in very easy steps. Please check our uh blog on it."
    ],
    "transcript_word_count": 2056,
    "transcript_chunk_count": 7
  },
  {
    "video_id": "4Emb4j1T6-8",
    "title": "SAM2 Tutorial 2025: Meta AI's NEW Segment Anything Model 2",
    "description": "Learn SAM2 (Segment Anything Model 2) - Meta AI's groundbreaking video and image segmentation tool that's 3x more efficient than previous models! In this comprehensive tutorial, you'll master both basic and advanced segmentation techniques using real-world examples.\n\nSAM2 represents a significant leap forward in computer vision technology, offering unprecedented accuracy in both image and video segmentation tasks. This tutorial covers everything from installation to advanced implementation, including the revolutionary streaming architecture that processes video frames with memory attention mechanisms. Unlike its predecessor, SAM2 delivers superior performance while being six times faster, making it ideal for real-time applications and large-scale projects.\n\nWhat You'll Learn:\n- Complete SAM2 installation and setup process\n- Box-based segmentation techniques for precise object detection\n- Point-based segmentation for quick object identification\n\nResources:\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/SAM2\nBlog: https://www.labellerr.com/blog/learn-sam-2-in-minutes/\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#SAM2 #SegmentAnything #MetaAI #ImageSegmentation #VideoSegmentation #ComputerVision #DeepLearning #ArtificialIntelligence #MachineLearning #AIModel #PromptableSegmentation #StreamingInference #RealTimeSegmentation #OpenSourceAI #PyTorch #DataScience #ImageProcessing #VideoProcessing #MaskAnnotation #ObjectSegmentation #FoundationModel #AITutorial #TechTutorial #Innovation #Research #Coding #Python #Tutorial #AI2025",
    "video_url": "https://www.youtube.com/watch?v=4Emb4j1T6-8",
    "embed_url": "https://www.youtube.com/embed/4Emb4j1T6-8",
    "duration": 847,
    "view_count": 420,
    "upload_date": "20250603",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "Hello everyone, welcome to this tutorial. Today we are diving into SAM 2, the segment editing model 2 which is a major leap forward in visible segmentation for both images and videos. Let's break down what makes SAM 2 so powerful and what you can do with it. So before that we will see its official research paper and see what it claim and learn more about SAM in greater detail. Let's go to its official scientific paper. Now let's get to its official research paper. SAM 2 official research paper is called segment anything in images and videos which is uh which is a fact that it's do both segment segmentation in both images and videos. Now let's go to introduction part in this it is telling us about the about SAM model which was good for static images but was not able to segment anything in uh video data which was a problem back then. So SAM 2 was built on SAM one but it added more features to to process a video video segmentation which is quite good. It says like uh if you read it our streaming architecture is natural generalized of SAM to video domain processes video frame one at a time equipped with memory means SAM 2 has added a memory attention module which helped in a segmentation of video frame by frame means it take one uh one frame uh segmentation result in memory and then predict that same segmentation in another frame of video which help in creating a relation between different frames and which object is segmented in the these frames. Let's get how it work. Uh first task is do pro prompt table visual segmentation like we provide pro prompt using boxes points or mask and then it goes to model and model segment uh do segmentation. So there is a prompt encoder which goes to mask decoder that goes to ma memory bank which goes to memory attention which which go through in same loop throughout the video. There is also a image encoder which take each frame of video uh that is go gone through a annotate annotation part that annotate or provide segmentation result. Let's go to another part. Uh it Sam 2 claims that uh so so SAM 2 uh can produce better segmentation accuracy while using 3x fewer interaction that than prior approaches. Further SAM to outperform prior work in established video object segmentation benchmark under multiple evaluation settings and delivers better performance compared to SAM on image segmentation benchmark while being six time faster. So SAM 2 is shown to be effective across variety of videos distribution as observed through numerous zero sort benchmarks including 17 for video segmentation, 37 for single image segmentation. Let's see its related work is SAM 2 which was released in 2023. Let's see this architecture. This is the architecture of SAM 2 model. It take a a frame of video in image encoder that is saved in memory attention which goes to mass decoder which is in which we provide our prompt through mass point and uh boxes that goes to images and perform segmentation of that object. Then it again goes to memory encoded and it goes to memory bank which goes to memory attention. In that way it form a loop and every frame of video is taken as a image but it's happen in a streamlined way which make it's like happening segmentation in real time. These are the result of SAM 2 on video. As you can see, it's good. Now, let's go to its implementation. Let's go through the implementation of SAM to the using this notebook. In this notebook, I will tell each step you need to perform segmentation using SAM 2. First, we are going to install some libraries. These are torque, torque wizard, numpy, matro, opencv, uh, pillow. Then we import those libraries. Let's check the version of each library imported libraries which is pytorch, torque vision and cuda. As you can see, I am using cuda. Now our first major step is to uh download the SAM to GitHub repo from its original uh GitHub repository which is Facebook research samp.git. Now we have installed sam 2.get we will also install a model called uh samto.1 her large. PT which is going to be used in segmentation. Now our model has been downloaded. We will decide what we are using. I'm using CUDA. So our second major step is to import a class called build sam to from sam to build sam and also another class called sam to image predicted from sam to sam to dash image d- predicted and also provide our model to a variable called sam to checkpoint and yml file to its model configuration and provide a build sample class with these arguments and also the device and This argument is store. This this function is stored in SAM to model variable which is stored in argument uh in SAM 2 image predictor which is stored in a variable called predictor. This predictor help us in performing segmentation using SAM 2. Now in this tutorial I am going to explain two ways you can perform segmentation on images. first using S boxes. Let's see it. To perform segmentation using boxes, I'm going to need some helper function. My first helper function is box segmentation, which will is going to need some arguments like image part, box coordinate, model checkpoint, and device. Here I'm going to set my device to KOD. In this part, I'm loading my model in my function which is this model configuration sam 2 sam 2 uh /1.yiml. And this now I'm going to load my image in the image variable using the image part. Now in this part I'm setting my image in a sam to predictor and then performing the uh prediction using this part as you can see it using arguments of boxes and uh and multim mask output is false. Now that mask output is stored in the mask list which I have created and also store the confidence threshold of each mask as you can see. And this is returned as mask list and score. Let's run this. Now I'm going to need a helper function to display my result. So I have created this function to display the mask result of this function using this function. As you can see it is taking image mask as an argument and pro showing the mask over the original image. Let's run this visualize better. I am going to create a function which help you see which boxes I have provided to the uh samu model. So I have created a function to visualize these boxes. Let's run the this function. Now let's take a sample image. I have taken a sample image of two polar beard. Now let's perform segmentation using sam 2 on this coordinate which I have provided this these are the coord box coordinate I have provided to the sam 2 model which is this and this is returning us with mask and so first let's visualize what boxes I have provided to the model as you can see this is the boxes I have provided to the model which is quite accurate Please show the location of uh the object I want to segment. Now let's visualize our result. As you can see it has pro done a great job in segmentation. The both object has been segmented and the mask is showing pretty well. Let's run it other to see in other color. As you can see the segmentation is quite good. Now let's take another image and perform segmentation on this box. Let's visualize the box I have provided to the image. This is the box I have provided to image. And we have to perform segmentation on the horse part, not the rider part. Let's see what happen. As you can see, SAM model SAM 2 model has performed great job in segmenting only the horse part and leaving out the rider part. Let's get to our next method of segmentation which is using point. To perform segmentation using points we have to again create some helper function. Our first helper function is similar as previous to show mask. This is a function I have created to show mask using the uh segmentation mask provided by the model. And then there's a function to display the point I have provided to the model. This is the function which performed that. There's also two function. You can see it. Now run this function. Now let's come to our main function which is point segmentation. This function takes arguments like image, point set, label set, predicted and performs segmentation using samp. Let's run this function. Now let's take our first sample image as an example. This is this was the first sample image we have using boxes criteria. Let's provide with it uh you point and perform segmentation. As you can see, I have given it two points which is this and this to perform segmentation. But the second object segmentation is not quite good using point but it was good using boxes but first object segmentation is perfect. Let's take another sample image. This is the sample image I pro provided in using box section. Let's perform segmentation using point. As you can see it has done a great job in segmentation and mask are not overlapping with rider which I want. With this our tutorial of segmentation using sample is over. To learn in great detail about SAM 2, you can check our blog on SAM 2 which is learn SAM 2 in minutes, the ultimate starter guide for 2025. In this guide, I have explained how you can perform segmentation using SAM 2 in greater deta detail. Please check it out.",
    "transcript_chunks": [
      "Hello everyone, welcome to this tutorial. Today we are diving into SAM 2, the segment editing model 2 which is a major leap forward in visible segmentation for both images and videos. Let's break down what makes SAM 2 so powerful and what you can do with it. So before that we will see its official research paper and see what it claim and learn more about SAM in greater detail. Let's go to its official scientific paper. Now let's get to its official research paper. SAM 2 official research paper is called segment anything in images and videos which is uh which is a fact that it's do both segment segmentation in both images and videos. Now let's go to introduction part in this it is telling us about the about SAM model which was good for static images but was not able to segment anything in uh video data which was a problem back then. So SAM 2 was built on SAM one but it added more features to to process a video video segmentation which is quite good. It says like uh if you read it our streaming architecture is natural generalized of SAM to video domain processes video frame one at a time equipped with memory means SAM 2 has added a memory attention module which helped in a segmentation of video frame by frame means it take one uh one frame uh segmentation result in memory and then predict that same segmentation in another frame of video which help in creating a relation between different frames and which object is segmented in the these frames. Let's get how it work. Uh first task is do pro prompt table visual segmentation like we provide pro prompt using boxes points or mask and then it goes to",
      "model and model segment uh do segmentation. So there is a prompt encoder which goes to mask decoder that goes to ma memory bank which goes to memory attention which which go through in same loop throughout the video. There is also a image encoder which take each frame of video uh that is go gone through a annotate annotation part that annotate or provide segmentation result. Let's go to another part. Uh it Sam 2 claims that uh so so SAM 2 uh can produce better segmentation accuracy while using 3x fewer interaction that than prior approaches. Further SAM to outperform prior work in established video object segmentation benchmark under multiple evaluation settings and delivers better performance compared to SAM on image segmentation benchmark while being six time faster. So SAM 2 is shown to be effective across variety of videos distribution as observed through numerous zero sort benchmarks including 17 for video segmentation, 37 for single image segmentation. Let's see its related work is SAM 2 which was released in 2023. Let's see this architecture. This is the architecture of SAM 2 model. It take a a frame of video in image encoder that is saved in memory attention which goes to mass decoder which is in which we provide our prompt through mass point and uh boxes that goes to images and perform segmentation of that object. Then it again goes to memory encoded and it goes to memory bank which goes to memory attention. In that way it form a loop and every frame of video is taken as a image but it's happen in a streamlined way which make it's like happening segmentation in real time. These are the result of SAM 2 on video. As you can see, it's good. Now, let's go to",
      "its implementation. Let's go through the implementation of SAM to the using this notebook. In this notebook, I will tell each step you need to perform segmentation using SAM 2. First, we are going to install some libraries. These are torque, torque wizard, numpy, matro, opencv, uh, pillow. Then we import those libraries. Let's check the version of each library imported libraries which is pytorch, torque vision and cuda. As you can see, I am using cuda. Now our first major step is to uh download the SAM to GitHub repo from its original uh GitHub repository which is Facebook research samp.git. Now we have installed sam 2.get we will also install a model called uh samto.1 her large. PT which is going to be used in segmentation. Now our model has been downloaded. We will decide what we are using. I'm using CUDA. So our second major step is to import a class called build sam to from sam to build sam and also another class called sam to image predicted from sam to sam to dash image d- predicted and also provide our model to a variable called sam to checkpoint and yml file to its model configuration and provide a build sample class with these arguments and also the device and This argument is store. This this function is stored in SAM to model variable which is stored in argument uh in SAM 2 image predictor which is stored in a variable called predictor. This predictor help us in performing segmentation using SAM 2. Now in this tutorial I am going to explain two ways you can perform segmentation on images. first using S boxes. Let's see it. To perform segmentation using boxes, I'm going to need some helper function. My first helper function is box segmentation,",
      "which will is going to need some arguments like image part, box coordinate, model checkpoint, and device. Here I'm going to set my device to KOD. In this part, I'm loading my model in my function which is this model configuration sam 2 sam 2 uh /1.yiml. And this now I'm going to load my image in the image variable using the image part. Now in this part I'm setting my image in a sam to predictor and then performing the uh prediction using this part as you can see it using arguments of boxes and uh and multim mask output is false. Now that mask output is stored in the mask list which I have created and also store the confidence threshold of each mask as you can see. And this is returned as mask list and score. Let's run this. Now I'm going to need a helper function to display my result. So I have created this function to display the mask result of this function using this function. As you can see it is taking image mask as an argument and pro showing the mask over the original image. Let's run this visualize better. I am going to create a function which help you see which boxes I have provided to the uh samu model. So I have created a function to visualize these boxes. Let's run the this function. Now let's take a sample image. I have taken a sample image of two polar beard. Now let's perform segmentation using sam 2 on this coordinate which I have provided this these are the coord box coordinate I have provided to the sam 2 model which is this and this is returning us with mask and so first let's visualize what boxes I have provided to the",
      "model as you can see this is the boxes I have provided to the model which is quite accurate Please show the location of uh the object I want to segment. Now let's visualize our result. As you can see it has pro done a great job in segmentation. The both object has been segmented and the mask is showing pretty well. Let's run it other to see in other color. As you can see the segmentation is quite good. Now let's take another image and perform segmentation on this box. Let's visualize the box I have provided to the image. This is the box I have provided to image. And we have to perform segmentation on the horse part, not the rider part. Let's see what happen. As you can see, SAM model SAM 2 model has performed great job in segmenting only the horse part and leaving out the rider part. Let's get to our next method of segmentation which is using point. To perform segmentation using points we have to again create some helper function. Our first helper function is similar as previous to show mask. This is a function I have created to show mask using the uh segmentation mask provided by the model. And then there's a function to display the point I have provided to the model. This is the function which performed that. There's also two function. You can see it. Now run this function. Now let's come to our main function which is point segmentation. This function takes arguments like image, point set, label set, predicted and performs segmentation using samp. Let's run this function. Now let's take our first sample image as an example. This is this was the first sample image we have using boxes criteria. Let's provide with it",
      "uh you point and perform segmentation. As you can see, I have given it two points which is this and this to perform segmentation. But the second object segmentation is not quite good using point but it was good using boxes but first object segmentation is perfect. Let's take another sample image. This is the sample image I pro provided in using box section. Let's perform segmentation using point. As you can see it has done a great job in segmentation and mask are not overlapping with rider which I want. With this our tutorial of segmentation using sample is over. To learn in great detail about SAM 2, you can check our blog on SAM 2 which is learn SAM 2 in minutes, the ultimate starter guide for 2025. In this guide, I have explained how you can perform segmentation using SAM 2 in greater deta detail. Please check it out."
    ],
    "transcript_word_count": 1652,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "L9ae7cvJ7Ow",
    "title": "Perform All Vision Tasks using Florence 2",
    "description": "Welcome to this tutorial on Microsoft Florence 2, one of the most advanced AI models for computer vision tasks! In this video, we'll explore how Florence 2 brings together a wide range of vision and vision-language capabilities—including image captioning (simple, detailed, and more detailed), object detection, segmentation, dense captioning, and OCR: using a unified, prompt-based approach.\n\nWe'll walk through the research behind Florence 2, its architecture, and step-by-step code implementation using the Hugging Face Transformers library. You'll learn how to:\n- Generate captions for images at varying levels of detail\n- Detect and recognize objects with bounding boxes and labels\n- Perform dense region captioning to understand object relationships\n- Segment objects at the pixel level for precise visual understanding\n- Use referring expression segmentation and caption-to-face grounding\n- Extract text from images using OCR\n\nFlorence 2 is trained on the massive FLD-5B dataset, featuring 5.4 billion annotations across 126 million images, making it a powerful foundation model for any computer vision application.\n\nWhether you're a researcher, developer, or AI enthusiast, this tutorial will help you get started with Florence 2 and unlock its full potential.\n\nDon't forget to check out the detailed blog post and resources linked in the description for more in-depth information!\n\nRESOURCES:\nBLOG: https://www.labellerr.com/blog/how-to-perform-various-tasks-using-florence-2/\nNOTEBOOK: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/Florence2\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n\n#Florence2 #ComputerVision #AIModel #ImageCaptioning #ObjectDetection #ImageSegmentation #VisionAI #MachineLearning #DeepLearning #OCR #AIProgramming #TechTutorial #HuggingFace #ArtificialIntelligence #UnifiedAI #AITutorial #DataScience #AIResearch #OpenSourceAI",
    "video_url": "https://www.youtube.com/watch?v=L9ae7cvJ7Ow",
    "embed_url": "https://www.youtube.com/embed/L9ae7cvJ7Ow",
    "duration": 1364,
    "view_count": 127,
    "upload_date": "20250530",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "florence 2 model",
      "roboflow",
      "roboflow tutorial"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "[Music] Hello everyone, welcome to this tutorial. Today we are diving into something really exciting like Microsoft Florence 2 which is honestly one of the coolest AI model I have ever seen. If you ever wonder how can we get a single AI model to do basically everything related to computer vision from describing to images to detecting object to segment parts of picture. This is the model you are looking for. Think about it in this way. Imagine having one model that can do any task you like like object detection, image captioning, uh object segmentation and OCR. So before Diving into this tutorial, we will check what's its research paper tell us about it. Now review its research paper. It's named Florence to advancing a unified representation for variety of visas which is a befitting name for this model. Uh it's a novel v a novel vision foundation model with a unified and prompt based representation for variety of computer vision and vision language task means uh it's a novel approach and it take prompt based uh data or images to perform its vision computer vision task or vision language task. Visual language task means it take images and textual prompt both together and and put it in a models to get results. So uh flawless were designed to take uh text form which I have described as text instruction and generate this level result in text form whether it's be captioning object detection rounding or segmentation. uh it's trained on FLD 5B data set uh encompassing of 5.4 billion comprehensive annotation and across 126 million images. So let's get to its architecture. As you can see this is the architecture of Lawrence 2. Uh it's a multitask prompt means it's takes various type of prompts or it means task like object detection segmentation OCR and uh and image then boards it get it's get into embedding it can be visual and text and location embedding then that that result is go to through transformers encoder then transformers decoder and the result is generated either text or location tokens through which we describe what's an image or what based on the task we have provided it like in here you can see person riding a red bicycle on road feature so it's uh image level understandings means it understand a higher level semantics and foster comprehensive understanding of images through linguistic description like Using text prompt it can understand the meaning of the image means semantics of like if a object is on some other object and understand it the image like how the object is in relationship with another image. So another feature is its reason pixel level recognition like facilitated detail object and entity localization between images encapsulation with object and their spatial context like I previously said it's also done in pixel level like it's a very detailed model to understand objects and relationship with its environment like its color and how its relationship with other objects of same category or different category. And another feature is fine grain visual semantic alignment task require fine understanding of both text images. It uh involves locating the image reason that corrects corresponds to text phrases such as object attributes and relation like it's a feature that means like you text the textual description we have given it is also uh has a relationship with the images like if I have detected this object so where is this object is and how it is with other object object using textual uh tech textual description. So let's go to it more. So as you can see how it's work. So you can read about it in our notebook more. Let's get to the uh actual uh code implementation of Florence 2. Let's see how can you perform various vision task using Florence 2 on your own. So we have to first install some libraries which are transformer request torch pillow mattrow liv. most important is transformers here as I am going to use hugging face library to uh import some classes. So let's run this now import those uh modules in our environment. After that I have to choose between CUDA or CPU. If CUDA is level then device will have CUDA and otherwise it will have CPU. So this is the most important class you have to import import from transformers we have to import autoprocessor and auto module for casual LLM. So we create two instances of these class model for automodel for casual LLM from which I import this model Microsoft Lawrence to large. So this is the model you have to import uh to perform Florence 2 on your own and the processor processor is needed to resize the images in the format which model is model required. So let's run this. While running we will in uh download a sample image on which I'm going to show various vision tasks I'm going to perform. So as you can see this is the example image I have taken to perform various vision task forens. As you can see, it contains various uh objects like camera, books, a vase or a bottle and a table and a blur background. So we have to first create the function to perform Florence 2. This is the function I have created. It's take a task prompt in which I will uh uh tell the model which task I have to perform and the image on which I'm going to perform. Uh if that task from require text input I will per give it here or if not it will be a none. So the image is converted into RGB format. The processor is taken the argument of prompt which is the this as you can see as a textual prompt and image as an argument and return the inputs. Then the input is get is gone through a model generated which is uh converted into the model format and that is gone through the batch decoder which have you seen in the architecture. So the that generated test is gone through here as post-process generation after the after the result it's converted that result into the human readable format. So this part converts the model data into human readable format. So and after that the result is parsed as return. Let's run this. So our first vision task is image captioning. As you know image captioning describing what's going in image like if there is an apple on the table it will say there is an apple red apple on the a brown table or xyz let's it in Florence do there are three ways you can cap perform captioning a simple captioning a more detailed captioning or a more detailed caption uh let's see what's happening caption So as you can see a camera sitting on top of a stack of books next to the a vase of flower which is right as you can see there's a camera sitting on a book next to a vase of flower but it doesn't mention anything about the bottle but a bottle is also a primary object here. Let's see what's happening. Detail caption. The image show an old camera sitting a top a stack of books on a table which is uh with a flower beside it. The background is slightly blurred giving the image a dreamy feel. It's tell the semantics of the image quite well like the age of the camera. It's a old camera sitting on a top of books which is right on a table with a flower way beside it which is right and it's also tell about the background of the image like it's a blur background it has said it is slightly blurred so and I what feeling that image is giving means it's giving the image a dreamy feel. Let's see what happened with more detailed caption. So in detail caption it say the image. So a stack of old books on a wooden table. On top of the books there is a vintage camera with a black body uh and a silver lens. Next to the camera there are two glass bottle with labels on them. The bottles are filled with small pink flower. Background is a dark gray wall with a septile textured. The overall mood of image is rustic and vintage. So the first point I mentioned it has mention about the two bottles here which two glass bottle precisely like this detailed caption and caption doesn't mention about two bottle. It's all every time it say about this bottle as a primary object but not about this as so it mention about two glass bottles which was not mentioned previously in any of the captioning uh uh format. So it's a quite good is here. So another thing is that it's give uh generally the same meaning as this but it's also tell about what kind of books like it's a stack of old books. Here it say only a stack of books mean it's telling about the age of the books or also about the wooden table like what texture the table is of. So it's meaning if you have to explain in simple terms it's telling detailed about every object it see like for uh like for books it tell about it's a stack of old books of table it tell of a wooden table a camera it's a vintage camera with black body and silver lens and also the location of camera to the portal so it's give a more detailed description of the image. So it's based on you what kind of captioning you want for your task. If you want a simple captioning like a person is standing in front of door this will be enough. But if you want a more detailed captioning like if two uh young boys are fighting in school background in beside the swing. So more detail captioning will be better option. Now let's get to another uh v vision task of Florence. Our next computer vision task of Florence 2 is object detection and recognition. First we have to create a helper function which help us in visualizing the model output with images. So I will run this function. Now our first task prompt is object except which is represented using OD. I will provide that uh task prompt to give model and run it. I am printing the result of the models. This is giving me the bounding boxes of the object. As you can see these are the bonding and also providing us with the labels of the object it have detected in the image. These are right there are five four instance of books, one instance of portal and one instance of camera and base each. Let's visualize this on image. As you can see uh after visualizing this on image you can see it is done quite well. So the camera the books and the portal and base. Now the next task the next task is dense reason capsing means it's uh let's show you to a visual same as it same as previous in detecting the the bounding boxes and the objects label but it's also telling us the relationship between the uh other box like the other objects like vintage book on wooden table. Let's see it visually. As you can see, it's telling about a visual medical book. It's also telling about vintage book on wooden table with cherry blossom branches in glass bottle. This is the description of this person. This is a discussion of this person. So let's get to our next task prompt which is reason proposal. Let's run this. As you can same as previous but it is only giving us the bounding boxes of the object not label. As you can see labels are empty here. Let's run this. You are saying it's that giving us the information of all the object it has detected but not the labels of the objects. So these are the all the object it has detected currently. Let's get to our next computer vision task. Our next computer vision task of Florence 2 is segmentation. First we have to create a helper function to represent that output visually. I will run this helper function. Now let's get to our task prompt. First our task prompt is referring express and segmentation. In this I have to give the textual input like camera which is the object the image as an argument to our model and task prompt which is referring expression segmentation and image and the result is the segmentation mask of the camera. As you can see it has lots of values in it but we have to visualize it. Let's see what's happen in images. As you can see the object has been detected and clearly segmented by pixel perfect level. Similarly, you can do it with other objects like book. These are the book object in the image. Let's see what happened. As you can see for every instance of book has been segmented. Let's get to our next. Let's take the text input portal. As you can see it has segmented the portal in the image. Now our next will be race. As you can see it has segmented the waist clearly. Next will be cherry blossom branches which are these. Let's segment it. It has segmented a cherry blossom which is uh kind of general segmentation but it's quite good for the model. It is uh next is uh task prompt is caption to phase grounding means uh I have to give a text input of caption what is in the image and it will ground it in the image like if I have the the im the apple is on the table so it will label the t apple and the table together and leave everything else as you can See the a camera an old camera books a table flower and a flower w here it has taken portal as flower base which is currently wrong. So with this the segmentation part is over there. Let's get to our next computer vision task. Our last computer vent task of Florence 2 is OCR. For this I will take another image for a sample. Let's show you the sample image. As you can see this is a s sample image I am choosing. It has a it has a text textual part which is going to be used in OCR detection. Let's give the OCR as a text from the model. As you can see it has read you are another line not another line stuck you are just scared to start. So it has detecting this part clearly and another object another text in also let's see what with the region this is the task for I have to provide OCR with reason and it gives the bonding boxes of the OCR it has detected with it. So to to visualize it, I have to create another helper function. Let's draw the OCR bounding boxes on the image. So if you see it you are saying that you are not stuck you are just here to start and other [Music] ocr and Yes, it has performed good quite well for a allpurpose model. With this our tutorial on how to perform vision task using Florence 2 is over. You can read about it in more detail on our labellerr website. In this blog I have tell in detail about Florence what it is and what other things you can do in greater detail. Please check it out.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome to this tutorial. Today we are diving into something really exciting like Microsoft Florence 2 which is honestly one of the coolest AI model I have ever seen. If you ever wonder how can we get a single AI model to do basically everything related to computer vision from describing to images to detecting object to segment parts of picture. This is the model you are looking for. Think about it in this way. Imagine having one model that can do any task you like like object detection, image captioning, uh object segmentation and OCR. So before Diving into this tutorial, we will check what's its research paper tell us about it. Now review its research paper. It's named Florence to advancing a unified representation for variety of visas which is a befitting name for this model. Uh it's a novel v a novel vision foundation model with a unified and prompt based representation for variety of computer vision and vision language task means uh it's a novel approach and it take prompt based uh data or images to perform its vision computer vision task or vision language task. Visual language task means it take images and textual prompt both together and and put it in a models to get results. So uh flawless were designed to take uh text form which I have described as text instruction and generate this level result in text form whether it's be captioning object detection rounding or segmentation. uh it's trained on FLD 5B data set uh encompassing of 5.4 billion comprehensive annotation and across 126 million images. So let's get to its architecture. As you can see this is the architecture of Lawrence 2. Uh it's a multitask prompt means it's takes various type of prompts or it",
      "means task like object detection segmentation OCR and uh and image then boards it get it's get into embedding it can be visual and text and location embedding then that that result is go to through transformers encoder then transformers decoder and the result is generated either text or location tokens through which we describe what's an image or what based on the task we have provided it like in here you can see person riding a red bicycle on road feature so it's uh image level understandings means it understand a higher level semantics and foster comprehensive understanding of images through linguistic description like Using text prompt it can understand the meaning of the image means semantics of like if a object is on some other object and understand it the image like how the object is in relationship with another image. So another feature is its reason pixel level recognition like facilitated detail object and entity localization between images encapsulation with object and their spatial context like I previously said it's also done in pixel level like it's a very detailed model to understand objects and relationship with its environment like its color and how its relationship with other objects of same category or different category. And another feature is fine grain visual semantic alignment task require fine understanding of both text images. It uh involves locating the image reason that corrects corresponds to text phrases such as object attributes and relation like it's a feature that means like you text the textual description we have given it is also uh has a relationship with the images like if I have detected this object so where is this object is and how it is with other object object using textual uh tech textual description. So let's go to",
      "it more. So as you can see how it's work. So you can read about it in our notebook more. Let's get to the uh actual uh code implementation of Florence 2. Let's see how can you perform various vision task using Florence 2 on your own. So we have to first install some libraries which are transformer request torch pillow mattrow liv. most important is transformers here as I am going to use hugging face library to uh import some classes. So let's run this now import those uh modules in our environment. After that I have to choose between CUDA or CPU. If CUDA is level then device will have CUDA and otherwise it will have CPU. So this is the most important class you have to import import from transformers we have to import autoprocessor and auto module for casual LLM. So we create two instances of these class model for automodel for casual LLM from which I import this model Microsoft Lawrence to large. So this is the model you have to import uh to perform Florence 2 on your own and the processor processor is needed to resize the images in the format which model is model required. So let's run this. While running we will in uh download a sample image on which I'm going to show various vision tasks I'm going to perform. So as you can see this is the example image I have taken to perform various vision task forens. As you can see, it contains various uh objects like camera, books, a vase or a bottle and a table and a blur background. So we have to first create the function to perform Florence 2. This is the function I have created. It's take a task prompt in which I",
      "will uh uh tell the model which task I have to perform and the image on which I'm going to perform. Uh if that task from require text input I will per give it here or if not it will be a none. So the image is converted into RGB format. The processor is taken the argument of prompt which is the this as you can see as a textual prompt and image as an argument and return the inputs. Then the input is get is gone through a model generated which is uh converted into the model format and that is gone through the batch decoder which have you seen in the architecture. So the that generated test is gone through here as post-process generation after the after the result it's converted that result into the human readable format. So this part converts the model data into human readable format. So and after that the result is parsed as return. Let's run this. So our first vision task is image captioning. As you know image captioning describing what's going in image like if there is an apple on the table it will say there is an apple red apple on the a brown table or xyz let's it in Florence do there are three ways you can cap perform captioning a simple captioning a more detailed captioning or a more detailed caption uh let's see what's happening caption So as you can see a camera sitting on top of a stack of books next to the a vase of flower which is right as you can see there's a camera sitting on a book next to a vase of flower but it doesn't mention anything about the bottle but a bottle is also a primary object here. Let's see",
      "what's happening. Detail caption. The image show an old camera sitting a top a stack of books on a table which is uh with a flower beside it. The background is slightly blurred giving the image a dreamy feel. It's tell the semantics of the image quite well like the age of the camera. It's a old camera sitting on a top of books which is right on a table with a flower way beside it which is right and it's also tell about the background of the image like it's a blur background it has said it is slightly blurred so and I what feeling that image is giving means it's giving the image a dreamy feel. Let's see what happened with more detailed caption. So in detail caption it say the image. So a stack of old books on a wooden table. On top of the books there is a vintage camera with a black body uh and a silver lens. Next to the camera there are two glass bottle with labels on them. The bottles are filled with small pink flower. Background is a dark gray wall with a septile textured. The overall mood of image is rustic and vintage. So the first point I mentioned it has mention about the two bottles here which two glass bottle precisely like this detailed caption and caption doesn't mention about two bottle. It's all every time it say about this bottle as a primary object but not about this as so it mention about two glass bottles which was not mentioned previously in any of the captioning uh uh format. So it's a quite good is here. So another thing is that it's give uh generally the same meaning as this but it's also tell about what kind of",
      "books like it's a stack of old books. Here it say only a stack of books mean it's telling about the age of the books or also about the wooden table like what texture the table is of. So it's meaning if you have to explain in simple terms it's telling detailed about every object it see like for uh like for books it tell about it's a stack of old books of table it tell of a wooden table a camera it's a vintage camera with black body and silver lens and also the location of camera to the portal so it's give a more detailed description of the image. So it's based on you what kind of captioning you want for your task. If you want a simple captioning like a person is standing in front of door this will be enough. But if you want a more detailed captioning like if two uh young boys are fighting in school background in beside the swing. So more detail captioning will be better option. Now let's get to another uh v vision task of Florence. Our next computer vision task of Florence 2 is object detection and recognition. First we have to create a helper function which help us in visualizing the model output with images. So I will run this function. Now our first task prompt is object except which is represented using OD. I will provide that uh task prompt to give model and run it. I am printing the result of the models. This is giving me the bounding boxes of the object. As you can see these are the bonding and also providing us with the labels of the object it have detected in the image. These are right there are five four instance of",
      "books, one instance of portal and one instance of camera and base each. Let's visualize this on image. As you can see uh after visualizing this on image you can see it is done quite well. So the camera the books and the portal and base. Now the next task the next task is dense reason capsing means it's uh let's show you to a visual same as it same as previous in detecting the the bounding boxes and the objects label but it's also telling us the relationship between the uh other box like the other objects like vintage book on wooden table. Let's see it visually. As you can see, it's telling about a visual medical book. It's also telling about vintage book on wooden table with cherry blossom branches in glass bottle. This is the description of this person. This is a discussion of this person. So let's get to our next task prompt which is reason proposal. Let's run this. As you can same as previous but it is only giving us the bounding boxes of the object not label. As you can see labels are empty here. Let's run this. You are saying it's that giving us the information of all the object it has detected but not the labels of the objects. So these are the all the object it has detected currently. Let's get to our next computer vision task. Our next computer vision task of Florence 2 is segmentation. First we have to create a helper function to represent that output visually. I will run this helper function. Now let's get to our task prompt. First our task prompt is referring express and segmentation. In this I have to give the textual input like camera which is the object the image as",
      "an argument to our model and task prompt which is referring expression segmentation and image and the result is the segmentation mask of the camera. As you can see it has lots of values in it but we have to visualize it. Let's see what's happen in images. As you can see the object has been detected and clearly segmented by pixel perfect level. Similarly, you can do it with other objects like book. These are the book object in the image. Let's see what happened. As you can see for every instance of book has been segmented. Let's get to our next. Let's take the text input portal. As you can see it has segmented the portal in the image. Now our next will be race. As you can see it has segmented the waist clearly. Next will be cherry blossom branches which are these. Let's segment it. It has segmented a cherry blossom which is uh kind of general segmentation but it's quite good for the model. It is uh next is uh task prompt is caption to phase grounding means uh I have to give a text input of caption what is in the image and it will ground it in the image like if I have the the im the apple is on the table so it will label the t apple and the table together and leave everything else as you can See the a camera an old camera books a table flower and a flower w here it has taken portal as flower base which is currently wrong. So with this the segmentation part is over there. Let's get to our next computer vision task. Our last computer vent task of Florence 2 is OCR. For this I will take another image for a",
      "sample. Let's show you the sample image. As you can see this is a s sample image I am choosing. It has a it has a text textual part which is going to be used in OCR detection. Let's give the OCR as a text from the model. As you can see it has read you are another line not another line stuck you are just scared to start. So it has detecting this part clearly and another object another text in also let's see what with the region this is the task for I have to provide OCR with reason and it gives the bonding boxes of the OCR it has detected with it. So to to visualize it, I have to create another helper function. Let's draw the OCR bounding boxes on the image. So if you see it you are saying that you are not stuck you are just here to start and other [Music] ocr and Yes, it has performed good quite well for a allpurpose model. With this our tutorial on how to perform vision task using Florence 2 is over. You can read about it in more detail on our labellerr website. In this blog I have tell in detail about Florence what it is and what other things you can do in greater detail. Please check it out."
    ],
    "transcript_word_count": 2625,
    "transcript_chunk_count": 9
  },
  {
    "video_id": "N-1_rt5kKN0",
    "title": "SegFormer Tutorial: Master Semantic Segmentation | Step-by-Step Guide",
    "description": "Unlock the power of next-generation computer vision with this hands-on SegFormer tutorial! \n\nIn this video, we dive deep into semantic segmentation using SegFormer, a revolutionary transformer-based architecture that’s changing how we understand images at the pixel level. Whether you’re a beginner or an experienced ML practitioner, this step-by-step Jupyter notebook guide will help you:\n\n- Understand the principles behind SegFormer and its advantages over traditional CNNs\n- Set up your environment and required libraries (PyTorch, Transformers, Pillow, etc.)\n- Load and run the NVIDIA SegFormer B5 model for real-world image segmentation\n- Visualize segmentation masks and interpret results for applications like autonomous driving, medical imaging, and more\n- Create reusable functions for fast, efficient inference on new images\n\nWe’ll walk through practical examples, compare results, and show you how to leverage SegFormer for your own projects. If you want to master semantic segmentation and stay ahead in computer vision, this video is for you!\n\nResources:\nBLOG: https://www.labellerr.com/blog/segformer/\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/SegFormer\n\nChapters\n0:00 Introduction to Semantic Segmentation with SegFormer\n0:29 Why SegFormer is Revolutionary\n1:26 Key Features of SegFormer Architecture\n2:24 Real-World Applications of SegFormer\n2:50 Setting Up the Environment\n3:14 Loading the NVIDIA SegFormer B5 Model\n4:07 Preparing a Sample Image for Segmentation\n4:39 Visualizing Segmentation Results\n6:32 Comparing Results with Original Image\n6:47 Creating a Reusable Inference Function\n8:24 Helper Function for Online Images\n8:52 Testing on Mountain Grassland Image\n9:43 Autonomous Driving POV Segmentation\n10:11 Church Image Segmentation\n10:56 Another Autonomous Driving Example\n11:42 Road to Mountain Segmentation\n12:45 Muddy Road with Grass Segmentation\n13:10 Conclusion and Further Reading\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=N-1_rt5kKN0",
    "embed_url": "https://www.youtube.com/embed/N-1_rt5kKN0",
    "duration": 816,
    "view_count": 443,
    "upload_date": "20250528",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat tool",
      "annotation software",
      "visual data",
      "deep learning",
      "machine learning",
      "artificial intelligence",
      "segformer",
      "semantic segmentation",
      "segformer tutorial",
      "segformer implementation",
      "data annotation tutorial",
      "SegFormer",
      "SemanticSegmentation",
      "ComputerVision",
      "DeepLearning",
      "PyTorch",
      "JupyterNotebook",
      "MachineLearning",
      "ImageSegmentation",
      "artificialintelligence",
      "neural networks",
      "panoptic segmentation vs semantic segmentation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone and welcome back to this tutorial. Today we are diving into one of the most exciting development in computer vision semantic segmentation using SEC former a revolutionary transformersbased architecture that's changing how we approach pixel level image se understanding. In this tutorial, we are will be working through a hands-on Jupyter notebook that demonstrates sec former inference step by step. If you ever wondered how AI system can identify and segment different objects in images like distinguishing between cars, trees, buildings and people at the pixel level then this video is perfect for you. SEC former represent a pivotal shift in how semantic segmentation is approached. Blending the global context modeling of transformer with efficiency of lightweight MLPS unlike tradition uh CNN approaches. Uh SEC former introduced a clean simple design that excel across diverse environment while being significantly more efficient than previous method. By the end of this tutorial, you will understand not just how to use sec former but also the underlying principle that makes it so powerful. Before we dive into core, let's understand what's make s former so revolutionary and what you can accomplish with this technology. So first sec former features are hierarchical transformers encoder that output multiscale features without requiring positional encoding. This is uh crucial because it avoids interpolaration issues that plagues other models when testing resolution differs from training resolution. Second, the architecture uses a lightweight all MLP decoder that aggregates information from different layer combines both local attention and global attention to create powerful representation. This simple design is actually the key to secform efficiency and transformer architecture. This technology has numerous real world application like autonomous driving system uses semantic segmentation for road scene understanding. Medical imaging relies on it for organ and tissue identification. Argument reality application uses it for scene understanding and urban planning leverage it for satellite image analysis. Now let's dive into actual implementation. Let's start by setting up the environment. First, we have to install some required libraries like transformers, uh, torch, pillow, mattplot, live and numpy. Then we are going to import those modules. Now the most important one, we have to load model and image processor. uh I am choosing my device as KUDA if available as CPU. Then we are going to use a model called Nvidia SE former B5 fine-tune AD. This model is trained on AD data set which we are going to use. Also I have created an instance of processor and model. So the the task of processor is to resize the image in our model's required format and provide it to our model which we will use to perform segmentation on our image. Now the the model and processor is in our environment. Then let's prepare an image. I'm going to take a sample image from AD data set to test our model. As you can see, this image is going to be used for segmentation testing. Let's uh give this image to our processor as an argument and that uh result will be go inside our model. Let's visualize our result. I'm going to use different color for each class. So I have created a function which give me a color for each class. Let's run that. Now I will run this function. As uh as you can see the postprocess semantic segmentation is used. This function resize the image from the model output to the original size which we are going to represent using matrix. As you can see each pixel of the image has different classes like if the pixel belong to this classes all the pixel belong to this that class will show same kind of uh value but it cannot be visualized clearly. So we are going to visualize it on our image. As you can see this is the result of our segmentation model sec former. As you can see the grass has been uh completely green by the segmentation mask. There is also the mask. The sky is all yellow because it belong to same same class. The house is in same color as blue. There is some error in segmentation. As you can see, it has not clearly detected uh or understand the picture. We will check it using our original image. Let's compare with our original image. This is the original segmentation mask of the image. As you can see, this was the original segmentation which the model hasn't quite understand, but it has done a great job in understanding other classes. Now, let's test our model on other scenarios like other images and see how well it perform. To perform set format inference on images, I'm going to create a function which will merge all these previous step into a single step. Using this function, it will make our task easier to perform segmentation using sec for my model. Now first we import our libraries. Then as previously explained I'm going to create instance of processor and model and also provide it with the model which we are going to use which is Nvidia SE former. Now creating the function this function is has a duty to take images and input and display the sec form model result on our image. So as here you can see this is the part where it take image and convert into the RGB format. Then that image is given to processor as an argument which is used by model to convert or provide us output. That output is resized using post-process semantic segmentation uh which help us understanding better what our image segmentation is. This is the segment map which we are getting. To visualize this better, we have to draw different color for each uh classes which is done by this part of the code. Now this part help us in displaying our image with a segmentation mask. Now run this code. Now we are also creating a helper function which help us in downloading or downloading or uh you can say showing image from uh internet using just URL. As you can see I am taking my first example. Let's display our example. This is an image of a mountainside grassland. You can see there are various class classes or objects like there is a class of cow, there's a class of tree, there is a class of grassland and there is a class of mountains. Let's perform our segmented uh using se uh seg former. As you can see it has done a good job understanding or segment performing semantic segmentation on the image. You can see the tree class is different color from the grass grass class and the cow class is different from other classes. There's a sky part. There's a mountain part. Now let's take another example. This is a point of view of a autonomous driving vehicle. So let's segment it. The semantic segmentation is very perfect. It has detected our truck completely which help our driving car understanding its environment better. Now let's take another example. This is a church uh church image between a grass grass area. There's a im uh trees in the background. Let's perform segmentation on it. As you can see the church building is completely in pink color and the grass is in green color and tree is in dark green color and the brushes of bruises of the uh in front of church is in yellow color. This helps a computer to understand what object position are. Now let's take another example. This is a point of view of another autonomous driving vehicle which help uh in understanding what is the scenario of the uh road. Now perform segmentation on it. As you can see, it is done a great job in segmenting uh CL car class from load class and background classes. The blue part is all the car which you have to avoid during driving. Now let's take another example. This is an image of a road leading to mountain. Let's perform segment semantic segmentation on it. The result of semantic segmentation is good. It can understand the road quite well and the m mountain also and the side forest is very much understandable using semantic mask. Now let's take another example. As you can see, semantic segmentation is very good. It has understand our road quite well, which is difficult. As you can see, there is lot lots of grass and it's a muddy road. So, we can understand where is grass, where is tree, where is our river. Let's uh with this our tutorial on sec former inference is over. To read more you can check our blog on sec former tutorial master semantic segmentation fast. In this blog you we were going to explain how can you can perform semantic segmentation in great detail and what are the benefits of semantic segmentation in computer vision.",
    "transcript_chunks": [
      "[Music] Hello everyone and welcome back to this tutorial. Today we are diving into one of the most exciting development in computer vision semantic segmentation using SEC former a revolutionary transformersbased architecture that's changing how we approach pixel level image se understanding. In this tutorial, we are will be working through a hands-on Jupyter notebook that demonstrates sec former inference step by step. If you ever wondered how AI system can identify and segment different objects in images like distinguishing between cars, trees, buildings and people at the pixel level then this video is perfect for you. SEC former represent a pivotal shift in how semantic segmentation is approached. Blending the global context modeling of transformer with efficiency of lightweight MLPS unlike tradition uh CNN approaches. Uh SEC former introduced a clean simple design that excel across diverse environment while being significantly more efficient than previous method. By the end of this tutorial, you will understand not just how to use sec former but also the underlying principle that makes it so powerful. Before we dive into core, let's understand what's make s former so revolutionary and what you can accomplish with this technology. So first sec former features are hierarchical transformers encoder that output multiscale features without requiring positional encoding. This is uh crucial because it avoids interpolaration issues that plagues other models when testing resolution differs from training resolution. Second, the architecture uses a lightweight all MLP decoder that aggregates information from different layer combines both local attention and global attention to create powerful representation. This simple design is actually the key to secform efficiency and transformer architecture. This technology has numerous real world application like autonomous driving system uses semantic segmentation for road scene understanding. Medical imaging relies on it for organ and tissue identification. Argument reality",
      "application uses it for scene understanding and urban planning leverage it for satellite image analysis. Now let's dive into actual implementation. Let's start by setting up the environment. First, we have to install some required libraries like transformers, uh, torch, pillow, mattplot, live and numpy. Then we are going to import those modules. Now the most important one, we have to load model and image processor. uh I am choosing my device as KUDA if available as CPU. Then we are going to use a model called Nvidia SE former B5 fine-tune AD. This model is trained on AD data set which we are going to use. Also I have created an instance of processor and model. So the the task of processor is to resize the image in our model's required format and provide it to our model which we will use to perform segmentation on our image. Now the the model and processor is in our environment. Then let's prepare an image. I'm going to take a sample image from AD data set to test our model. As you can see, this image is going to be used for segmentation testing. Let's uh give this image to our processor as an argument and that uh result will be go inside our model. Let's visualize our result. I'm going to use different color for each class. So I have created a function which give me a color for each class. Let's run that. Now I will run this function. As uh as you can see the postprocess semantic segmentation is used. This function resize the image from the model output to the original size which we are going to represent using matrix. As you can see each pixel of the image has different classes like if the pixel belong",
      "to this classes all the pixel belong to this that class will show same kind of uh value but it cannot be visualized clearly. So we are going to visualize it on our image. As you can see this is the result of our segmentation model sec former. As you can see the grass has been uh completely green by the segmentation mask. There is also the mask. The sky is all yellow because it belong to same same class. The house is in same color as blue. There is some error in segmentation. As you can see, it has not clearly detected uh or understand the picture. We will check it using our original image. Let's compare with our original image. This is the original segmentation mask of the image. As you can see, this was the original segmentation which the model hasn't quite understand, but it has done a great job in understanding other classes. Now, let's test our model on other scenarios like other images and see how well it perform. To perform set format inference on images, I'm going to create a function which will merge all these previous step into a single step. Using this function, it will make our task easier to perform segmentation using sec for my model. Now first we import our libraries. Then as previously explained I'm going to create instance of processor and model and also provide it with the model which we are going to use which is Nvidia SE former. Now creating the function this function is has a duty to take images and input and display the sec form model result on our image. So as here you can see this is the part where it take image and convert into the RGB format. Then that image",
      "is given to processor as an argument which is used by model to convert or provide us output. That output is resized using post-process semantic segmentation uh which help us understanding better what our image segmentation is. This is the segment map which we are getting. To visualize this better, we have to draw different color for each uh classes which is done by this part of the code. Now this part help us in displaying our image with a segmentation mask. Now run this code. Now we are also creating a helper function which help us in downloading or downloading or uh you can say showing image from uh internet using just URL. As you can see I am taking my first example. Let's display our example. This is an image of a mountainside grassland. You can see there are various class classes or objects like there is a class of cow, there's a class of tree, there is a class of grassland and there is a class of mountains. Let's perform our segmented uh using se uh seg former. As you can see it has done a good job understanding or segment performing semantic segmentation on the image. You can see the tree class is different color from the grass grass class and the cow class is different from other classes. There's a sky part. There's a mountain part. Now let's take another example. This is a point of view of a autonomous driving vehicle. So let's segment it. The semantic segmentation is very perfect. It has detected our truck completely which help our driving car understanding its environment better. Now let's take another example. This is a church uh church image between a grass grass area. There's a im uh trees in the background. Let's perform segmentation",
      "on it. As you can see the church building is completely in pink color and the grass is in green color and tree is in dark green color and the brushes of bruises of the uh in front of church is in yellow color. This helps a computer to understand what object position are. Now let's take another example. This is a point of view of another autonomous driving vehicle which help uh in understanding what is the scenario of the uh road. Now perform segmentation on it. As you can see, it is done a great job in segmenting uh CL car class from load class and background classes. The blue part is all the car which you have to avoid during driving. Now let's take another example. This is an image of a road leading to mountain. Let's perform segment semantic segmentation on it. The result of semantic segmentation is good. It can understand the road quite well and the m mountain also and the side forest is very much understandable using semantic mask. Now let's take another example. As you can see, semantic segmentation is very good. It has understand our road quite well, which is difficult. As you can see, there is lot lots of grass and it's a muddy road. So, we can understand where is grass, where is tree, where is our river. Let's uh with this our tutorial on sec former inference is over. To read more you can check our blog on sec former tutorial master semantic segmentation fast. In this blog you we were going to explain how can you can perform semantic segmentation in great detail and what are the benefits of semantic segmentation in computer vision."
    ],
    "transcript_word_count": 1487,
    "transcript_chunk_count": 5
  },
  {
    "video_id": "9rnRmx5WQzc",
    "title": "Mask2Former Tutorial: Complete Guide to all Segmentation",
    "description": "Learn how to use Meta AI's revolutionary Mask2Former for pixel-perfect image segmentation! This comprehensive tutorial covers all three types of segmentation: semantic, instance, and panoptic.\n\nWhat You'll Learn:\n✅ Understanding Mask2Former architecture and masked attention mechanism\n✅ Implementing semantic segmentation for object classification\n✅ Mastering instance segmentation for individual object detection\n✅ Exploring panoptic segmentation combining both approaches\n✅ Hands-on coding with Hugging Face Transformers library\n✅ Real-world applications in autonomous driving, medical imaging, and robotics\n\nResources:\nBlog: https://www.labellerr.com/blog/mask2former-hands-on-tutorial-guide/\nNotebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/Mask2Former\n\nChapters\n0:00 Introduction\n1:50 What is Mask2Former?\n2:49 Setting Up the Code\n3:13 Sample Image Overview\n3:57 Semantic Segmentation\n7:49 Instance Segmentation\n10:38 Panoptic Segmentation\n13:15 Conclusion\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=9rnRmx5WQzc",
    "embed_url": "https://www.youtube.com/embed/9rnRmx5WQzc",
    "duration": 824,
    "view_count": 527,
    "upload_date": "20250528",
    "uploader": "Labellerr",
    "tags": [
      "Panoptic Segmentation",
      "Deep Learning",
      "Computer Vision",
      "Meta AI",
      "Python Tutorial",
      "Jupyter Notebook",
      "data annotation",
      "image annotation tool",
      "Mask2Former",
      "Image Segmentation",
      "Semantic Segmentation",
      "Vision Transformer",
      "AI research",
      "Instance Segmentation",
      "instance segmentation vs semantic segmentation vs object detection",
      "computer science",
      "python",
      "python tutorial for beginners",
      "jupyter notebook tutorial",
      "python programming language",
      "best image segmentation models"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone, welcome to this tutorial. Today we are diving deep into one of the most powerful computer vision model out there marks to former. If you are ever wondering how AI can understand and separate different objects in images with pixel perfect precision, you are in the right video. By the end of this video, you will know exactly how to use marks tor for three different types of image segmentation and I will show you some mind-blowing results. So what exactly is Max to former developed by Meta AI researcher in 2022. Max reformer is what we call a universal image segmentation architecture. Now what makes it special is that it can handle all major segmentation tasks with a single unified framework. Think of it as a one super smart AI that can look at any image and tell you what objects are in the image, what exactly each object is located and separate them with pixel perfect accuracy. The beauty of marks to former is that it's built on original marks former but introduces key animation that makes it both faster and accurate. It uses something called marks attention which helps the model focus on relevant parts of the image while ignoring irrelevant background noises. So the results speaks for themsel. Marx forward achieves state-of-the-art performance across multiple data sets including Koko AD20K and citycapes. But enough let's understand what segmentation actually means and why it matters. All right. Now let's get our hand dirty with some code. I will be walking you through this Jupyter notebook step by step. You can find the link to this notebook in the description below. First, let's install all the required libraries. We are using the hugging face transformers library which makes our working with mark reformer incredibly easy. First install these packages then import these packages in our environment. I'm creating some helper function which will help in better visualization of our result. As the model gives an output in matrix format, we can't visualize it. So we have to create a function to visualize it. Like this function visualize segmentation result with object names and color lines on the image. And this function so binary marks which means it show as a solid color of the result on the image. And this is help in visualizing semantic segmentation means it's uh used in only semantic segmentation. Let's run this code. Now I'm going to use a sample image in our notebook. This will be the sample image. This is an image of five cards standing in a uh uh standing or overlapping each other and and there is a background which contain houses, trees and other things and the cars are in a parking area. This is a perfect image to visualize different kinds of segmentation. Let's get to our first segmentation. Our first segmentation is semantic segmentation. This is like painting by numbers. Every pixel in the image gets labeled with what it represent. All pixel belonging to person class get one color. All belonging to car class get another color and so on. Its use cases are various like in autonomous driving car it used in identifying road, sidewalk and buildings. In medical field it used in identifying different tissues and or organ and many more. The key thing here is that it doesn't distinguish between different instance. All cars are just cars here regardless of how many are there. Let's you walk it through. I'm going to import uh auto image processor and mark to former for universal segmentation classes from transformer library. Let's run this code. Here I am creating a function which will perform segment semantic segmentation. I am using model from Facebook marks to former swim large ad semantic which is trained on cityscaped data. So I will create two instance processors and model. Processor is used in resizing the image to the models requirement. Let's uh run this function. And once more I'm creating this part load function using a URL or the image part which helps me in loading image model. Then I will provide that image to the processor as an argument as you can see here. And then provide the processor result to the model which will uh which will help uh us in finding the semantic segmentation. Then using post-process semantic segmentation I will resize the output result to the value office original size and then return the predicted marks image and model. I have run this function. Let's visualize it. I will run the semantic segmentation and get three values predicted mask image uh and model. Let's run it. As you can see, I have gotten a semantic segmentation. You can see every CL car class got a green color, every building class get a red color, every pavement class get a purple color. So there is no difference between which card it is but all are detected as card here. This is the beauty of semantic segmentation. Every class belong to one color. So let's see the binary mask of it. As you can see you cannot differentiate between different instances of class here. There are five cars in the image but you can see uh there are any number of cars can be here. So this is also a problem with semantic segmentation but it help us in generalizing where is the object in as you can see this green area is the uh car area of the image and this blue area is the building area this dark blue area is of the pavement area. Let's get to our next segmentation. Our next segmentation is instance segmentation. This is where things get interesting. Not only does it identify what each pixel represent, but it also separates different instance of same object. So if there are five car in image, each car gets its own unique mask and colored. Its use cases varies a lot. Like in retail analytics, it's used in counting product of sales. In quality control, in manufacturing, it helps in identifying individual defects on product. In sports analytics, it used in tracking individual player. In medical field, it used in finding cell counting. In biology research, this is perfect when you need to know how many of something, not just what it is. Let's run its code. So I have created a function to run instant segmentation. It uses Facebook marks to former swim large poco instance which use in finding instant segmentation. Similarly a se semantic segmentation I have created two instance of model and processor and created a function which help in finding uh loading the image to our function and pro I have used the image and as an argument to the processor which provide us with input and that input goes to model which provides us with output and that output is resized using post-process instance segmentation which pro return us result image and model. Let's run this function now to visualize it. We will run this functions. As you can see our original image has five cars, buildings and many other things. But in instance segmentation it only focuses on main parts of image which is car. You can see there are five car which different color. You can count it 1 2 3 4 5. It help us in understanding how many cars are there in image and where they belong. So uh you can visualize this result on the image also. You can see each car gets different colored which is shown here. So this is all for instance segmentation. Let's get to our final segmentation. Finally our last segmentation is penoptic segmentation. Think of this as a mix of board segmentation. It combines semantic and instance segmentation. It handles background classes like sky, road, grass with segment segmentation and things classes like people, cars, animal with instant segmentation. It's used in various scenarios like in advanced autonomous driving systems. It helps in differentiating different cars from background. In augmented reality application, it helps in projecting objects in real world. In robotics, it helps in differentiating multiple instance of same objects from background. And in video editing, it's used in various uh features of image editing. This gives us the most complete understanding of scene possible. Let's run its code. As you can see, it has same steps. We have to follow as previous model. We have to fi find a model called Facebook master former swimbased cocoa panoptic which is the model for running pinoptic segmentation and provided to checkpoint and this checkpoint is provided to processor and model. Then same process repeated again which provides us with result image and model. Let's run this code. So to visualize this we have to run this function. As you can see the result we have gotten is different from other result like it also show different instance of cars but also so houses pavement in a semantic point of view like sky is all merged houses are all merged but car are car are in different instance this is the beauty of poptic segmentation we can visualize this on our image as you can say a house is in same color but cars are in different color color and the pavement is also in same color. With this uh we have seen all types of semantic segmentation and perform it using our marks to former model. With this our tutorial on marks to former is over. You can read more about it on our blog. [Music] In this blog of marks to former hands-on tutorial guide, we will explain about marks to former in more detailed manner. You can check it out on our website blog.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome to this tutorial. Today we are diving deep into one of the most powerful computer vision model out there marks to former. If you are ever wondering how AI can understand and separate different objects in images with pixel perfect precision, you are in the right video. By the end of this video, you will know exactly how to use marks tor for three different types of image segmentation and I will show you some mind-blowing results. So what exactly is Max to former developed by Meta AI researcher in 2022. Max reformer is what we call a universal image segmentation architecture. Now what makes it special is that it can handle all major segmentation tasks with a single unified framework. Think of it as a one super smart AI that can look at any image and tell you what objects are in the image, what exactly each object is located and separate them with pixel perfect accuracy. The beauty of marks to former is that it's built on original marks former but introduces key animation that makes it both faster and accurate. It uses something called marks attention which helps the model focus on relevant parts of the image while ignoring irrelevant background noises. So the results speaks for themsel. Marx forward achieves state-of-the-art performance across multiple data sets including Koko AD20K and citycapes. But enough let's understand what segmentation actually means and why it matters. All right. Now let's get our hand dirty with some code. I will be walking you through this Jupyter notebook step by step. You can find the link to this notebook in the description below. First, let's install all the required libraries. We are using the hugging face transformers library which makes our working with mark reformer incredibly easy.",
      "First install these packages then import these packages in our environment. I'm creating some helper function which will help in better visualization of our result. As the model gives an output in matrix format, we can't visualize it. So we have to create a function to visualize it. Like this function visualize segmentation result with object names and color lines on the image. And this function so binary marks which means it show as a solid color of the result on the image. And this is help in visualizing semantic segmentation means it's uh used in only semantic segmentation. Let's run this code. Now I'm going to use a sample image in our notebook. This will be the sample image. This is an image of five cards standing in a uh uh standing or overlapping each other and and there is a background which contain houses, trees and other things and the cars are in a parking area. This is a perfect image to visualize different kinds of segmentation. Let's get to our first segmentation. Our first segmentation is semantic segmentation. This is like painting by numbers. Every pixel in the image gets labeled with what it represent. All pixel belonging to person class get one color. All belonging to car class get another color and so on. Its use cases are various like in autonomous driving car it used in identifying road, sidewalk and buildings. In medical field it used in identifying different tissues and or organ and many more. The key thing here is that it doesn't distinguish between different instance. All cars are just cars here regardless of how many are there. Let's you walk it through. I'm going to import uh auto image processor and mark to former for universal segmentation classes from transformer library. Let's",
      "run this code. Here I am creating a function which will perform segment semantic segmentation. I am using model from Facebook marks to former swim large ad semantic which is trained on cityscaped data. So I will create two instance processors and model. Processor is used in resizing the image to the models requirement. Let's uh run this function. And once more I'm creating this part load function using a URL or the image part which helps me in loading image model. Then I will provide that image to the processor as an argument as you can see here. And then provide the processor result to the model which will uh which will help uh us in finding the semantic segmentation. Then using post-process semantic segmentation I will resize the output result to the value office original size and then return the predicted marks image and model. I have run this function. Let's visualize it. I will run the semantic segmentation and get three values predicted mask image uh and model. Let's run it. As you can see, I have gotten a semantic segmentation. You can see every CL car class got a green color, every building class get a red color, every pavement class get a purple color. So there is no difference between which card it is but all are detected as card here. This is the beauty of semantic segmentation. Every class belong to one color. So let's see the binary mask of it. As you can see you cannot differentiate between different instances of class here. There are five cars in the image but you can see uh there are any number of cars can be here. So this is also a problem with semantic segmentation but it help us in generalizing where is the object",
      "in as you can see this green area is the uh car area of the image and this blue area is the building area this dark blue area is of the pavement area. Let's get to our next segmentation. Our next segmentation is instance segmentation. This is where things get interesting. Not only does it identify what each pixel represent, but it also separates different instance of same object. So if there are five car in image, each car gets its own unique mask and colored. Its use cases varies a lot. Like in retail analytics, it's used in counting product of sales. In quality control, in manufacturing, it helps in identifying individual defects on product. In sports analytics, it used in tracking individual player. In medical field, it used in finding cell counting. In biology research, this is perfect when you need to know how many of something, not just what it is. Let's run its code. So I have created a function to run instant segmentation. It uses Facebook marks to former swim large poco instance which use in finding instant segmentation. Similarly a se semantic segmentation I have created two instance of model and processor and created a function which help in finding uh loading the image to our function and pro I have used the image and as an argument to the processor which provide us with input and that input goes to model which provides us with output and that output is resized using post-process instance segmentation which pro return us result image and model. Let's run this function now to visualize it. We will run this functions. As you can see our original image has five cars, buildings and many other things. But in instance segmentation it only focuses on main parts of image",
      "which is car. You can see there are five car which different color. You can count it 1 2 3 4 5. It help us in understanding how many cars are there in image and where they belong. So uh you can visualize this result on the image also. You can see each car gets different colored which is shown here. So this is all for instance segmentation. Let's get to our final segmentation. Finally our last segmentation is penoptic segmentation. Think of this as a mix of board segmentation. It combines semantic and instance segmentation. It handles background classes like sky, road, grass with segment segmentation and things classes like people, cars, animal with instant segmentation. It's used in various scenarios like in advanced autonomous driving systems. It helps in differentiating different cars from background. In augmented reality application, it helps in projecting objects in real world. In robotics, it helps in differentiating multiple instance of same objects from background. And in video editing, it's used in various uh features of image editing. This gives us the most complete understanding of scene possible. Let's run its code. As you can see, it has same steps. We have to follow as previous model. We have to fi find a model called Facebook master former swimbased cocoa panoptic which is the model for running pinoptic segmentation and provided to checkpoint and this checkpoint is provided to processor and model. Then same process repeated again which provides us with result image and model. Let's run this code. So to visualize this we have to run this function. As you can see the result we have gotten is different from other result like it also show different instance of cars but also so houses pavement in a semantic point of view like sky",
      "is all merged houses are all merged but car are car are in different instance this is the beauty of poptic segmentation we can visualize this on our image as you can say a house is in same color but cars are in different color color and the pavement is also in same color. With this uh we have seen all types of semantic segmentation and perform it using our marks to former model. With this our tutorial on marks to former is over. You can read more about it on our blog. [Music] In this blog of marks to former hands-on tutorial guide, we will explain about marks to former in more detailed manner. You can check it out on our website blog."
    ],
    "transcript_word_count": 1624,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "M0duRkrGFEg",
    "title": "YOLO-NAS Explained: Object Detection with Neural Architecture Search | Comparison with YOLOv8",
    "description": "Welcome to this comprehensive tutorial on YOLO-NAS, one of the most advanced variants of the popular YOLO object detection models! In this video, we’ll explore what makes YOLO-NAS unique, including its use of Neural Architecture Search (NAS) for automatic model optimization.\n\nWhat You’ll Learn:\n- What is YOLO-NAS and how is it different from other YOLO models?\n- How to install and set up YOLO-NAS using Python.\n- Step-by-step guide to performing object detection with YOLO-NAS.\n- Real-world examples and results on the COCO dataset.\n- Side-by-side comparison: YOLO-NAS vs YOLOv8 – which model performs better?\n- Tips for using YOLO-NAS in your own projects.\n\n\nResources:\nBLOG: https://www.labellerr.com/blog/the-ultimate-yolo-nas-guide-2025-what-it-is-how-to-use/\n\nNOTEBOOK: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/YOLO/YOLO-NAS\n\nChapters\n0:00 Introduction to YOLO-NAS\n0:55 Installing Required Python Libraries\n1:12 YOLO-NAS Overview & Model Setup\n2:00 Running Object Detection with YOLO-NAS\n3:02 Installation & Environment Setup\n3:10 Comparing YOLO-NAS with YOLOv8\n5:01 Dataset Preparation & Evaluation\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=M0duRkrGFEg",
    "embed_url": "https://www.youtube.com/embed/M0duRkrGFEg",
    "duration": 341,
    "view_count": 145,
    "upload_date": "20250523",
    "uploader": "Labellerr",
    "tags": [
      "semantic markup",
      "data cleaning",
      "data auditing",
      "image processing",
      "labeling software",
      "data validation",
      "deep learning",
      "dataset creation",
      "machine learning",
      "image metadata",
      "labeling process",
      "computer vision",
      "data annotation",
      "image annotation tool",
      "data preprocessing",
      "annotation software",
      "semantic segmentation",
      "image classification",
      "manual annotation",
      "image segmentation",
      "data quality",
      "annotation workflow",
      "quality assurance",
      "data annotation tools",
      "computer science"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone, welcome to this tutorial in which we are going to explore one of the popular variants of YOLO which is YOLO NAS where NAS means neural architecture search which make it different from other variants of YOLO. What make YOLONAS vessel? It uses advanced neural architecture search or NAS which means the model structure is automatically optim optimized by AI to be a efficient and accurate as possible. In this tutorial we will also compare YOLO with YOLO V8 to see which perform better in object detection. Let's start our tutorial. First we start by installing important Python libraries for YOLONAS. We import these libraries like Matt Pro uh computer cv2 also remember super gradient is required to uh use yolon using ultralytics module. So to use YOLONAS model, we have to import NAS class from our ultralytics module and uh use this YOLO NAS model to perform our object detection. We create a NAS variable for it. Let's check about it. As you can see our model has clearly been uh installed in our environment which these are the information about model which has 66 25 layered in par these parameter and gradients. Let's perform object detection on a sample image. As you can see, it has performed object detection in our sample image. It has detected two horses and two person. Let's take another example for object detection. So it it has worked well in this image. It has detected all four element and a bird. Let's another example. Yes, it has performed well in this example already. Buloness is this model of uloness is trained on a cocoa data set. So it will able to detect all classes of cocoa data set. Let's take another example. So it has detected four instance of horse. It has well detected the horse which has been overlapping another horse. Let's compare it with our YOLO V8 model. So to compare it, I have created a function which will compare it with YOLO V8. Let's run this function. Let's compare it with yolo v8. So as you can see in our earlier image of yolo detection, it has performed similarly. Let's take another example. As you can see, yolo NAS has failed because it detected four instance of giraffe whereas there is only three instance and YOLO V8 has also failed in it as it has detected five instance of giraffe. Let's compare it on another image. As you can see, Yolon NASA has detected very small object easily like YOLO Viet has only detected a horse but YOLONAS has detected a horse, a sheep, a cow and another cow. And beneath there is also a horse. Uh means it has detected two horses, one SE and two cows. So it means it is better at object detection than YOLO V8. Let's take another example. It has performed object detection quite well. Both models performed with accuracy like both has detected three zebras and two giraffes. With this our tutorial on Yolo NAS is over. To read about more you can check our blog on YOLONAS on our label platform. In this blog we will learn about YOLO in greater detail and see its uh object detection performance on various metrics.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome to this tutorial in which we are going to explore one of the popular variants of YOLO which is YOLO NAS where NAS means neural architecture search which make it different from other variants of YOLO. What make YOLONAS vessel? It uses advanced neural architecture search or NAS which means the model structure is automatically optim optimized by AI to be a efficient and accurate as possible. In this tutorial we will also compare YOLO with YOLO V8 to see which perform better in object detection. Let's start our tutorial. First we start by installing important Python libraries for YOLONAS. We import these libraries like Matt Pro uh computer cv2 also remember super gradient is required to uh use yolon using ultralytics module. So to use YOLONAS model, we have to import NAS class from our ultralytics module and uh use this YOLO NAS model to perform our object detection. We create a NAS variable for it. Let's check about it. As you can see our model has clearly been uh installed in our environment which these are the information about model which has 66 25 layered in par these parameter and gradients. Let's perform object detection on a sample image. As you can see, it has performed object detection in our sample image. It has detected two horses and two person. Let's take another example for object detection. So it it has worked well in this image. It has detected all four element and a bird. Let's another example. Yes, it has performed well in this example already. Buloness is this model of uloness is trained on a cocoa data set. So it will able to detect all classes of cocoa data set. Let's take another example. So it has detected four instance of horse.",
      "It has well detected the horse which has been overlapping another horse. Let's compare it with our YOLO V8 model. So to compare it, I have created a function which will compare it with YOLO V8. Let's run this function. Let's compare it with yolo v8. So as you can see in our earlier image of yolo detection, it has performed similarly. Let's take another example. As you can see, yolo NAS has failed because it detected four instance of giraffe whereas there is only three instance and YOLO V8 has also failed in it as it has detected five instance of giraffe. Let's compare it on another image. As you can see, Yolon NASA has detected very small object easily like YOLO Viet has only detected a horse but YOLONAS has detected a horse, a sheep, a cow and another cow. And beneath there is also a horse. Uh means it has detected two horses, one SE and two cows. So it means it is better at object detection than YOLO V8. Let's take another example. It has performed object detection quite well. Both models performed with accuracy like both has detected three zebras and two giraffes. With this our tutorial on Yolo NAS is over. To read about more you can check our blog on YOLONAS on our label platform. In this blog we will learn about YOLO in greater detail and see its uh object detection performance on various metrics."
    ],
    "transcript_word_count": 543,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "YGTH46dWB20",
    "title": "Introducing Classification Agent | Labellerr's New Automated Annotation Workflow",
    "description": "Hey everyone! Today, I'm excited to show you how Labellerr's new agentic workflow makes vehicle image classification faster and smarter. Before, we classified each image by hand, which was slow and sometimes led to mistakes. Now, with our Agentic AI system, we batch process images and get automatic answers to classification questions. You can quickly review and verify the results, save templates for future projects, and use this approach for images, videos, audio, and text. Tasks that took hours now finish in minutes. Try Labellerr to speed up your annotation with AI agents!\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=YGTH46dWB20",
    "embed_url": "https://www.youtube.com/embed/YGTH46dWB20",
    "duration": 206,
    "view_count": 115,
    "upload_date": "20250522",
    "uploader": "Labellerr",
    "tags": [
      "image metadata",
      "metadata management",
      "tagging software",
      "image processing",
      "image classification",
      "image organization",
      "smart tagging",
      "visual data",
      "data curation tools",
      "data optimization",
      "content tagging",
      "data management",
      "data preprocessing",
      "data visualization",
      "data analytics",
      "image tagging software",
      "data workflow",
      "image datasets",
      "ai image tagging",
      "image curation",
      "data science",
      "deep learning",
      "data governance",
      "business intelligence",
      "data scientist",
      "what is data governance"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Hey everyone and welcome. Today I want to show you something super exciting we've built here at Labeller. I am currently classifying these vehicle scenes. We have three classification questions for each image. First, a radio button for volume of traffic. Is it high, low, or a clear road? Second, a drop down for how is the weather? We choose between sunny, cloudy, rainy, fog, or snowy. And third a multi select for what objects are present in the image. We select objects like pedestrian, car, truck, bike or bus. Now let me tell you how we used to do this. In the past, our annotators manually selected the answer for every single classification question. They had to look at each image, judge the scene, and pick the most appropriate option. This took a lot of time. You can see it here. clicking, thinking image by image. It was slow. Plus, sometimes our annotators made mistakes. For example, they might not select pedestrian, which might be mistaken with bike if it's at a distance. This added more review time later. But now we have an amazing agentic way to do classification. We've introduced a new system which automatically answers these classification questions for us. Watch this. I will select the data set then set the classification questions. [Music] [Music] Now let's select the LLM. I will choose Gemini 2 flash. [Music] And here I can save this template also so that in this workspace if I ever need it again I can just load it back in any of my [Music] [Music] projects. Here you can see create project with labellerr AI. I will choose this and this will set things in motion. Now you can see 50 files remaining, zero files in review. And here's the best part. Our new classification agent batch processes images. This means it handles many images all in one go. My job now is simple. I directly review the image and just verify if the AI's output is correct. Tasks that used to take a couple of hours or a day can now be finished in minutes. This really boosts our speed and accuracy. This powerful Agentic approach isn't just for images. We can apply the same principles to speed up annotation for video, audio, and even textual data. Want to make your annotation process faster and smarter? Explore labellerr to speed up your annotation with agents.",
    "transcript_chunks": [
      "Hey everyone and welcome. Today I want to show you something super exciting we've built here at Labeller. I am currently classifying these vehicle scenes. We have three classification questions for each image. First, a radio button for volume of traffic. Is it high, low, or a clear road? Second, a drop down for how is the weather? We choose between sunny, cloudy, rainy, fog, or snowy. And third a multi select for what objects are present in the image. We select objects like pedestrian, car, truck, bike or bus. Now let me tell you how we used to do this. In the past, our annotators manually selected the answer for every single classification question. They had to look at each image, judge the scene, and pick the most appropriate option. This took a lot of time. You can see it here. clicking, thinking image by image. It was slow. Plus, sometimes our annotators made mistakes. For example, they might not select pedestrian, which might be mistaken with bike if it's at a distance. This added more review time later. But now we have an amazing agentic way to do classification. We've introduced a new system which automatically answers these classification questions for us. Watch this. I will select the data set then set the classification questions. [Music] [Music] Now let's select the LLM. I will choose Gemini 2 flash. [Music] And here I can save this template also so that in this workspace if I ever need it again I can just load it back in any of my [Music] [Music] projects. Here you can see create project with labellerr AI. I will choose this and this will set things in motion. Now you can see 50 files remaining, zero files in review. And here's the best part. Our",
      "new classification agent batch processes images. This means it handles many images all in one go. My job now is simple. I directly review the image and just verify if the AI's output is correct. Tasks that used to take a couple of hours or a day can now be finished in minutes. This really boosts our speed and accuracy. This powerful Agentic approach isn't just for images. We can apply the same principles to speed up annotation for video, audio, and even textual data. Want to make your annotation process faster and smarter? Explore labellerr to speed up your annotation with agents."
    ],
    "transcript_word_count": 403,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "9855PcoFceQ",
    "title": "RT-DETR vs RT-DETRv2: Full Comparison | Real-Time Object Detection Tutorial",
    "description": "Unlock the next level of real-time object detection! In this comprehensive tutorial, we compare two cutting-edge models: RT-DETR and its advanced successor, RT-DETRv2. Discover how these transformer-based detectors stack up against each other-and against popular models like YOLO-in terms of speed, accuracy, and versatility.\n\nWhat you'll learn in this video:\n- The evolution from DETR to RT-DETR and RT-DETRv2\n- Key architectural differences and improvements\n- Real-world performance: small object detection, long-distance accuracy, and multi-task flexibility\n- Step-by-step code walkthrough using Python\n- Live demos on images and videos, including crowded scenes and edge cases\n- Which model is best for your use case: surveillance, robotics, autonomous vehicles, or scalable deployment\n\nBLOG: https://www.labellerr.com/blog/rt-detrv2-beats-yolo-full-comparison-tutorial/\n\nNOTEBOOK: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/DETR\n\nChapters\n0:00 Introduction\n0:35 Understanding DETR: The Original Model\n1:51 Introduction to RT-DETR\n2:48 RT-DETRv2: Improvements and Features\n4:09 Setting Up the Environment\n5:03 Loading and Processing Sample Image\n6:58 Object Detection with RT-DETR\n8:47 Object Detection with RT-DETRv2\n9:50 Long-Distance Detection: Elephants and Zebras\n10:57 Small Object Detection: Sheep Example\n11:56 Another Sheep Detection Example\n13:06 Real-Time Inference on Video: Football Match\n14:24 Conclusion and Further Resources\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n\n#RTDETR #RTDETRv2 #ObjectDetection #RealTimeDetection #Transformers #VisionTransformers #DETR #YOLO #ComputerVision #AITutorial #DeepLearning #MachineLearning #ImageProcessing #AIComparison #DetectionTransformer #COCODataset #ModelBenchmarking #PythonAI #PyTorch #OpenSourceAI #ArtificialIntelligence #AIResearch  #VideoAnalytics #AIModels",
    "video_url": "https://www.youtube.com/watch?v=9855PcoFceQ",
    "embed_url": "https://www.youtube.com/embed/9855PcoFceQ",
    "duration": 892,
    "view_count": 511,
    "upload_date": "20250521",
    "uploader": "Labellerr",
    "tags": [
      "computer vision",
      "data annotation",
      "image annotation tool",
      "image segmentation",
      "machine learning",
      "ai tools",
      "image classification",
      "annotation software",
      "computer vision apps",
      "ai image tagging",
      "image processing",
      "labeling software",
      "ai tagging",
      "data labelling",
      "deep learning",
      "object detection",
      "YOLO",
      "PyTorch",
      "YOLO object detection",
      "YOLOv8",
      "detection transformer",
      "model benchmarking",
      "yolo ai",
      "yolov8n",
      "python",
      "yolov10",
      "yolov8 tracking",
      "yolov5 custom object detection"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone, welcome to this tutorial. In this video, we are going to compare two modern object detectors RTER and its upgrade version RTER V2. But before we get into difference uh between them, let's take a step back and understand the original model that started it all deter and why it needed improvement in the first place. Let's get started. So DER or detection transformers was introduced by Facebook AI in 2020. It was the first object detector to use a transformers encoder architectured removing the need for handcrafted component like anchor boxes region proposals and non-max suppression. The idea was elegant pro pass an image through a CNN flatten the features run them through a transformers and use a fixed set of learned objects queries to directly predict bounding boxes and class labels. But der has some serious limitation. It was very slow to train like often requiring over 500 epochs to converge. It had high inference latency making it making it unsuitable for realtime application. It struggled with small object because it didn't use multiscale features and it wasn't flexible adapting it to tasks like segmentation or key point wasn't a straightforward. So while deter introduced a new direction it wasn't practical for many real world scenarios that's where RT came in developed by BU in 2023 it was it kept the transformers idea but restructured things to make it real time and edge device friendly like some of its key feature include using lightweight CNN backbones like conext or mobile net uh adding multi scale features to improve small object detection using top K query selection instead of decoding over all queries faster and similar matching mechanism uh like uh and ditching the slow Hungarian algorithm and the result was faster training and real-time inference and decent accuracy artiller proved that transformer speed detector could actually compete with yolo like model in pro production environment. Now let's talk about its upgrade. RT v2 released in 2024. While RTER was all about speed, RTER v2 focus on both versatility and efficiency across platform. Here what RTD V2 improved introduced a hybrid task cascaded allowing it to handle detection instant segmentation keyoint detection and more use multi-resolution deformable attention like uh which resulted in improving feature quality optimize it for low power devices through support for quantization and which make the overall performance better without sacrificing latency making it ideal for both cloud and edge divi edge environment. So to sum it up if you need faster and accurate object detection RT is solid but if you want something that's task flexible plat platform optimize and more powerful overall RT v2 is the best model you need. So let's come back to our comparison. Before going straight for object detection using RTD, we have to first set up some environment for it. I'm going to use Nvidia L4 GPU for my tutorial. Now install some Python libraries which we are going to use in our tutorial. These are transformer torque pillow mattl request open cv tim. Let's check the version of each library. As you are seeing I'm using the latest version of each libraries. So let's import libraries to our notebook. Now I have created a helper function which help me to uh download image using URL and load it in our notebook. Let's run that function. Now I'm using this sample image for our tutorial. As you can see, it contains lots of objects which are very far and in front of you and lots of random objects like a duck or flowers and bicycle which is in the lower corner of the image. Now for our main code block how I am going to uh use RT editor. I am using transformers module to import two classes. RT editor for object detection, RT editor for image processor and now this device variable helps me to choose between CUDA and CPU if available. So model in this uh we are going to load model. I'm using pecking u R50 VD. It's a simple model which is good for both accuracy and speed and a processor. The processor is here to uh to resize our image like normalize or to cut image to provide it to our model. Now first we are going to provide our image to our processor and input that value to our model. As you can see I'm using tor nat as it helps me to save memory. Now I am pre prep-processing my image. As you can see image size I am reversing it from height to width to width to as PyTorch required in that format. Now let's run that on our image. As we can see uh detection has been observed in our image. So the scores are the confidence level of each objects. Labels represent which object uh belongs to which class and boxes give us the coordinates of our object. But you are seeing in label we are getting uh uh integer as a value but classes name should be of string. We are going to see that later later. Let's see how many classes we how many object we have detected in this image. As you can see I have detected 21 objects in this image. So let's talk about what label means here. Seven. As you know uh RT RT is trained on Coco data set. So Coco data set has uh classes which index on them. So these are the classes of Coco data set. As you can uh it's a zerobased indexing. So the seven will be here it's a truck and let's see in our image. Yes there is a truck in our image. So let's create a function which will simplify that. As you can see I have created a function which simplify this process and also visualize my results on the image and return the modified image. Let's run this image. run this function. So I have created same function for our RT editor. We just have to change the value here. Uh I am going to use RT editor V2 R50 the similar model of RT editor series. Let's run this function. Let's inference result on our image. As you can see, artillery deter has done a great job in detecting uh objects. It's a detected ducks, bicycle which is in the very corner, person truck, but it has failed to detect some key objects like plant which is in our PCO data set. Let's see what happened with RTD V2. As you can see, it has detected more objects than RT det. It has detected our port. It had detected stop sign. Yes, it has detected person bicycle. It has done a better job at detecting than RT. So let's take in another image for our sample. As you can see this is a image of uh elephants and zebra from a very far away distance. Let's see what happened. First we check RTOR V2. As you can see RT editor has nearly detected all the objects in our image. Let's see what happened with RTD. It has detected but if you see it uh here like uh it has detected to fail some zebras but our model RT detor V2 has detected all the zebras clearly. So this is a main feature of RTER V2 which is good at detecting object at long distance or of small sizes. Let's take another example. It's a image which contains lots of se let's run that uh inference our model on it. I'm using RT v2 first. So the art v2 has detected all these shapes nearly but it failed to detect these here. As you can see it has leave four save undetected. Let's try this image on our RT v1. Let's it also failed to detect these objects. So board RT failed to detect the seeps which are here. Let's take another example. So in this example you are seeing uh a lots of shapes. Uh so let's inference our model on it. So the inference result is here. So as you can see it has detected all the seeps in our image. Let's try it with art uh RT deter as you can see it is also detected most of the sips clearly but it failed to detect some sips behind which has been clearly detected by RT V2 as you can it has not detected these sips which is uh its downside. inside as it can't detect object which are smaller inside. Let's try our model on a video. The main power of RT series is its capability for realtime inference. So we are going to test it on a video of a football match. This is the sample video I'm going to use as an example. As you can see it has many object like like player footballs people in crowd which we are going to see if both model can detect it clearly. Let's uh perform inference on the video. For that I have created a function which will inference our model on the video and save it. Let's run this function now. Let's see the result of that inference on our sample video. With this our tutorial on comparison between RTER and RTER has been over. To read in more detail you can check our blog on our labellerrs website. In this blog you will learn about RTER V2 in great detail and compare it with YOLO. Let's see you in another tutorial.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome to this tutorial. In this video, we are going to compare two modern object detectors RTER and its upgrade version RTER V2. But before we get into difference uh between them, let's take a step back and understand the original model that started it all deter and why it needed improvement in the first place. Let's get started. So DER or detection transformers was introduced by Facebook AI in 2020. It was the first object detector to use a transformers encoder architectured removing the need for handcrafted component like anchor boxes region proposals and non-max suppression. The idea was elegant pro pass an image through a CNN flatten the features run them through a transformers and use a fixed set of learned objects queries to directly predict bounding boxes and class labels. But der has some serious limitation. It was very slow to train like often requiring over 500 epochs to converge. It had high inference latency making it making it unsuitable for realtime application. It struggled with small object because it didn't use multiscale features and it wasn't flexible adapting it to tasks like segmentation or key point wasn't a straightforward. So while deter introduced a new direction it wasn't practical for many real world scenarios that's where RT came in developed by BU in 2023 it was it kept the transformers idea but restructured things to make it real time and edge device friendly like some of its key feature include using lightweight CNN backbones like conext or mobile net uh adding multi scale features to improve small object detection using top K query selection instead of decoding over all queries faster and similar matching mechanism uh like uh and ditching the slow Hungarian algorithm and the result was faster training and real-time inference",
      "and decent accuracy artiller proved that transformer speed detector could actually compete with yolo like model in pro production environment. Now let's talk about its upgrade. RT v2 released in 2024. While RTER was all about speed, RTER v2 focus on both versatility and efficiency across platform. Here what RTD V2 improved introduced a hybrid task cascaded allowing it to handle detection instant segmentation keyoint detection and more use multi-resolution deformable attention like uh which resulted in improving feature quality optimize it for low power devices through support for quantization and which make the overall performance better without sacrificing latency making it ideal for both cloud and edge divi edge environment. So to sum it up if you need faster and accurate object detection RT is solid but if you want something that's task flexible plat platform optimize and more powerful overall RT v2 is the best model you need. So let's come back to our comparison. Before going straight for object detection using RTD, we have to first set up some environment for it. I'm going to use Nvidia L4 GPU for my tutorial. Now install some Python libraries which we are going to use in our tutorial. These are transformer torque pillow mattl request open cv tim. Let's check the version of each library. As you are seeing I'm using the latest version of each libraries. So let's import libraries to our notebook. Now I have created a helper function which help me to uh download image using URL and load it in our notebook. Let's run that function. Now I'm using this sample image for our tutorial. As you can see, it contains lots of objects which are very far and in front of you and lots of random objects like a duck or flowers and bicycle",
      "which is in the lower corner of the image. Now for our main code block how I am going to uh use RT editor. I am using transformers module to import two classes. RT editor for object detection, RT editor for image processor and now this device variable helps me to choose between CUDA and CPU if available. So model in this uh we are going to load model. I'm using pecking u R50 VD. It's a simple model which is good for both accuracy and speed and a processor. The processor is here to uh to resize our image like normalize or to cut image to provide it to our model. Now first we are going to provide our image to our processor and input that value to our model. As you can see I'm using tor nat as it helps me to save memory. Now I am pre prep-processing my image. As you can see image size I am reversing it from height to width to width to as PyTorch required in that format. Now let's run that on our image. As we can see uh detection has been observed in our image. So the scores are the confidence level of each objects. Labels represent which object uh belongs to which class and boxes give us the coordinates of our object. But you are seeing in label we are getting uh uh integer as a value but classes name should be of string. We are going to see that later later. Let's see how many classes we how many object we have detected in this image. As you can see I have detected 21 objects in this image. So let's talk about what label means here. Seven. As you know uh RT RT is trained on Coco data",
      "set. So Coco data set has uh classes which index on them. So these are the classes of Coco data set. As you can uh it's a zerobased indexing. So the seven will be here it's a truck and let's see in our image. Yes there is a truck in our image. So let's create a function which will simplify that. As you can see I have created a function which simplify this process and also visualize my results on the image and return the modified image. Let's run this image. run this function. So I have created same function for our RT editor. We just have to change the value here. Uh I am going to use RT editor V2 R50 the similar model of RT editor series. Let's run this function. Let's inference result on our image. As you can see, artillery deter has done a great job in detecting uh objects. It's a detected ducks, bicycle which is in the very corner, person truck, but it has failed to detect some key objects like plant which is in our PCO data set. Let's see what happened with RTD V2. As you can see, it has detected more objects than RT det. It has detected our port. It had detected stop sign. Yes, it has detected person bicycle. It has done a better job at detecting than RT. So let's take in another image for our sample. As you can see this is a image of uh elephants and zebra from a very far away distance. Let's see what happened. First we check RTOR V2. As you can see RT editor has nearly detected all the objects in our image. Let's see what happened with RTD. It has detected but if you see it uh here like uh",
      "it has detected to fail some zebras but our model RT detor V2 has detected all the zebras clearly. So this is a main feature of RTER V2 which is good at detecting object at long distance or of small sizes. Let's take another example. It's a image which contains lots of se let's run that uh inference our model on it. I'm using RT v2 first. So the art v2 has detected all these shapes nearly but it failed to detect these here. As you can see it has leave four save undetected. Let's try this image on our RT v1. Let's it also failed to detect these objects. So board RT failed to detect the seeps which are here. Let's take another example. So in this example you are seeing uh a lots of shapes. Uh so let's inference our model on it. So the inference result is here. So as you can see it has detected all the seeps in our image. Let's try it with art uh RT deter as you can see it is also detected most of the sips clearly but it failed to detect some sips behind which has been clearly detected by RT V2 as you can it has not detected these sips which is uh its downside. inside as it can't detect object which are smaller inside. Let's try our model on a video. The main power of RT series is its capability for realtime inference. So we are going to test it on a video of a football match. This is the sample video I'm going to use as an example. As you can see it has many object like like player footballs people in crowd which we are going to see if both model can detect it clearly. Let's",
      "uh perform inference on the video. For that I have created a function which will inference our model on the video and save it. Let's run this function now. Let's see the result of that inference on our sample video. With this our tutorial on comparison between RTER and RTER has been over. To read in more detail you can check our blog on our labellerrs website. In this blog you will learn about RTER V2 in great detail and compare it with YOLO. Let's see you in another tutorial."
    ],
    "transcript_word_count": 1590,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "L0nhtdvu6z0",
    "title": "YOLOv11 Tutorial: Object Detection, Segmentation, Pose Estimation | Complete Guide for Beginners",
    "description": "Unlock the full potential of YOLOv11 in this comprehensive tutorial! Learn how to perform a range of computer vision tasks-including object detection, image segmentation, pose estimation, oriented object detection, and image classification-using the latest YOLOv11 model.\n\nIn this step-by-step guide, you'll discover:\n- How to install and set up all required libraries (matplotlib, Pillow, Numpy, OpenCV, Ultralytics)\n- The difference between object detection, segmentation, and classification\n\nHow to use YOLOv11 for:\n- Detecting and classifying objects in images\n- Assigning pixel-level labels with image segmentation\n- Detecting rotated objects with oriented bounding boxes\n- Estimating human poses with keypoint detection\n- Classifying images into thousands of categories\n\nYOLOv11 Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/Model%20Notebooks/YOLO/YOLOv11/YOLO-EXP-all-vision-task.ipynb\n\nBlog: https://www.labellerr.com/blog/how-to-perform-yolos-various-task/\n\nChapters\n0:00 Introduction to YOLOv11\n0:39 Installing Required Libraries\n1:17 Object Detection with YOLOv11\n2:49 Image Segmentation with YOLOv11\n6:09 Oriented Object Detection\n8:35 Pose Estimation with YOLOv11\n10:20 Image Classification with YOLOv11\n12:45 Conclusion and Further Resources\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1\n\n#YOLOv11 #ObjectDetection #ImageSegmentation #PoseEstimation #ComputerVision #DeepLearning #Python #Ultralytics #Tutorial #AI",
    "video_url": "https://www.youtube.com/watch?v=L0nhtdvu6z0",
    "embed_url": "https://www.youtube.com/embed/L0nhtdvu6z0",
    "duration": 787,
    "view_count": 308,
    "upload_date": "20250519",
    "uploader": "Labellerr",
    "tags": [
      "bounding box annotation",
      "bounding box",
      "image annotation",
      "bounding box object detection",
      "video annotation",
      "data annotation",
      "bounding boxes",
      "2d bounding box annotation",
      "image segmentation",
      "2d object detection",
      "data labeling",
      "semantic segmentation",
      "object detection",
      "video segmentation",
      "annotation software",
      "image analysis",
      "ai annotation",
      "data labeling service",
      "semantic labeling",
      "annotation workflow",
      "computer vision",
      "image tagging",
      "image processing",
      "ai image tagging"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Welcome to this tutorial on how to perform various reason tasks using YOLO V1. In this tutorial, we are going to explore various tasks you can perform using YOLO V1 which are object detection, image segmentation, image classification, pose estimation, oriented object detection. We will learn what each task does and how to run it using YOLO V1. Let's get started. First, we will start by installing required libraries. I am going to install matt.live Live, pillow, numpy, open CV and ultra litics. We are going to see what each libraries does. Matlo lift to visualize result. Pillow to load data like images in Python friendly environment. Numpy to perform fast numerical operation on image data. Open CV to read and write image or to draw saves or to manipulate pixels. and most important ultraalytics to import our yolo model in our environment. So first task is object detection. So what is it? Object detection finds and classify objects in an image with bounding boxes in it. uh boxing using bounding boxes. Like if I am going to take an uh image as an example, you are going to see there are various images in in various objects in this image like there is a person, there is another person, there is bus, there is cycle and we are going to detect it using uh YOLO v1 model. For object detection, we just have to import YOLO V11 uh N uh point PT variant. Uh N stand for it's the smallest model of YOLO V1 object detection series. We can also detect X variant which is going to be heavy. So now detect our objects in image. So you're going to see there are various objects we have detected using YOLO V1 object detection center like handbag, bicycle, backpack, bus, person and uh car. These are all the things which are in Coco data set. So now let's go to our next uh uh next uh task image segmentation. Image segmentation. What is it? Image segmentation assign class label to each pixel. Perfect for precision understanding of shapes and boundary. Like let's take our previous like in bond in object detection the model just draw bonding boxes around the object. But in image segmentation it will assign each pixel of that object a value like if that belongs to an object or not. Let's take an example image like you are seeing two cowboys uh in front of a beach. So we are going to import our image segmentation model. So image segmentation model we have just to use uh dash seg uh we can also use x variant for this model. Now we are going to use image segmentation in that first we use uh see how many objects we are detect. It has detected two person two horses which is right. Let's visualize these result on our image. As you can see each pixel is getting a different color uh for each class of object. uh for horse each pixel is getting red color or pink color or and for person each pixel is getting a blue color. Through this it is we can better visualize what object is and where it is. So let's take another example like you are seeing uh an image of various vehicles. Let's uh use uh object segment uh image segmentation in it. Now you're seeing the car uh is getting uh white color and the motorcycle is getting green color and the behind it there is a bus which is a pink color. So it give us a better visualization of what object shape is and where it is like but uh the standard yolo model can detect segmentation with bonding boxes which is quite irritating for us. We can create a function special function for that just display segmentation. We will run that function. Now let's visualize our object in uh without any bounding boxes. As you can see the image objects in images are segmented very nicely and there is no bonding boxes uh in it. So let's get to our next uh uh task of YOLO V1 which is oriented object detection. What is oriented object detection? Uh it's same as object detection but it's just rotate bonding boxes. It is useful in aial and aerial or rotated views. Like I have to give an example. Uh you are seeing here there is an aerial view of uh airplane. Uh you can see we are going to first use object detection model on it and see the result. As you are seeing audio detection model has dropped bonding boxes which is taking a lot of space. This area is very much empty. This area is very much empty. So it's create it's creating drawing bonding ball on the axis of X and Y which uh is very space consuming in our image. But using oriented object detection, we we are uh going to visualize these object uh detection bonding boxes using a more precise rotated bonding boxes. As you can see, it has been rotated and a specific angle which matches the uh object orientation and it reduces our bonding box uh area which is good in aial uh aerial viewing like if I have to view from our spy I don't want our bonding boxes to be overlapping each other. So let's take another example. Uh this is an uh aerial view of ship on in ocean. Let's visualize our oriented object detection on it. As you are seeing the or the bonding boxes is clearly in orientation of the shape. It is leaving no extra spaces or area in our detection which is very beneficial aerial surveillance. Now let's get to next vision task which is pose estimation. We we all know about pose estimation but pose estimation is basically key point detection. We just have to join key points uh to make it a skeleton which we call pose estimation in YOLO. We can uh use this model using dash pose. So we import our YOLO model. Let's take an example. This is a basketball athlete as you are seeing it. Uh now let's uh visualize pose estimation on it. So as you are seeing it's detecting key point at every joint like shoulder, elbow, hand there it also is and it's on knees and foot and on head. It's great for analyzing the person orientation in a 3D space. Let's take uh another example. This is an athlete who is running running. In this image, you are seeing an athlete running. So, we will use our pose estimation model on it. uh as we say Euro 11 has detected it perfectly. The knee and joints has been detected very clearly and there is uh his skull and there is his feet. Let's get to our next vision task which is classification. So in what is different with object detection and classification like if I'm detecting object it's also labeling the uh class object object but in classification it's predicting main class of the entire image for example first we import our model which is using dash cls let's take an example in this image we are going to classify the whole image class means the the major object of this image like it's a bird. We have to classify which bird it is. So let's classify it using uh YOLO V1 model. As you can see it's an ostrich. It's saying 100% that it's an ostrich. Let's take another example. Uh it's also a bird. Let detects which bird it is. Uh it's a vulture which is detected by YOLO V1. Let's take another example. It's also a bird. Let's see which bird it is. It's a bald eagle. So in that way we can classify many objects using classification model of YOLO. So let's see how many classification models are there. So, oh, there are 1,000 classification classes in YOLO V11. As you can see, it's ranges from goldface to uh kite to Indian cobra to many more. thousand classes are there for classification. Uh so with this we end our tutorial. To explore more to explore more we recommend you to read our blog on the only uh YOLO varn multi-leing guide you will ever need. In this guide, we are going to explore this tutorial.",
    "transcript_chunks": [
      "[Music] Welcome to this tutorial on how to perform various reason tasks using YOLO V1. In this tutorial, we are going to explore various tasks you can perform using YOLO V1 which are object detection, image segmentation, image classification, pose estimation, oriented object detection. We will learn what each task does and how to run it using YOLO V1. Let's get started. First, we will start by installing required libraries. I am going to install matt.live Live, pillow, numpy, open CV and ultra litics. We are going to see what each libraries does. Matlo lift to visualize result. Pillow to load data like images in Python friendly environment. Numpy to perform fast numerical operation on image data. Open CV to read and write image or to draw saves or to manipulate pixels. and most important ultraalytics to import our yolo model in our environment. So first task is object detection. So what is it? Object detection finds and classify objects in an image with bounding boxes in it. uh boxing using bounding boxes. Like if I am going to take an uh image as an example, you are going to see there are various images in in various objects in this image like there is a person, there is another person, there is bus, there is cycle and we are going to detect it using uh YOLO v1 model. For object detection, we just have to import YOLO V11 uh N uh point PT variant. Uh N stand for it's the smallest model of YOLO V1 object detection series. We can also detect X variant which is going to be heavy. So now detect our objects in image. So you're going to see there are various objects we have detected using YOLO V1 object detection center like handbag, bicycle, backpack,",
      "bus, person and uh car. These are all the things which are in Coco data set. So now let's go to our next uh uh next uh task image segmentation. Image segmentation. What is it? Image segmentation assign class label to each pixel. Perfect for precision understanding of shapes and boundary. Like let's take our previous like in bond in object detection the model just draw bonding boxes around the object. But in image segmentation it will assign each pixel of that object a value like if that belongs to an object or not. Let's take an example image like you are seeing two cowboys uh in front of a beach. So we are going to import our image segmentation model. So image segmentation model we have just to use uh dash seg uh we can also use x variant for this model. Now we are going to use image segmentation in that first we use uh see how many objects we are detect. It has detected two person two horses which is right. Let's visualize these result on our image. As you can see each pixel is getting a different color uh for each class of object. uh for horse each pixel is getting red color or pink color or and for person each pixel is getting a blue color. Through this it is we can better visualize what object is and where it is. So let's take another example like you are seeing uh an image of various vehicles. Let's uh use uh object segment uh image segmentation in it. Now you're seeing the car uh is getting uh white color and the motorcycle is getting green color and the behind it there is a bus which is a pink color. So it give us a better visualization",
      "of what object shape is and where it is like but uh the standard yolo model can detect segmentation with bonding boxes which is quite irritating for us. We can create a function special function for that just display segmentation. We will run that function. Now let's visualize our object in uh without any bounding boxes. As you can see the image objects in images are segmented very nicely and there is no bonding boxes uh in it. So let's get to our next uh uh task of YOLO V1 which is oriented object detection. What is oriented object detection? Uh it's same as object detection but it's just rotate bonding boxes. It is useful in aial and aerial or rotated views. Like I have to give an example. Uh you are seeing here there is an aerial view of uh airplane. Uh you can see we are going to first use object detection model on it and see the result. As you are seeing audio detection model has dropped bonding boxes which is taking a lot of space. This area is very much empty. This area is very much empty. So it's create it's creating drawing bonding ball on the axis of X and Y which uh is very space consuming in our image. But using oriented object detection, we we are uh going to visualize these object uh detection bonding boxes using a more precise rotated bonding boxes. As you can see, it has been rotated and a specific angle which matches the uh object orientation and it reduces our bonding box uh area which is good in aial uh aerial viewing like if I have to view from our spy I don't want our bonding boxes to be overlapping each other. So let's take another example. Uh",
      "this is an uh aerial view of ship on in ocean. Let's visualize our oriented object detection on it. As you are seeing the or the bonding boxes is clearly in orientation of the shape. It is leaving no extra spaces or area in our detection which is very beneficial aerial surveillance. Now let's get to next vision task which is pose estimation. We we all know about pose estimation but pose estimation is basically key point detection. We just have to join key points uh to make it a skeleton which we call pose estimation in YOLO. We can uh use this model using dash pose. So we import our YOLO model. Let's take an example. This is a basketball athlete as you are seeing it. Uh now let's uh visualize pose estimation on it. So as you are seeing it's detecting key point at every joint like shoulder, elbow, hand there it also is and it's on knees and foot and on head. It's great for analyzing the person orientation in a 3D space. Let's take uh another example. This is an athlete who is running running. In this image, you are seeing an athlete running. So, we will use our pose estimation model on it. uh as we say Euro 11 has detected it perfectly. The knee and joints has been detected very clearly and there is uh his skull and there is his feet. Let's get to our next vision task which is classification. So in what is different with object detection and classification like if I'm detecting object it's also labeling the uh class object object but in classification it's predicting main class of the entire image for example first we import our model which is using dash cls let's take an example in this",
      "image we are going to classify the whole image class means the the major object of this image like it's a bird. We have to classify which bird it is. So let's classify it using uh YOLO V1 model. As you can see it's an ostrich. It's saying 100% that it's an ostrich. Let's take another example. Uh it's also a bird. Let detects which bird it is. Uh it's a vulture which is detected by YOLO V1. Let's take another example. It's also a bird. Let's see which bird it is. It's a bald eagle. So in that way we can classify many objects using classification model of YOLO. So let's see how many classification models are there. So, oh, there are 1,000 classification classes in YOLO V11. As you can see, it's ranges from goldface to uh kite to Indian cobra to many more. thousand classes are there for classification. Uh so with this we end our tutorial. To explore more to explore more we recommend you to read our blog on the only uh YOLO varn multi-leing guide you will ever need. In this guide, we are going to explore this tutorial."
    ],
    "transcript_word_count": 1395,
    "transcript_chunk_count": 5
  },
  {
    "video_id": "O9QX8W9xXJY",
    "title": "OWLv2: Zero-Shot Vision-Language Model Explained + Hands-on Demo",
    "description": "Owl Vision-Language Model Tutorial | Zero-Shot Object Detection with OWLv2\nWelcome to this in-depth tutorial on OWL (Open-World Learning), a powerful vision-language model by Google that enables zero-shot object detection and classification — no training or fine-tuning required!\n\nIn this video, you'll learn:\n- What is OWLv2 and how it works\n- Key features like zero-shot detection, Open Vocabulary, etc\n- How OWLv2 combines vision and language\n- Environment setup and required libraries\n- How to visualize object detection using text prompts\n\nOWLv2 Notebook: https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/tree/main/Model%20Notebooks/OWL/OWLv2\n\nBlog: https://www.labellerr.com/blog/how-to-perform-various-task-using-owl-v2/\n\nChapters\n0:00 Introduction\n0:27 Overview of OWLv2 and Its Capabilities\n1:33 Setting Up the Environment and Libraries\n4:02 Zero-Shot Detection Demonstration\n9:30 Open Vocabulary Classification\n12:05 Detecting Similar Objects Between Images\n13:52 Conclusion and Further Resources\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=O9QX8W9xXJY",
    "embed_url": "https://www.youtube.com/embed/O9QX8W9xXJY",
    "duration": 850,
    "view_count": 267,
    "upload_date": "20250514",
    "uploader": "Labellerr",
    "tags": [
      "similarity search",
      "vision transformer",
      "transformers pytorch",
      "ocr",
      "vlm",
      "LLM",
      "deep learning",
      "vision llm",
      "Vision language models",
      "OWL",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "ai tools",
      "data management",
      "data preprocessing",
      "image classification",
      "cvat features",
      "labeling software",
      "artificial intelligence",
      "image captioning",
      "text to image ai",
      "machine learning",
      "object detection",
      "Machine Learning",
      "Natural Language Processing",
      "NLP"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] Hello everyone, welcome to this tutorial. In this tutorial, we are going to learn how to implement uh various features of owl model. Uh before moving into that, we have to learn what is owl and what key features it can perform. Let's dive into it. Uh so in this tutorial we we are going to explore what is all and its capabilities how to set up its environment and what key features it can look like zero sort detection and other other various features and we have to visualize this feature on images. So what is owl? Owl is a vision language model means using language we can detect various object and also classify them. It sounds uh interesting but it is very uh very it's make our work very much easier like in in traditional models we have to train it on a specific data set to detect it but using all you have to just specify or describe the object and it will automatically detect it. Uh so the the task of uh fine-tuning or training training the model has gone. So let's uh download our required libraries for it. So I I'm going to use Nvidia L4 GPU for my notebook. So what are the libraries I'm going to need? So as you can see I have uh download installed request to download image from uh internet pillow to work with to work with images torque to inference model result and to transformers to uh install model from hugging face matt live to to draw bounding boxes on it and skyp is also necessary for So let's check the version of each library. As you can see I am using uh hugging uh these uh these version of libraries. So let's import it. Now uh our first main task we have to import our model from hugging face. We can use transformers module of uh hugging face. We have to just use V2 processor and import this uh model which is used for model detection. Uh we have to create two instance a processor and a model. Uh now we are going to create some helper function for our owl models like a function to load image from the internet. As you can see it is using request library and image library to download an image and use in in our notebook. And another function which are going which is going to take uh the image the queries we are going to give to the all model processor and model which we have loaded and threshold means what at what confidence image should be considered as detection. So and it will provide us with result we are going to use to draw bonding boxes like I have created a special function for it to uh draw bonding boxes for for our detection. Now let's move into its first capability zero sort detection. As you can see owl is famous for detecting detecting object without specifically training it. So, so we it's a zero sort model. So, what is zero sort model? Zero sort detection is a technique that allows model to detect and identify objects in images without ever being explicit trained on specific objects. So like I am going to use a sample image like this. As you can see there is a c cowboy catching a cow using rope. So uh I have entered a query of cow and horse to and I have provided with threshold level of 0.7. So now we are going to see how it detect object. As you can see uh our image has uh detected a cow and a horse. A as you can there is a cow and a horse but you are missing it. If I load lower this value like take example for one now what happened. Now you can see I'm detecting various instances of horse and cow but you are not seeing in this image. Let's draw bonding boxes to see our result. Uh as you can see there is a horse. There's a horse and in this background there are also horse and cows which owl is detecting. To see it clearly we we will see the original image. As you can see uh I will zoom it. Uh as you can see there are some horses or cow which we humans cannot see it clearly but model can detect it but the these cows and these cow and horses are not our main focus. Uh so and let's see what will happen if we increase the threshold as you are seeing I am getting no result means no object is detected in the confidence range above and 0.9 so I will keep it 0.7. So I as you can see I have detected one instance of cow and a single instance of horse. So drawing that on our image we can see there is a cow with 70 0.76 uh confidence and horse 0.73 confidence which is right. So let's take another example like let's use this image. This image contain various objects like cloth, shoes, toy car, books, mobile phone, uh, some goggles and various other objects. Now we are going to provide a query which contain the name like S, Jeep, phone, glasses, pen and etc. Let's see what happen. Our model has detected these objects like let's see uh yes it had detected cloth on on which our object is placed a toy there is also a toy there is also a toy there is also a cloth s uh s and uh and glasses and book a bug and books here. So it's working fine. Let's see how many objects are there in it has detected 22 objects. It has totally detected. Now let's take another image for example. Uh it as you can see is a very dark image. It's contain various objects like glass glasses, uh cup, candle, books, clock and I have given the same the name as query. So let's move on. Now it has detected our query object like candle. It has detected candle two times. Uh glasses, book uh books, cup and clock. As you can see, it is all uh detected all the query in our uh all the objects of our query. Uh let's move on to our next key feature. Open vocabulary classification. To explain it simply I will give you example like this is a glass I am wearing it can but if I am wearing sun goggles it also we consider a glasses but it's also called as sads so we have to if if a model is using a single label it will not consider sads as glass and goggles but has a open vocabulary so it's using a natural language means uh many objects Objects can have many name. Let's take an example. This is an image of objects. Uh but we have to focus on this object like sun goggles and this glasses. These both can considered glasses. But only this can consider as sads. So let's move on. Let's see how what will happen when we input glasses. As we can see it has detected both our glasses. Uh it uh it has it as a sung goggles also as glasses and our normal glasses as glasses. So let's what see what happen when we input says as an in uh query. So we can see it has only detected sades sades of said as this example means some goggles can be considered as said but not this goggles can consider as sad. So it's is a feature of all that a single object can have various names uh which is very useful in real world scenarios like I can't print OB an object on various labels but using implementing in v uh using NLP natural language I can uh train it like if I have to detect camera modern camera with various name I can use owl Uh let's see also see how many instance of states have detected only one which is this. Let's move to its next key feature. Detect similar objects between two images using all. Uh before moving I have to show something like there's an object in this image and there is objects in this image. If I have to find this object which is a cat in this image, I'll can help it. I don't have to give it give it a description like find cat in this. I have to just provide this image as a query and this image as a target to find these this object in uh this image. So let's uh see what happened. As you can see my target image is provided as image and c in place of queries I have just provided the query image. So, let's see. As you can see, four instance of that object is found in our target image. Let's see our target image. As you can see, there are four cats in our objects. Let's draw bonding boxes around it. Yes, the owl model has predicted our detected our objects correctly. There are four cats and these are added there. So it's a very good feature without using any description. We can also found objects in owl. Uh with this I will end my tutorial. You can also read about in in my article on labellerr platform. Uh how to perform object detection task using our V2 model.",
    "transcript_chunks": [
      "[Music] Hello everyone, welcome to this tutorial. In this tutorial, we are going to learn how to implement uh various features of owl model. Uh before moving into that, we have to learn what is owl and what key features it can perform. Let's dive into it. Uh so in this tutorial we we are going to explore what is all and its capabilities how to set up its environment and what key features it can look like zero sort detection and other other various features and we have to visualize this feature on images. So what is owl? Owl is a vision language model means using language we can detect various object and also classify them. It sounds uh interesting but it is very uh very it's make our work very much easier like in in traditional models we have to train it on a specific data set to detect it but using all you have to just specify or describe the object and it will automatically detect it. Uh so the the task of uh fine-tuning or training training the model has gone. So let's uh download our required libraries for it. So I I'm going to use Nvidia L4 GPU for my notebook. So what are the libraries I'm going to need? So as you can see I have uh download installed request to download image from uh internet pillow to work with to work with images torque to inference model result and to transformers to uh install model from hugging face matt live to to draw bounding boxes on it and skyp is also necessary for So let's check the version of each library. As you can see I am using uh hugging uh these uh these version of libraries. So let's import it. Now uh",
      "our first main task we have to import our model from hugging face. We can use transformers module of uh hugging face. We have to just use V2 processor and import this uh model which is used for model detection. Uh we have to create two instance a processor and a model. Uh now we are going to create some helper function for our owl models like a function to load image from the internet. As you can see it is using request library and image library to download an image and use in in our notebook. And another function which are going which is going to take uh the image the queries we are going to give to the all model processor and model which we have loaded and threshold means what at what confidence image should be considered as detection. So and it will provide us with result we are going to use to draw bonding boxes like I have created a special function for it to uh draw bonding boxes for for our detection. Now let's move into its first capability zero sort detection. As you can see owl is famous for detecting detecting object without specifically training it. So, so we it's a zero sort model. So, what is zero sort model? Zero sort detection is a technique that allows model to detect and identify objects in images without ever being explicit trained on specific objects. So like I am going to use a sample image like this. As you can see there is a c cowboy catching a cow using rope. So uh I have entered a query of cow and horse to and I have provided with threshold level of 0.7. So now we are going to see how it detect object. As you",
      "can see uh our image has uh detected a cow and a horse. A as you can there is a cow and a horse but you are missing it. If I load lower this value like take example for one now what happened. Now you can see I'm detecting various instances of horse and cow but you are not seeing in this image. Let's draw bonding boxes to see our result. Uh as you can see there is a horse. There's a horse and in this background there are also horse and cows which owl is detecting. To see it clearly we we will see the original image. As you can see uh I will zoom it. Uh as you can see there are some horses or cow which we humans cannot see it clearly but model can detect it but the these cows and these cow and horses are not our main focus. Uh so and let's see what will happen if we increase the threshold as you are seeing I am getting no result means no object is detected in the confidence range above and 0.9 so I will keep it 0.7. So I as you can see I have detected one instance of cow and a single instance of horse. So drawing that on our image we can see there is a cow with 70 0.76 uh confidence and horse 0.73 confidence which is right. So let's take another example like let's use this image. This image contain various objects like cloth, shoes, toy car, books, mobile phone, uh, some goggles and various other objects. Now we are going to provide a query which contain the name like S, Jeep, phone, glasses, pen and etc. Let's see what happen. Our model has detected these objects like let's",
      "see uh yes it had detected cloth on on which our object is placed a toy there is also a toy there is also a toy there is also a cloth s uh s and uh and glasses and book a bug and books here. So it's working fine. Let's see how many objects are there in it has detected 22 objects. It has totally detected. Now let's take another image for example. Uh it as you can see is a very dark image. It's contain various objects like glass glasses, uh cup, candle, books, clock and I have given the same the name as query. So let's move on. Now it has detected our query object like candle. It has detected candle two times. Uh glasses, book uh books, cup and clock. As you can see, it is all uh detected all the query in our uh all the objects of our query. Uh let's move on to our next key feature. Open vocabulary classification. To explain it simply I will give you example like this is a glass I am wearing it can but if I am wearing sun goggles it also we consider a glasses but it's also called as sads so we have to if if a model is using a single label it will not consider sads as glass and goggles but has a open vocabulary so it's using a natural language means uh many objects Objects can have many name. Let's take an example. This is an image of objects. Uh but we have to focus on this object like sun goggles and this glasses. These both can considered glasses. But only this can consider as sads. So let's move on. Let's see how what will happen when we input glasses. As we can",
      "see it has detected both our glasses. Uh it uh it has it as a sung goggles also as glasses and our normal glasses as glasses. So let's what see what happen when we input says as an in uh query. So we can see it has only detected sades sades of said as this example means some goggles can be considered as said but not this goggles can consider as sad. So it's is a feature of all that a single object can have various names uh which is very useful in real world scenarios like I can't print OB an object on various labels but using implementing in v uh using NLP natural language I can uh train it like if I have to detect camera modern camera with various name I can use owl Uh let's see also see how many instance of states have detected only one which is this. Let's move to its next key feature. Detect similar objects between two images using all. Uh before moving I have to show something like there's an object in this image and there is objects in this image. If I have to find this object which is a cat in this image, I'll can help it. I don't have to give it give it a description like find cat in this. I have to just provide this image as a query and this image as a target to find these this object in uh this image. So let's uh see what happened. As you can see my target image is provided as image and c in place of queries I have just provided the query image. So, let's see. As you can see, four instance of that object is found in our target image. Let's see our",
      "target image. As you can see, there are four cats in our objects. Let's draw bonding boxes around it. Yes, the owl model has predicted our detected our objects correctly. There are four cats and these are added there. So it's a very good feature without using any description. We can also found objects in owl. Uh with this I will end my tutorial. You can also read about in in my article on labellerr platform. Uh how to perform object detection task using our V2 model."
    ],
    "transcript_word_count": 1587,
    "transcript_chunk_count": 6
  },
  {
    "video_id": "E82zMbnw37Q",
    "title": "How AI Detects Threats : Image Annotation With Labellerr",
    "description": "In this video, we’ll show you how to train AI models to recognize threats using Labellerr’s intuitive annotation platform. Watch as we:\n\n- Upload a real-world surveillance image\n- Use the bounding box tool to highlight the person\n- Assign attributes to describe actions like “threatening gesture”\n- Apply SAM for pixel-perfect segmentation of the knife\n- Classify the frame as a potential security risk\n\nLabellerr streamlines the entire annotation process, making it easy to create high-quality training data for computer vision and security AI. With support for automated labeling, advanced segmentation, attribute tagging, and classification, Labellerr helps you build smarter, safer AI systems-faster and more accurately\n\nChapters\n0:00 Introduction to Threat Detection\n0:12 Why AI is Essential for Security Surveillance\n0:28 Uploading Image to Labellerr\n0:34 Using Bounding Box Tool\n0:38 Assigning Threatening Gesture Attribute\n0:45 Annotating Knife with SAM Segmentation\n0:52 Classifying Frame as Security Risk\n1:01 Creating Annotated Frame for AI Training\n1:11 Learn More and Book a Demo\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=E82zMbnw37Q",
    "embed_url": "https://www.youtube.com/embed/E82zMbnw37Q",
    "duration": 76,
    "view_count": 62,
    "upload_date": "20250509",
    "uploader": "Labellerr",
    "tags": [
      "polygon annotation",
      "machine learning",
      "Machine Learning",
      "visual data",
      "image ai",
      "image tagging",
      "image processing",
      "computer vision",
      "data science",
      "ai ai camera",
      "facial recognition cv",
      "camera surveillance",
      "tracking cameras",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "ai tools",
      "data management",
      "image segmentation",
      "data labelling software",
      "image metadata",
      "deep learning",
      "dataset creation",
      "metadata management",
      "image classification"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "How many of you noticed the knife in this person's hand? If you did, great job. But remember, this is just a single frame. Imagine trying to spot such details in thousands of frames, 24/7. It's simply impossible for any human to maintain that level of concentration all the time. That's why AI models are deployed for security and surveillance. But for AI to recognize threats, we first need to teach it what to look for. This is where labellerr steps up. I've uploaded the image into labellerr. First, I'll use the bounding box tool to highlight the person. I'll then add attribute to the man highlighting his action as threatening gesture. Next, I'll annotate the knife using SAM for precise segmentation. Then, I'll use the classification feature to tag this frame as a potential security risk. This helps the AI not just see but understand the context. In just a minute, we've created a perfectly annotated frame ready to train AI for real world security. And if you're worried about how we can do such tasks in a video, we already have that covered on our channel. Book a demo to learn more. Link in the description.",
    "transcript_chunks": [
      "How many of you noticed the knife in this person's hand? If you did, great job. But remember, this is just a single frame. Imagine trying to spot such details in thousands of frames, 24/7. It's simply impossible for any human to maintain that level of concentration all the time. That's why AI models are deployed for security and surveillance. But for AI to recognize threats, we first need to teach it what to look for. This is where labellerr steps up. I've uploaded the image into labellerr. First, I'll use the bounding box tool to highlight the person. I'll then add attribute to the man highlighting his action as threatening gesture. Next, I'll annotate the knife using SAM for precise segmentation. Then, I'll use the classification feature to tag this frame as a potential security risk. This helps the AI not just see but understand the context. In just a minute, we've created a perfectly annotated frame ready to train AI for real world security. And if you're worried about how we can do such tasks in a video, we already have that covered on our channel. Book a demo to learn more. Link in the description."
    ],
    "transcript_word_count": 198,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "lBsC5lgE4A4",
    "title": "How AI Learns to See: Image Segmentation with Labellerr",
    "description": "We’re used to seeing AI remove people or objects from photos in seconds—but how does it actually understand what to remove? The answer lies in image segmentation.\n\nIn this video, we show you how image segmentation works using Labellerr. We annotate an image step by step—segmenting the lady, the man, and the background. It’s fast, accurate, and flexible.\n\nBut no model is perfect. We reveal a real-world limitation of Segment Anything Model (SAM)—where a hand gets skipped due to occlusion—and show how to fix it using Labellerr’s eraser and group tools. You’ll see how you can easily reassign annotations so your AI learns the right object relationships.\n\nWe also touch on CLIP mode, a feature that helps you close tiny gaps between objects for pixel-perfect segmentation\n\nChapters\n0:00 Introduction to Image Segmentation\n0:22 Uploading and Annotating the Lady\n0:31 Segmenting the Man\n0:36 Segmenting the Background\n0:44 Reviewing Annotations\n0:47 Identifying SAM's Limitation\n0:58 Fixing Errors with Eraser Tool\n1:11 Adding New Annotations\n1:14 Using Group Tool for Object Relationships\n1:27 Closing Gaps with CLIP Mode\n1:52 Final Segmentation Results\n1:59 Learn More and Book a Demo\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=lBsC5lgE4A4",
    "embed_url": "https://www.youtube.com/embed/lBsC5lgE4A4",
    "duration": 125,
    "view_count": 33,
    "upload_date": "20250508",
    "uploader": "Labellerr",
    "tags": [
      "object detection",
      "image classification",
      "object annotation",
      "image localization",
      "pixel tracking",
      "deep learning",
      "image processing",
      "python",
      "instance segmentation",
      "machine learning",
      "segment anything",
      "ml",
      "data science",
      "nlp",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "ai tools",
      "data management",
      "OpenCV",
      "Segmentation",
      "Tensorflow",
      "Deep Learning",
      "image segmentation",
      "natural language processing"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "We're all amazed by what AI can do these days, like removing backgrounds or even people from images with just a click. But have you ever wondered how AI models actually achieve these complex tasks? The secret behind these results is data segmentation. It teaches AI to recognize and separate different parts of an image. And that's exactly where labellerr comes in. Here I've uploaded an image in our tool. Firstly, let's annotate the lady. In just a few clicks, we're done with it. Now, let's segment the man. Fast, right? At last, let's segment the background. I'll quickly do it. So, here we are. With just a few clicks, all the primary objects are annotated, fast and accurate. But is it perfect? Let's see by decreasing the transparency. Notice how Sam has a limitation. It hasn't labeled this region of hand under man as it is separated by the lady. But don't worry, you don't have to start over. Just use the eraser tool to remove the error. Instead of manual annotation, I'll use Sam with the eraser tool, too. Then add a new annotation for that area. At last, use the group tool to assign the same ID. This way, the model learns that both objects belong to the same person. labellerr is that flexible for your problems. See how both these objects are assigned ID number one. Now, let's close gaps between the objects for pixel perfect segmentation. We have clip mode to help you out with this. We've already covered the demo of clip mode on our channel. Do check that out. See how precise and smoothly the borders are labeled. Even though you could manually drag the key points to adjust the common borders, using clip mode is a faster and better solution. So, in just a few minutes, we've segmented this image with accuracy and ease. I hope you have now understood this concept. To know more, book a demo with Leveler. Link in the description.",
    "transcript_chunks": [
      "We're all amazed by what AI can do these days, like removing backgrounds or even people from images with just a click. But have you ever wondered how AI models actually achieve these complex tasks? The secret behind these results is data segmentation. It teaches AI to recognize and separate different parts of an image. And that's exactly where labellerr comes in. Here I've uploaded an image in our tool. Firstly, let's annotate the lady. In just a few clicks, we're done with it. Now, let's segment the man. Fast, right? At last, let's segment the background. I'll quickly do it. So, here we are. With just a few clicks, all the primary objects are annotated, fast and accurate. But is it perfect? Let's see by decreasing the transparency. Notice how Sam has a limitation. It hasn't labeled this region of hand under man as it is separated by the lady. But don't worry, you don't have to start over. Just use the eraser tool to remove the error. Instead of manual annotation, I'll use Sam with the eraser tool, too. Then add a new annotation for that area. At last, use the group tool to assign the same ID. This way, the model learns that both objects belong to the same person. labellerr is that flexible for your problems. See how both these objects are assigned ID number one. Now, let's close gaps between the objects for pixel perfect segmentation. We have clip mode to help you out with this. We've already covered the demo of clip mode on our channel. Do check that out. See how precise and smoothly the borders are labeled. Even though you could manually drag the key points to adjust the common borders, using clip mode is a faster and better solution. So, in",
      "just a few minutes, we've segmented this image with accuracy and ease. I hope you have now understood this concept. To know more, book a demo with Leveler. Link in the description."
    ],
    "transcript_word_count": 332,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "ZZhtAJXY-kk",
    "title": "Why AI Still Struggles with Hands and How labellerr fixes It!",
    "description": "Ever noticed how AI-generated hands often look weird? Too many fingers, missing parts, or strange shapes—it’s not magic gone wrong. It’s usually bad training data. When datasets are poorly annotated, AI models learn those same mistakes.\n\nIn this video, we’ll show you exactly how to fix those errors using Labellerr. We start with a pre-annotated image of a hand and walk you through correcting misplaced keypoints, adding missing parts like the wrist, and cleaning up unnecessary annotations. You can also adjust things like the size and transparency of dots for better clarity.\n\nWhether you're switching from another tool or starting from scratch, Labellerr adapts to your workflow. It supports both raw and pre-annotated uploads and gives you full control over the annotation process.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=ZZhtAJXY-kk",
    "embed_url": "https://www.youtube.com/embed/ZZhtAJXY-kk",
    "duration": 95,
    "view_count": 43,
    "upload_date": "20250507",
    "uploader": "Labellerr",
    "tags": [
      "computer graphics",
      "computer vision",
      "gesture detection",
      "YOLO Object Detection",
      "Pose Estimation",
      "ML Datasets",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "ai tools",
      "data management",
      "image segmentation",
      "data labelling software",
      "image metadata",
      "stable diffusion",
      "labeling software",
      "image classification",
      "deep learning",
      "data preprocessing",
      "metadata management",
      "keypoint annotation",
      "hand tracking",
      "motion capture"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "AI generated hands often look unnatural. Extra fingers, missing joints or palms that seem to melt into wrists. Why does this happen? One major reason, poorly annotated training data. If the original data set has mislabeled hand structures, the AI learns those mistakes. Fixing these errors starts with precise annotation. That's where Labelair steps in. A platform flexible enough to handle pre-anotated images and raw data. Let's see how it works. Here's a pre-anotated hand image uploaded via SDK. Notice the misplaced dots on the knuckles. These errors could confuse AI models. If the annotation points don't match real hand anatomy, the model learns the wrong patterns and produces unrealistic results. That's why it's so important to review and correct these details. So, yes, corrected them by simply dragging the key points. You can create new objects, too, if something necessary is missing, like the wrist here. And if you want to delete something unnecessary, simply select the annotation and press the delete key. We also provide an option to adjust the radius and transparency of the dots. Once annotations are fixed, use the classification feature to tag this image. We know that it's a left human hand, so let's classify it. So whether you're migrating from another tool or starting fresh, Leveler adapts to your workflow. Book a demo with labellerr to know more about different annotation types. Link in the description.",
    "transcript_chunks": [
      "AI generated hands often look unnatural. Extra fingers, missing joints or palms that seem to melt into wrists. Why does this happen? One major reason, poorly annotated training data. If the original data set has mislabeled hand structures, the AI learns those mistakes. Fixing these errors starts with precise annotation. That's where Labelair steps in. A platform flexible enough to handle pre-anotated images and raw data. Let's see how it works. Here's a pre-anotated hand image uploaded via SDK. Notice the misplaced dots on the knuckles. These errors could confuse AI models. If the annotation points don't match real hand anatomy, the model learns the wrong patterns and produces unrealistic results. That's why it's so important to review and correct these details. So, yes, corrected them by simply dragging the key points. You can create new objects, too, if something necessary is missing, like the wrist here. And if you want to delete something unnecessary, simply select the annotation and press the delete key. We also provide an option to adjust the radius and transparency of the dots. Once annotations are fixed, use the classification feature to tag this image. We know that it's a left human hand, so let's classify it. So whether you're migrating from another tool or starting fresh, Leveler adapts to your workflow. Book a demo with labellerr to know more about different annotation types. Link in the description."
    ],
    "transcript_word_count": 233,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "K_dHbA_Rux4",
    "title": "Preserving Old/Damaged Photos With Labellerr",
    "description": "Imagine stumbling upon an old photograph—maybe a historic street, a family gathering, or a forgotten moment frozen in time. But without context, it's just an image. Who’s in it? When was it taken? What story does it hold?\n\nIn this video, I use Labellerr’s Gen-AI features to uncover the details behind an old photo. From generating captions and predicting the era, to tagging objects like flags and classifying image quality — this tool does in minutes what used to take hours.\n\nWhether you're preserving family albums or working on historical archives, Labellerr makes annotation smart, quick, and reliable. Watch how effortlessly it brings the past to life.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=K_dHbA_Rux4",
    "embed_url": "https://www.youtube.com/embed/K_dHbA_Rux4",
    "duration": 120,
    "view_count": 27,
    "upload_date": "20250505",
    "uploader": "Labellerr",
    "tags": [
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "image segmentation",
      "annotation software",
      "image datasets",
      "image classification",
      "segmentation software",
      "machine learning",
      "data science",
      "data preprocessing",
      "segmented images",
      "data engineering",
      "dataset generator",
      "labeling software",
      "visual data",
      "data annotation tools",
      "data collection",
      "object detection",
      "photo annotation",
      "AI photo tagging",
      "photo metadata",
      "annotate photos",
      "image tagging software"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Imagine finding an old photo in your attic, maybe a family portrait or a historic street view. These photos connect us to stories and memories from the past. But to truly preserve them, we need context. Who's in the photo, when it was taken, and what it shows. Doing all of that manually, it takes time, effort, and lots of guesswork. That's where Libler helps. So, here we are. I've uploaded this old photo in our tool. Now, take a guess. When do you think this photo was clicked? No worries. Our Gen AI can help. I select the captioning feature here and type the prompt to get my required answer and then save it. Now, after returning to the labeling window, click predict and Gen AI provides us the answer. Of course, a human should still verify the result, but just see how much time you saved. Without Gen AI, you'd still be scrolling through Google trying to figure it out. Next, let's add more details. labellerr has an attribute feature for that. Let's say I've annotated this flag. Now I can add an attribute, for example, the nation it represents. This makes the data richer and easier to search later. Now after annotating the flag using SAM, we'll see the attribute field. Let's type America here. And that's it. You can also classify the entire image. This adds a global tag to the photo. labellerr gives you options to choose the classification type. I'll go with the drop down and classify this image based on its quality. What used to take hours can now be done in just a few minutes with a few clicks. Whether you're a historian, an archivist, or just organizing old family albums, Leeler makes it fast, consistent, and easy. So go ahead, bring your old photos to life. Tag your history faster and smarter with labellerr. Book a demo with labellerr to know more about annotation. Link in the description.",
    "transcript_chunks": [
      "Imagine finding an old photo in your attic, maybe a family portrait or a historic street view. These photos connect us to stories and memories from the past. But to truly preserve them, we need context. Who's in the photo, when it was taken, and what it shows. Doing all of that manually, it takes time, effort, and lots of guesswork. That's where Libler helps. So, here we are. I've uploaded this old photo in our tool. Now, take a guess. When do you think this photo was clicked? No worries. Our Gen AI can help. I select the captioning feature here and type the prompt to get my required answer and then save it. Now, after returning to the labeling window, click predict and Gen AI provides us the answer. Of course, a human should still verify the result, but just see how much time you saved. Without Gen AI, you'd still be scrolling through Google trying to figure it out. Next, let's add more details. labellerr has an attribute feature for that. Let's say I've annotated this flag. Now I can add an attribute, for example, the nation it represents. This makes the data richer and easier to search later. Now after annotating the flag using SAM, we'll see the attribute field. Let's type America here. And that's it. You can also classify the entire image. This adds a global tag to the photo. labellerr gives you options to choose the classification type. I'll go with the drop down and classify this image based on its quality. What used to take hours can now be done in just a few minutes with a few clicks. Whether you're a historian, an archivist, or just organizing old family albums, Leeler makes it fast, consistent, and easy. So go ahead, bring",
      "your old photos to life. Tag your history faster and smarter with labellerr. Book a demo with labellerr to know more about annotation. Link in the description."
    ],
    "transcript_word_count": 327,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "it4JT8kQavM",
    "title": "How to Use Natural Language Search in Labellerr: Find Datasets with Ease!",
    "description": "In this video, I'll show you how to use the powerful natural language search feature in Labellerr to quickly find datasets without having to remember exact names. Whether you're searching for a specific image or content , just type your prompt and let the tool do the rest! Watch to learn how to refine your search, view relevant datasets, and more.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=it4JT8kQavM",
    "embed_url": "https://www.youtube.com/embed/it4JT8kQavM",
    "duration": 72,
    "view_count": 40,
    "upload_date": "20250501",
    "uploader": "Labellerr",
    "tags": [
      "Natural language search",
      "Smart search AI",
      "Data filtering tool",
      "Photo search software",
      "AI-powered data retrieval",
      "AI search technology",
      "Data set search tool",
      "Smart data retrieval",
      "Data search tool",
      "AI image filtering",
      "Image search AI",
      "Photo dataset search",
      "AI data search",
      "AI search tool",
      "AI data filtering",
      "AI-powered search",
      "Natural language query",
      "Photo search AI",
      "Smart data search",
      "Image data filtering",
      "labellerr data search"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Hey, today I'll show you how to use natural language search in labellerr's tool. First, go to the settings section. Here you will see all the data sets attached to your workspace. You can search for the data set you need by typing its name in the search bar. But what if you don't remember the exact data set name? For example, the one containing a gray car. So in such case, instead of manually checking each data set, you can activate the natural language search feature. Simply enter a prompt describing what you are looking for, like data set with a gray car, and the tool will filter out the relevant files for you. Once the results appear, you can view the images and all related details. You can also adjust the confidence level from here to refine the search based on how complex your prompt is. Clicking show data sets will display the data sets that match your query. Finding specific data sets is this simple with labellerr. We have covered other features on our channel, so be sure to check those out, too.",
    "transcript_chunks": [
      "Hey, today I'll show you how to use natural language search in labellerr's tool. First, go to the settings section. Here you will see all the data sets attached to your workspace. You can search for the data set you need by typing its name in the search bar. But what if you don't remember the exact data set name? For example, the one containing a gray car. So in such case, instead of manually checking each data set, you can activate the natural language search feature. Simply enter a prompt describing what you are looking for, like data set with a gray car, and the tool will filter out the relevant files for you. Once the results appear, you can view the images and all related details. You can also adjust the confidence level from here to refine the search based on how complex your prompt is. Clicking show data sets will display the data sets that match your query. Finding specific data sets is this simple with labellerr. We have covered other features on our channel, so be sure to check those out, too."
    ],
    "transcript_word_count": 186,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "99Cr4KuvcvA",
    "title": "How Computer Vision is Transforming Sports, On and Off The Field",
    "description": "In this video, we explore how computer vision is reshaping the world of sports—both on and off the field. From preventing injuries and automating officiating to enhancing performance analytics and elevating the fan experience, computer vision brings unmatched precision and insights to the game.\n\nWe also highlight how Labellerr plays a crucial role by enabling high-quality data annotation that powers these advanced AI systems. Whether it's labeling key actions, tagging game events, or tracking player attributes, Labellerr helps build the foundation for smarter sports technology.\n\nChapters\n0:00 Introduction to Computer Vision in Sports\n0:21 Challenges in Sports Today\n1:10 How Computer Vision Solves These Issues\n1:40 Injury Prevention with AI Vision\n1:56 Player and Ball Tracking with AI\n2:10 Augmented Reality for Fans\n2:28 Labellerr’s Role in Sports AI\n3:02 Conclusion and Call to Action\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=99Cr4KuvcvA",
    "embed_url": "https://www.youtube.com/embed/99Cr4KuvcvA",
    "duration": 207,
    "view_count": 131,
    "upload_date": "20250430",
    "uploader": "Labellerr",
    "tags": [
      "AI retail solutions",
      "Retail CCTV analytics",
      "AI retail technology",
      "image segmentation",
      "annotation software",
      "image datasets",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "predictive analytics",
      "Sports Analytics",
      "Keypoint Detection",
      "Ball Tracking",
      "Object Detection",
      "object detection python",
      "computer vision",
      "machine learning projects",
      "deep learning",
      "opencv python",
      "object tracking",
      "NLP",
      "tensorflow",
      "ML"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Did you know that in sports, a single missed call or unnoticed injury can change the outcome of a game or even an athletes career? Traditional methods of analysis can't keep up with the speed and complexity of modern sports. But computer vision is changing the game. In today's video, let's explore how computer vision is revolutionizing sports and how labellerr plays its role. The challenges in sports today. The referee misses a crucial foul. The wrong call changes the outcome and the controversy erupts. This is what a missed call leads to. A player limps slightly, but no one notices. Fatigue or improper movement goes undetected, leading to injuries that could have been prevented. Coaches and analysts struggle to track every move. Manual stats can't capture the full story. Fans at home want more than just the score. They want to feel the excitement, see every angle, and interact with real-time stats, and instant highlights. How computer vision solves these issues. Computer vision acts as a superhuman coach, referee, and analyst for sports. Here's how. Computer vision tracks ball trajectory, player positions, and contact points in real time. Cameras analyze body movements to highlight risky postures like awkward landings in basketball, which reduces injuries by up to 30%. Some advanced computer vision models help track athletes diets by analyzing their food intake. They identify food types, portion sizes, and provide real-time nutritional insights. AI powered cameras track player movements, ball paths, and even facial expressions. AI models studies this data to reveal techniques, positioning, and decisions among the players. Augmented reality AR brings fans closer to the game. We, as fans, have desire to have a stadium-like feel while sitting at home. This technology makes that possible. But here's the catch. All these systems need millions of labeled images to train the AI accurately. That's where LeBeller steps in. Computer vision in sports depends on highquality labeled data. labellerr makes this easy by features like SAM help label players, balls, and key actions without overlap, even in crowded scenes. This helps models track attributes more accurately by adding extra details to each object. labellerr also provides event tagging which is one of the widely used features by annotators. It makes it easier to highlight important sections of the game. This and many other features have been explained on our channel already. Do check that out. We can easily conclude that computer vision is required both on and off the field in sports and is not limited to any particular game. The possibilities are infinite. It depends on us how we use it and Labaler is your first step in this direction. Book a demo with Labaler to explore more about computer vision and data annotation. Link in the description.",
    "transcript_chunks": [
      "Did you know that in sports, a single missed call or unnoticed injury can change the outcome of a game or even an athletes career? Traditional methods of analysis can't keep up with the speed and complexity of modern sports. But computer vision is changing the game. In today's video, let's explore how computer vision is revolutionizing sports and how labellerr plays its role. The challenges in sports today. The referee misses a crucial foul. The wrong call changes the outcome and the controversy erupts. This is what a missed call leads to. A player limps slightly, but no one notices. Fatigue or improper movement goes undetected, leading to injuries that could have been prevented. Coaches and analysts struggle to track every move. Manual stats can't capture the full story. Fans at home want more than just the score. They want to feel the excitement, see every angle, and interact with real-time stats, and instant highlights. How computer vision solves these issues. Computer vision acts as a superhuman coach, referee, and analyst for sports. Here's how. Computer vision tracks ball trajectory, player positions, and contact points in real time. Cameras analyze body movements to highlight risky postures like awkward landings in basketball, which reduces injuries by up to 30%. Some advanced computer vision models help track athletes diets by analyzing their food intake. They identify food types, portion sizes, and provide real-time nutritional insights. AI powered cameras track player movements, ball paths, and even facial expressions. AI models studies this data to reveal techniques, positioning, and decisions among the players. Augmented reality AR brings fans closer to the game. We, as fans, have desire to have a stadium-like feel while sitting at home. This technology makes that possible. But here's the catch. All these systems need millions of labeled images",
      "to train the AI accurately. That's where LeBeller steps in. Computer vision in sports depends on highquality labeled data. labellerr makes this easy by features like SAM help label players, balls, and key actions without overlap, even in crowded scenes. This helps models track attributes more accurately by adding extra details to each object. labellerr also provides event tagging which is one of the widely used features by annotators. It makes it easier to highlight important sections of the game. This and many other features have been explained on our channel already. Do check that out. We can easily conclude that computer vision is required both on and off the field in sports and is not limited to any particular game. The possibilities are infinite. It depends on us how we use it and Labaler is your first step in this direction. Book a demo with Labaler to explore more about computer vision and data annotation. Link in the description."
    ],
    "transcript_word_count": 460,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "Pj6zxwWpviM",
    "title": "Boost Retail Success Using Customer Behaviour Analytics",
    "description": "In this video, we explore how customer behavior detection can transform retail and service industries. Learn how Labellerr’s advanced tools—like event tagging, SAM 2 tracking, and the attribute feature—help businesses turn raw video footage into actionable insights. See how easy it is to annotate data, track customer behavior, and enhance your operations\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=Pj6zxwWpviM",
    "embed_url": "https://www.youtube.com/embed/Pj6zxwWpviM",
    "duration": 210,
    "view_count": 6,
    "upload_date": "20250429",
    "uploader": "Labellerr",
    "tags": [
      "Customer behavior tracking",
      "Retail store optimization",
      "Video event tagging",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "segment anything",
      "segment anything v2",
      "video tracking",
      "large language model",
      "artificial intelligence",
      "computer vision",
      "Retail customer behavior"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "We all know that cameras capture every step we take, every product we pick up, and every conversation we have. But how and where can this be used? Welcome to the world of customer behavior detection. Whether it's in bustling retail stores, highsecurity banking environments, welcoming hotels, or the digital platforms we use everyday, this technology is quietly shaping smarter, more personalized experiences all around us. With the right insights, businesses can predict what customers want next, prevent problems before they arise, style the store according to average customer behavior. Everywhere we go, cameras and microphones capture tons of footage and audio, but no human can watch it all or spot every important detail. That's why computer vision is essential to automatically detect key behaviors and suspicious activities. To train these AI models, highquality data annotation is the foundation. Let's see how our tool labor can be used for the annotation process. First, I'll show you event tagging in this uploaded CCTV footage from a retail shop. Okay, so I just noticed four events till the time I played the video. Let's create them quickly. Event tagging helps businesses understand what's happening in their stores, identify peak service times, and improve staff allocation. Now, I'll tag the events by placing the arrow head at the right spot and then simply clicking on the object. This could be stretched or compressed, too. I'll quickly label other events, too. Oh, here I just made a mistake by creating this object as polygon type. No problem. In labellerr, you can update it anytime. I just deleted the previous one and made a new object. It's this simple to execute in labellerr. Next, I'll use the SAM 2 feature. Instead of manually annotating every single frame, this tool automatically tracks objects. All you need to do is annotate a single frame and then rightclick this right dot and apply SAM 2 tracking. You'll have to wait for a few minutes now. Okay. So now you can see in the timeline that our object is annotated in all frames. This saves hours of manual work and ensures accurate consistent data for analysis. Similarly, other objects like the shopkeeper here can be annotated too. For each tagged event, we can note additional information like the customer's estimated age group, whether they're carrying a bag, or if they seem satisfied or confused. This rich data helps businesses dig deeper into customer journeys and tailor their strategies. Also, the attribute feature can be used on objects, too. Customer behavior detection is shaping the future of retail and beyond. With advanced tools like labellerr, you can turn raw video footage into actionable insights, making every customer interaction count. If you want to know how Lebler can help you with your annotation process, book a demo with us. The link is provided in the description.",
    "transcript_chunks": [
      "We all know that cameras capture every step we take, every product we pick up, and every conversation we have. But how and where can this be used? Welcome to the world of customer behavior detection. Whether it's in bustling retail stores, highsecurity banking environments, welcoming hotels, or the digital platforms we use everyday, this technology is quietly shaping smarter, more personalized experiences all around us. With the right insights, businesses can predict what customers want next, prevent problems before they arise, style the store according to average customer behavior. Everywhere we go, cameras and microphones capture tons of footage and audio, but no human can watch it all or spot every important detail. That's why computer vision is essential to automatically detect key behaviors and suspicious activities. To train these AI models, highquality data annotation is the foundation. Let's see how our tool labor can be used for the annotation process. First, I'll show you event tagging in this uploaded CCTV footage from a retail shop. Okay, so I just noticed four events till the time I played the video. Let's create them quickly. Event tagging helps businesses understand what's happening in their stores, identify peak service times, and improve staff allocation. Now, I'll tag the events by placing the arrow head at the right spot and then simply clicking on the object. This could be stretched or compressed, too. I'll quickly label other events, too. Oh, here I just made a mistake by creating this object as polygon type. No problem. In labellerr, you can update it anytime. I just deleted the previous one and made a new object. It's this simple to execute in labellerr. Next, I'll use the SAM 2 feature. Instead of manually annotating every single frame, this tool automatically tracks objects. All you need",
      "to do is annotate a single frame and then rightclick this right dot and apply SAM 2 tracking. You'll have to wait for a few minutes now. Okay. So now you can see in the timeline that our object is annotated in all frames. This saves hours of manual work and ensures accurate consistent data for analysis. Similarly, other objects like the shopkeeper here can be annotated too. For each tagged event, we can note additional information like the customer's estimated age group, whether they're carrying a bag, or if they seem satisfied or confused. This rich data helps businesses dig deeper into customer journeys and tailor their strategies. Also, the attribute feature can be used on objects, too. Customer behavior detection is shaping the future of retail and beyond. With advanced tools like labellerr, you can turn raw video footage into actionable insights, making every customer interaction count. If you want to know how Lebler can help you with your annotation process, book a demo with us. The link is provided in the description."
    ],
    "transcript_word_count": 475,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "8rFjDaU9sOE",
    "title": "How Computer Vision is Transforming Healthcare",
    "description": "In healthcare, every second counts—and a single oversight can change a patient’s fate. In this video, we explore how computer vision is revolutionizing modern medicine, from faster diagnoses to safer surgeries and 24/7 patient monitoring. Learn how these AI advancements are powered by accurate data annotation—and how Labellerr plays a pivotal role in making it possible.\n\nDiscover:\n- Real-world challenges in healthcare\n - How AI-powered computer vision solves them\n - Why labeled data is the backbone of medical AI\n - Labellerr’s contribution to advancing healthcare solutions\n\nChapters\n0:00 Introduction to Healthcare Challenges\n0:22 Major Hurdles in Healthcare\n0:58 How Computer Vision Helps\n1:15 AI in Medical Imaging\n1:18 Real-Time Surgical Guidance\n1:33 Patient Monitoring with Smart Cameras\n1:44 Automating Administrative Tasks\n1:54 Importance of Labeled Data\n2:00 Labellerr’s Role in Data Annotation\n2:14 Labellerr’s SAM Feature\n2:32 Conclusion: Redefining Healthcare\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=8rFjDaU9sOE",
    "embed_url": "https://www.youtube.com/embed/8rFjDaU9sOE",
    "duration": 178,
    "view_count": 70,
    "upload_date": "20250425",
    "uploader": "Labellerr",
    "tags": [
      "Computer vision for doctors",
      "AI healthcare automation",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "Computer vision healthcare",
      "AI in medicine",
      "Medical image annotation",
      "AI healthcare solutions",
      "Tumor detection AI",
      "Surgical video analysis",
      "Patient monitoring AI",
      "Medical AI technology",
      "AI medical diagnostics",
      "Healthcare data annotation",
      "AI X-ray analysis",
      "MRI scan AI",
      "Real-time surgical guidance",
      "AI medical imaging"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Did you know that in healthcare, a single misdiagnosis or delayed intervention can mean the difference between life and death? Such headlines remind us how critical accuracy and speed are in medicine. Today, we'll explore how computer vision is transforming healthcare and how leveller is playing its role. Despite cuttingedge tools, healthcare faces major hurdles. For example, tiny tumors or sometimes wounds go unnoticed in scans. And if they do get noticed, human misjudgments sometimes cause complications, and there's no need to explain the risk it carries along. Doctors and nurses face huge workloads. Reviewing thousands of images and records leads to fatigue, slower decisions, and sometimes errors. Also, nurses can't track everyone 24/7, leading to emergencies. These issues cost time, money, and most importantly, human lives. Computer vision acts like a superhuman assistant for doctors, nurses, and researchers. Here's how it's shaping the industry. AI scans X-rays, MRIs, and CTS to spot tumors, fractures, or infections faster and more accurately than the human eye. During operations, computer vision provides real-time imaging and guidance, helping surgeons navigate complex procedures, avoid mistakes, and improve recovery times. Even robotic assisted surgeries use CV to enhance precision. Smart cameras and sensors watch for subtle changes in patient movement, breathing, or vital signs, alerting staff instantly if something's wrong, whether in the ICU or through tele medicine at home. CV models automate admin tasks like tracking medicine records digitally, which eases the load off staff members. But here's the catch. All these breakthroughs rely on millions of accurately labeled medical images and videos to train the AI. That's where Labella steps in. Annotators can mark critical events in surgical videos or patient monitoring footage, making it easier to train AI to recognize emergencies or complications. labellerr's SAM feature helps label complex medical images with just a few clicks or sometimes with just a single click. From bounding boxes to polygons and key points, labellerr supports all the formats. To know more about other features, check out our channel. Computer vision is redefining healthcare, helping doctors diagnose earlier, operate safer, and monitor patients more closely wherever they are. But none of this is possible without precise data annotation. labellerr is your first step towards smarter, safer, and more accessible healthcare. Book a demo with Labaler to know more about computer vision and annotations. Link in the description.",
    "transcript_chunks": [
      "Did you know that in healthcare, a single misdiagnosis or delayed intervention can mean the difference between life and death? Such headlines remind us how critical accuracy and speed are in medicine. Today, we'll explore how computer vision is transforming healthcare and how leveller is playing its role. Despite cuttingedge tools, healthcare faces major hurdles. For example, tiny tumors or sometimes wounds go unnoticed in scans. And if they do get noticed, human misjudgments sometimes cause complications, and there's no need to explain the risk it carries along. Doctors and nurses face huge workloads. Reviewing thousands of images and records leads to fatigue, slower decisions, and sometimes errors. Also, nurses can't track everyone 24/7, leading to emergencies. These issues cost time, money, and most importantly, human lives. Computer vision acts like a superhuman assistant for doctors, nurses, and researchers. Here's how it's shaping the industry. AI scans X-rays, MRIs, and CTS to spot tumors, fractures, or infections faster and more accurately than the human eye. During operations, computer vision provides real-time imaging and guidance, helping surgeons navigate complex procedures, avoid mistakes, and improve recovery times. Even robotic assisted surgeries use CV to enhance precision. Smart cameras and sensors watch for subtle changes in patient movement, breathing, or vital signs, alerting staff instantly if something's wrong, whether in the ICU or through tele medicine at home. CV models automate admin tasks like tracking medicine records digitally, which eases the load off staff members. But here's the catch. All these breakthroughs rely on millions of accurately labeled medical images and videos to train the AI. That's where Labella steps in. Annotators can mark critical events in surgical videos or patient monitoring footage, making it easier to train AI to recognize emergencies or complications. labellerr's SAM feature helps label complex medical images with",
      "just a few clicks or sometimes with just a single click. From bounding boxes to polygons and key points, labellerr supports all the formats. To know more about other features, check out our channel. Computer vision is redefining healthcare, helping doctors diagnose earlier, operate safer, and monitor patients more closely wherever they are. But none of this is possible without precise data annotation. labellerr is your first step towards smarter, safer, and more accessible healthcare. Book a demo with Labaler to know more about computer vision and annotations. Link in the description."
    ],
    "transcript_word_count": 392,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "pBLWOe01QXU",
    "title": "How to Fine-Tune Yolo on Custom Dataset",
    "description": "Learn how to fine-tune a YOLO object detection model on your own custom dataset for real-world accuracy and performance.\nFrom setting up a Python environment to annotating images using the Labellerr platform, organizing your dataset in YOLO format, training the model, validating performance, and running live inference via webcam—this step-by-step guide covers it all.\nWhether you're building a hand detection system or customizing YOLO for your unique use case, this tutorial gives you the tools to make YOLO work for you.\n\nChapters\n0:00 Introduction to Fine-Tuning YOLO\n0:22 Step 1: Creating a Virtual Environment\n0:49 Step 2: Downloading Requirements\n1:22 Step 3: Labeling Dataset\n2:35 Step 4: Customizing Dataset to YOLO Format\n3:24 Step 5: Training YOLO Model\n4:42 Step 6: Validating the Model\n6:12 Step 7: Running Inference via Webcam\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=pBLWOe01QXU",
    "embed_url": "https://www.youtube.com/embed/pBLWOe01QXU",
    "duration": 389,
    "view_count": 1405,
    "upload_date": "20250425",
    "uploader": "Labellerr",
    "tags": [
      "TensorFlow",
      "IMAGE Classification",
      "Image Segmentation",
      "Ultralytics YOLOv8",
      "Pose Estimation",
      "ml",
      "Vision AI",
      "YOLOv8 Accuracy",
      "Custom Dataset",
      "RTDETR",
      "object detection classifier",
      "object detection deep learning",
      "roboflow",
      "object detection",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "Machine Learning",
      "Artificial Intelligence",
      "Data Science",
      "Deep Learning",
      "YOLO",
      "Object Detection",
      "PyTorch",
      "Finetuning",
      "ObjectDetection"
    ],
    "categories": [
      "Education"
    ],
    "transcript": "What if your AI model could detect exactly what you need in your unique environment? Generic YOLO models are great, but not tailored to your use case. In this video, learn how to fine-tune YOLO on your custom data set for precision, focus, and real world performance. By the end, you'll not only understand how to train YOLO, but how to make it work for you. Step one, creating a virtual environment. We start by creating a virtual environment using this command. Now I'll activate the environment for our YOLO project. We can also check which module is currently available using pip list. As you can see there is only the pip module available in our environment. Step two downloading requirements. Now installing the ultritics module. This will also install dependencies module to train YOLO model. While it's installing, go to the official PyTorch website and find the torch version suitable for your system. Copy the command from the website and paste it in the terminal once Ultralytics installation is complete. Now we can verify that all required modules have been installed using pip list. Now let's jump to step three, labeling data set. To train our YOLO model, we first need annotated data. We'll use the laborer annotation platform. Start by creating a new workspace. We will name our workspace as hand detection and click create domain. Our data set type is image. So we choose image and name our data set hand. Give it a description and upload all your images. I'll be doing it quickly. We have also covered almost all of our features on our channel. Do check them out in case of any trouble. So yes, all the selected images are uploaded. Now clicking on create data set to add images to our workspace. Create an object named hand and choose bounding box as the annotation format. We name our project as hand detection and click create project. Let's start labeling. We should choose our object class which is hand here and start drawing the bounding box on our image with precision. We will continue this process until all our images have been annotated. We'll click on the submit button and annotate all the hands in our data set. Step four, customizing data set to YOLO format. Now after our data set is annotated we should divide the data sets into three folders namely train val test. These folders should be used in training validating and evaluating our model. Each of these should contain two subfolders images for image files labels for yolo format annotation files. Now in data.yaml specify the paths to train val and test images. also define the number of classes and their names. This YAML file is necessary for our model training as it will provide the location to all the data set on which the model should be trained. Our data set is ready for fine-tuning. We can go to the next step. Step five, training YOLO model. YOLO model training script is simple, but choosing well-configured parameters is important. Each training argument plays a crucial role. So choose wisely based on your system. Each argument has a purpose in model training. Like this will represent image size. Batch represents batch size meaning the number of images taken at once by the model. Optimizer will add them. Lo will be initial learning rate. Device represents which GPU to use. Name represents model name. Save will save our model when completed. Save period will save our model weights in a particular chosen interval and the list goes on. Let's start the training script. You'll see yolo initializing and the model will begin training. I have chosen epoch equals 100. That obviously means the model will iterate through the training images 100 times and training time depends on your GPU power. Wait till the model is getting fine-tuned to your data set. Step six, validating the model. Once training is complete, a new folder is created from where you can copy its relative path and paste this into your val script. Here the confidence argument plays an important role as it signifies the model's confidence threshold for a prediction to be considered a detection. Here it's set to 0.9 which means 90%. Run the script and wait. After a few minutes a val folder will be created inside the runs directory. This folder contains metric graphs, validation image outputs and predicted image results. You can roam this folder and see how your model performed on the val data set images. Now validation process is completed. You can see all the metrics values as output. You can copy the result and paste it into another script for future evaluation. As we can see, our model trained well as its scores are great. Precision equals 0.999. Recall equals.999 and other metrics are also great. We will further evaluate these results in a future video. Now we go to the confusion matrix of our model. As we can see, it shows great results. The confusion matrix is great for analyzing model performance, especially for models with different classes. You can use it to further study your model. Step seven. At last, use this command line to inference your fine-tune model through webcam. For more insights and a deeper understanding, check out our detailed blog on this topic. Link is provided in the description.",
    "transcript_chunks": [
      "What if your AI model could detect exactly what you need in your unique environment? Generic YOLO models are great, but not tailored to your use case. In this video, learn how to fine-tune YOLO on your custom data set for precision, focus, and real world performance. By the end, you'll not only understand how to train YOLO, but how to make it work for you. Step one, creating a virtual environment. We start by creating a virtual environment using this command. Now I'll activate the environment for our YOLO project. We can also check which module is currently available using pip list. As you can see there is only the pip module available in our environment. Step two downloading requirements. Now installing the ultritics module. This will also install dependencies module to train YOLO model. While it's installing, go to the official PyTorch website and find the torch version suitable for your system. Copy the command from the website and paste it in the terminal once Ultralytics installation is complete. Now we can verify that all required modules have been installed using pip list. Now let's jump to step three, labeling data set. To train our YOLO model, we first need annotated data. We'll use the laborer annotation platform. Start by creating a new workspace. We will name our workspace as hand detection and click create domain. Our data set type is image. So we choose image and name our data set hand. Give it a description and upload all your images. I'll be doing it quickly. We have also covered almost all of our features on our channel. Do check them out in case of any trouble. So yes, all the selected images are uploaded. Now clicking on create data set to add images to our workspace. Create",
      "an object named hand and choose bounding box as the annotation format. We name our project as hand detection and click create project. Let's start labeling. We should choose our object class which is hand here and start drawing the bounding box on our image with precision. We will continue this process until all our images have been annotated. We'll click on the submit button and annotate all the hands in our data set. Step four, customizing data set to YOLO format. Now after our data set is annotated we should divide the data sets into three folders namely train val test. These folders should be used in training validating and evaluating our model. Each of these should contain two subfolders images for image files labels for yolo format annotation files. Now in data.yaml specify the paths to train val and test images. also define the number of classes and their names. This YAML file is necessary for our model training as it will provide the location to all the data set on which the model should be trained. Our data set is ready for fine-tuning. We can go to the next step. Step five, training YOLO model. YOLO model training script is simple, but choosing well-configured parameters is important. Each training argument plays a crucial role. So choose wisely based on your system. Each argument has a purpose in model training. Like this will represent image size. Batch represents batch size meaning the number of images taken at once by the model. Optimizer will add them. Lo will be initial learning rate. Device represents which GPU to use. Name represents model name. Save will save our model when completed. Save period will save our model weights in a particular chosen interval and the list goes on. Let's start the",
      "training script. You'll see yolo initializing and the model will begin training. I have chosen epoch equals 100. That obviously means the model will iterate through the training images 100 times and training time depends on your GPU power. Wait till the model is getting fine-tuned to your data set. Step six, validating the model. Once training is complete, a new folder is created from where you can copy its relative path and paste this into your val script. Here the confidence argument plays an important role as it signifies the model's confidence threshold for a prediction to be considered a detection. Here it's set to 0.9 which means 90%. Run the script and wait. After a few minutes a val folder will be created inside the runs directory. This folder contains metric graphs, validation image outputs and predicted image results. You can roam this folder and see how your model performed on the val data set images. Now validation process is completed. You can see all the metrics values as output. You can copy the result and paste it into another script for future evaluation. As we can see, our model trained well as its scores are great. Precision equals 0.999. Recall equals.999 and other metrics are also great. We will further evaluate these results in a future video. Now we go to the confusion matrix of our model. As we can see, it shows great results. The confusion matrix is great for analyzing model performance, especially for models with different classes. You can use it to further study your model. Step seven. At last, use this command line to inference your fine-tune model through webcam. For more insights and a deeper understanding, check out our detailed blog on this topic. Link is provided in the description."
    ],
    "transcript_word_count": 898,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "HjdzbQxQYMQ",
    "title": "How Computer Vision is Making Its Way In Manufacturing?",
    "description": "Factories lose billions every year due to defects, unplanned downtime, and inefficiencies. Traditional systems can’t keep pace with modern production demands, but computer vision is transforming the landscape.\n\nIn this video, we break down the biggest challenges manufacturers face today and how computer vision addresses them with real-time defect detection, predictive maintenance, safety monitoring, inventory tracking, and quality control.\n\nWe also explored the critical role of data annotation in powering these AI systems and how Labellerr makes this process faster and smarter.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=HjdzbQxQYMQ",
    "embed_url": "https://www.youtube.com/embed/HjdzbQxQYMQ",
    "duration": 168,
    "view_count": 128,
    "upload_date": "20250422",
    "uploader": "Labellerr",
    "tags": [
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "machine learning",
      "artificial intelligence",
      "fault diagnosis",
      "AI Computer Vision",
      "AI Defect Detection",
      "Smart Cameras",
      "V-Soft Consulting",
      "computing automation",
      "computer vision services",
      "deep learning camera",
      "image segmentation ai",
      "image object recognition software",
      "visual inspection",
      "ai inspection",
      "ai quality control",
      "air quality inspection",
      "ai visual inspection",
      "computer vision"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Did you know that the global manufacturing industry loses billions every year due to inefficiencies, defects, and downtime? Traditional methods are not able to keep up with the demand. But computer vision is changing the game. In today's video, we'll explore how computer vision is revolutionizing manufacturing and how labellerr plays a role. Let's dive in. Despite the presence of automation, factories still struggle with defects. Tiny cracks or misalignments go unnoticed until products fail quality checks. Unplanned downtime. Machines break down without warning. Worker safety risks. Human error or fatigue leads to accidents in high-risisk zones. Inventory chaos. Mismanaged parts delay production and increase costs. Inconsistent quality. Manual inspections miss flaws, hurting brand trust. These issues waste time, money, and resources. How computer vision solves these issues. Computer vision gives factories a digital eye to spot, predict, and act. Here's how. Cameras scan products on assembly lines. AI spots cracks, dents, or color mismatches in milliseconds. CV monitors machinery vibrations, heat, or wear. It alerts teams before parts fail, cutting down time by 40%. Smart cameras detect unsafe actions, eg no helmet in hazardous zones, and trigger instant alerts. AI scans shelves and bins, tracking parts in real time. No more delayed orders. Vision systems inspect every product, ensuring 100% consistency. No tired eyes, no missed flaws. But here's the catch. These systems need millions of labeled images, defects, machinery parts, safety gear to work accurately. That's where labellerr steps in. Computer vision needs lots of labeled images to work well. labellerr simplifies this by easy classification. This feature helps classify the data as a whole. For example, using boolean answers to determine whether workers are wearing helmets or not. SAM feature allows you to annotate objects with just a few clicks, speeding up tasks like annotating boxes during assembly line inspections. Flexible annotation types. labellerr supports bounding boxes, polygons, and key points as required by your project. Check out more features that Leveler offers. Demos are available on our channel. From spotting a single defective product to managing entire assembly lines, computer vision is redefining manufacturing. But none of this works without precise data annotation. Book a demo today to explore how labellerr can help your manufacturing operations thrive. Link in the description.",
    "transcript_chunks": [
      "Did you know that the global manufacturing industry loses billions every year due to inefficiencies, defects, and downtime? Traditional methods are not able to keep up with the demand. But computer vision is changing the game. In today's video, we'll explore how computer vision is revolutionizing manufacturing and how labellerr plays a role. Let's dive in. Despite the presence of automation, factories still struggle with defects. Tiny cracks or misalignments go unnoticed until products fail quality checks. Unplanned downtime. Machines break down without warning. Worker safety risks. Human error or fatigue leads to accidents in high-risisk zones. Inventory chaos. Mismanaged parts delay production and increase costs. Inconsistent quality. Manual inspections miss flaws, hurting brand trust. These issues waste time, money, and resources. How computer vision solves these issues. Computer vision gives factories a digital eye to spot, predict, and act. Here's how. Cameras scan products on assembly lines. AI spots cracks, dents, or color mismatches in milliseconds. CV monitors machinery vibrations, heat, or wear. It alerts teams before parts fail, cutting down time by 40%. Smart cameras detect unsafe actions, eg no helmet in hazardous zones, and trigger instant alerts. AI scans shelves and bins, tracking parts in real time. No more delayed orders. Vision systems inspect every product, ensuring 100% consistency. No tired eyes, no missed flaws. But here's the catch. These systems need millions of labeled images, defects, machinery parts, safety gear to work accurately. That's where labellerr steps in. Computer vision needs lots of labeled images to work well. labellerr simplifies this by easy classification. This feature helps classify the data as a whole. For example, using boolean answers to determine whether workers are wearing helmets or not. SAM feature allows you to annotate objects with just a few clicks, speeding up tasks like annotating boxes during",
      "assembly line inspections. Flexible annotation types. labellerr supports bounding boxes, polygons, and key points as required by your project. Check out more features that Leveler offers. Demos are available on our channel. From spotting a single defective product to managing entire assembly lines, computer vision is redefining manufacturing. But none of this works without precise data annotation. Book a demo today to explore how labellerr can help your manufacturing operations thrive. Link in the description."
    ],
    "transcript_word_count": 375,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "s-kiaeeVbxA",
    "title": "How Computer Vision is Transforming Agriculture",
    "description": "Agriculture is changing—and fast. As we head toward a future where we must feed nearly 10 billion people, the farming industry faces critical challenges: crop diseases, labor shortages, water mismanagement, and more. In this video, we break down how computer vision is tackling these issues head-on and how Labellerr helps make this technology possible through high-quality data annotation.\n\nWatch the full video to understand how drones, AI models, and smart vision systems are reshaping the way we grow food—faster, smarter, and more sustainably.\n\nChapters\n0:00 Introduction to Global Food Challenges\n0:22 Challenges in Traditional Farming\n0:53 How Computer Vision Solves Agricultural Issues\n1:01 Drones and AI for Crop Monitoring\n1:16 Targeted Herbicide Use with Vision Models\n1:21 Animal Behavior Tracking\n1:31 Fruit Ripeness and Yield Estimation\n1:40 Optimized Water Usage with Visual Data\n1:47 Smarter, Sustainable Agriculture\n1:54 The Role of Data Annotation\n2:01 Labellerr’s Annotation Features\n2:27 Redefining Agriculture with Computer Vision\n2:36 Labellerr’s Impact on AI Vision Systems\n\nCurious about how labeling tools actually work in agriculture? \nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=s-kiaeeVbxA",
    "embed_url": "https://www.youtube.com/embed/s-kiaeeVbxA",
    "duration": 167,
    "view_count": 206,
    "upload_date": "20250421",
    "uploader": "Labellerr",
    "tags": [
      "Agriculture techniques",
      "Farming techniques",
      "modern agriculture",
      "rural transformation",
      "agricultural revolution",
      "artificial intelligence for farming",
      "high tech farming",
      "farming technology",
      "farming robots",
      "urban farming",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "ai in agriculture",
      "artificial intelligence",
      "agriculture",
      "smart farming",
      "modern farming",
      "modern farming technology",
      "smart agriculture",
      "future of farming"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Did you know that by 2050 we'll need to feed nearly 10 billion people, but 40% of crops today are lost to pests, diseases, and climate extremes? Traditional farming can't keep up. Computer vision is the only way out. In today's video, we'll explore how computer vision is transforming agriculture and how Lebler plays a role in this transformation. Let's dive in. Despite advances in machinery and automation, agriculture still grapples with critical challenges like crop diseases go undetected until it's too late. Weed infestations reduce yields and increased costs. Over irrigation or under irrigation due to lack of real-time soil insights. Labor shortages make it tough to monitor large fields manually. All these issues directly affect food supply and farmer income. How computer vision solves these issues. Computer vision enables machines to see and analyze visual data. Here's how it's helping. Drones scan crops from above. AI analyzes leaf patterns to spot infections like blight or mildew before they spread. Computer vision models distinguish between crops and unwanted plants, enabling targeted herbicide use, saving money and protecting soil health. Cameras track animal behavior to detect signs of illness, stress, or pregnancy in real time, ensuring better herd management. Vision systems assess fruit color, size, and ripeness, helping harvest at the perfect time and estimate yield with precision. Visual data paired with sensors ensures water is used only where it's needed, reducing waste and improving crop health. These innovations are making agriculture smarter, more sustainable, and more scalable. But here's the catch. To train these AI systems, we need millions of images of crops, pests, animals, soil types, all annotated accurately. That's where LeBeller steps in. Leaveller offers almost every feature you need to streamline the process. Maybe you'll use our attribute feature to classify strawberries based on their ripeness level or our SAM feature to annotate the infected area with just a few clicks. With labellerr, you're good to go. To explore more features, sub demos are already provided on our channel. Make sure to check them out. From detecting a single diseased leaf to managing mega farms, computer vision is redefining agriculture. But none of this works without precise data annotation. labellerr ensures AI doesn't just see farms, it understands them. Book a demo with labellerr to explore more about annotations. Link in the description.",
    "transcript_chunks": [
      "Did you know that by 2050 we'll need to feed nearly 10 billion people, but 40% of crops today are lost to pests, diseases, and climate extremes? Traditional farming can't keep up. Computer vision is the only way out. In today's video, we'll explore how computer vision is transforming agriculture and how Lebler plays a role in this transformation. Let's dive in. Despite advances in machinery and automation, agriculture still grapples with critical challenges like crop diseases go undetected until it's too late. Weed infestations reduce yields and increased costs. Over irrigation or under irrigation due to lack of real-time soil insights. Labor shortages make it tough to monitor large fields manually. All these issues directly affect food supply and farmer income. How computer vision solves these issues. Computer vision enables machines to see and analyze visual data. Here's how it's helping. Drones scan crops from above. AI analyzes leaf patterns to spot infections like blight or mildew before they spread. Computer vision models distinguish between crops and unwanted plants, enabling targeted herbicide use, saving money and protecting soil health. Cameras track animal behavior to detect signs of illness, stress, or pregnancy in real time, ensuring better herd management. Vision systems assess fruit color, size, and ripeness, helping harvest at the perfect time and estimate yield with precision. Visual data paired with sensors ensures water is used only where it's needed, reducing waste and improving crop health. These innovations are making agriculture smarter, more sustainable, and more scalable. But here's the catch. To train these AI systems, we need millions of images of crops, pests, animals, soil types, all annotated accurately. That's where LeBeller steps in. Leaveller offers almost every feature you need to streamline the process. Maybe you'll use our attribute feature to classify strawberries based on their ripeness",
      "level or our SAM feature to annotate the infected area with just a few clicks. With labellerr, you're good to go. To explore more features, sub demos are already provided on our channel. Make sure to check them out. From detecting a single diseased leaf to managing mega farms, computer vision is redefining agriculture. But none of this works without precise data annotation. labellerr ensures AI doesn't just see farms, it understands them. Book a demo with labellerr to explore more about annotations. Link in the description."
    ],
    "transcript_word_count": 387,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "4E7H5XTdxh8",
    "title": "The Secret Eyes of Self-Driving Cars | Role of Labellerr in Computer Vision",
    "description": "Autonomous vehicles are no longer just a concept from the future. Equipped with advanced sensors and cameras, these vehicles rely heavily on computer vision to perceive and navigate their environment.\n\nIn this video, we break down how computer vision works in self-driving cars, the role of LiDAR, Radar, and Cameras, and how object detection, semantic segmentation, and tracking algorithms come together to make autonomous driving possible.\n\nDiscover how Labellerr supports this ecosystem by offering a powerful annotation platform tailored for AI model training. From bounding boxes to pixel-level segmentation, learn how Labellerr enables faster and more accurate model development for companies working on autonomous vehicle technologies.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=4E7H5XTdxh8",
    "embed_url": "https://www.youtube.com/embed/4E7H5XTdxh8",
    "duration": 149,
    "view_count": 73,
    "upload_date": "20250415",
    "uploader": "Labellerr",
    "tags": [
      "self-driving cars",
      "tesla autopilot",
      "google self-driving car",
      "apple car",
      "automotive technology",
      "driverless car",
      "LIDAR",
      "semantic segmentation",
      "object detection",
      "computer vision",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "data labeling software",
      "image processing",
      "self driving",
      "full self driving",
      "future technology",
      "autonomous cars"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Imagine cars like this. Pretty cartoonish, right? But what if I told you such cars are already here? The only difference is you can't see their eyes. They're not visible, but they exist in the form of advanced sensors and cameras that powers computer vision. It's a technology that enables machines to interpret visual data from cameras and sensors. As simple as it can get. But how does this work? I mean, how do they perceive objects in the environment? The sensory toolkit to see vehicles use a combination of LAR, radar, and cameras. Lidar bounces light off objects to create 3D maps, even in the dark. Radar uses radio waves, seeing through fog and snow, which makes it better than manual driving. Cameras capture details like lane markings and traffic lights. Multiple cameras are established to see every angle of the road. Now, let's dive deeper and see how computer vision uses these sensors and plays its role. Sensors are great at detecting objects. They can measure distances, speeds, and shapes, but they can't tell what those objects are. That's where computer vision plays its role. Self-driving cars employ object detection and recognition to identify multiple objects on the road, like other vehicles, road signs, and pedestrians. Using semantic segmentation, a car can label each pixel in an image to distinguish between road, lane markings, obstacles, and other elements. After detecting objects, the car uses advanced object tracking algorithms like deep sort and bite track to monitor objects. You can know more about these algorithms in our blog. Link is provided in the description. And the base for all this is annotation. That's where Labaler steps [Music] in. labellerr offers a powerful platform for data annotation, enabling the creation of precise labels for training AI models. Whether it's bounding boxes for object detection, semantic segmentation for pixel level understanding, or poly lines for lane marking detection, LeBeller has it covered. With LeBeller's advanced annotation capabilities and features like SAM and SAM 2, companies building self-driving technology can train their models faster and more efficiently. Check out our channel to know more about the features we offer. And don't forget to book a demo with labellerr to explore more about annotations.",
    "transcript_chunks": [
      "Imagine cars like this. Pretty cartoonish, right? But what if I told you such cars are already here? The only difference is you can't see their eyes. They're not visible, but they exist in the form of advanced sensors and cameras that powers computer vision. It's a technology that enables machines to interpret visual data from cameras and sensors. As simple as it can get. But how does this work? I mean, how do they perceive objects in the environment? The sensory toolkit to see vehicles use a combination of LAR, radar, and cameras. Lidar bounces light off objects to create 3D maps, even in the dark. Radar uses radio waves, seeing through fog and snow, which makes it better than manual driving. Cameras capture details like lane markings and traffic lights. Multiple cameras are established to see every angle of the road. Now, let's dive deeper and see how computer vision uses these sensors and plays its role. Sensors are great at detecting objects. They can measure distances, speeds, and shapes, but they can't tell what those objects are. That's where computer vision plays its role. Self-driving cars employ object detection and recognition to identify multiple objects on the road, like other vehicles, road signs, and pedestrians. Using semantic segmentation, a car can label each pixel in an image to distinguish between road, lane markings, obstacles, and other elements. After detecting objects, the car uses advanced object tracking algorithms like deep sort and bite track to monitor objects. You can know more about these algorithms in our blog. Link is provided in the description. And the base for all this is annotation. That's where Labaler steps [Music] in. labellerr offers a powerful platform for data annotation, enabling the creation of precise labels for training AI models. Whether it's bounding",
      "boxes for object detection, semantic segmentation for pixel level understanding, or poly lines for lane marking detection, LeBeller has it covered. With LeBeller's advanced annotation capabilities and features like SAM and SAM 2, companies building self-driving technology can train their models faster and more efficiently. Check out our channel to know more about the features we offer. And don't forget to book a demo with labellerr to explore more about annotations."
    ],
    "transcript_word_count": 371,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "QIlIg4yHR0Y",
    "title": "How Labellerr’s GenAI Simplifies Annotation for Images, Videos, Audio & Text",
    "description": "Discover how Labellerr’s GenAI transforms the annotation process across images, videos, audio, and text. In this demo video, we walk you through how GenAI assists with:\nAudio transcription and translation\nImage classification using contextual prompts\nVideo summarization and tagging support\nText summarization and key entity identification\n\nWhether you're dealing with complex datasets or tight deadlines, Labellerr’s integrated GenAI acts as your digital assistant, streamlining your annotation workflow without the need for switching tools.\n\nChapters\n0:00 Introduction to Labellerr’s GenAI\n0:20 Audio Annotation with GenAI\n0:57 Image Annotation with GenAI\n1:22 Video Annotation with GenAI\n1:37 Text Annotation with GenAI\n1:56 Labellerr as Your One-Stop Annotation Solution\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=QIlIg4yHR0Y",
    "embed_url": "https://www.youtube.com/embed/QIlIg4yHR0Y",
    "duration": 134,
    "view_count": 37,
    "upload_date": "20250408",
    "uploader": "Labellerr",
    "tags": [
      "preprocessing images",
      "data labeling software",
      "segmentation software",
      "image annotation",
      "video annotation",
      "audio annotation",
      "text annotation",
      "AI assistant",
      "machine learning annotation",
      "annotation software",
      "annotation workflow",
      "annotation tutorial",
      "data annotation",
      "dataset creation",
      "image datasets",
      "data preprocessing",
      "image segmentation",
      "labeling software",
      "image labeling",
      "image processing",
      "machine learning",
      "deep learning"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "For annotating images, videos, audio, or text, annotations can be tough. Complex data sets, tight deadlines, and endless details. That's where LeBeller shines. Our Genai feature is seamlessly integrated into the platform, acting as your digital helper to tackle any annotation task. Let's see some of its many features. Genai and audio annotation. First up, audio annotation. Imagine you've got a podcast in French and you need it transcribed and translated into English for labeling. Here's how it can be done. Writing title for classification. By the way, we have covered all other classification types on our channel. Writing prompt for translation here. Save the changes and click on predict. As we already typed the prompt earlier, it's not required. Now see within seconds we have the output here. With labellerr, you don't need to switch to another tabs. It's all under one tool. Genai and image annotation. Next, images. Suppose you're annotating a dusty old photo from the past. But you have limited knowledge of subject. With Genai in labellerrs classification section, you just ask what's this all about? And you'll see the instant result. Obviously, there is need for humans to verify, but it definitely reduces the load and saves time for annotators. Geni in video annotation. Why watch 15 minutes video when Genai can tell you the story? Genai has its application in video annotations, too. It could ease out event tagging problems, can give transcripts for your video, and provide help in other ways, too. Geni and text annotation. For annotators having text data, Genai has got your back. You don't need to waste time thinking about the summaries or finding nouns in the text. Genai eases the load and now you just need to read and verify. So, how many more use cases came up to your mind while seeing this video? As I said earlier, labellerr is your one-stop solution for all annotation tasks. labellerr's Gen AI is more than just a feature. It's your digital assistant that simplifies complex annotation tasks. Book a demo with labellerr to know more about annotations. Link in the description.",
    "transcript_chunks": [
      "For annotating images, videos, audio, or text, annotations can be tough. Complex data sets, tight deadlines, and endless details. That's where LeBeller shines. Our Genai feature is seamlessly integrated into the platform, acting as your digital helper to tackle any annotation task. Let's see some of its many features. Genai and audio annotation. First up, audio annotation. Imagine you've got a podcast in French and you need it transcribed and translated into English for labeling. Here's how it can be done. Writing title for classification. By the way, we have covered all other classification types on our channel. Writing prompt for translation here. Save the changes and click on predict. As we already typed the prompt earlier, it's not required. Now see within seconds we have the output here. With labellerr, you don't need to switch to another tabs. It's all under one tool. Genai and image annotation. Next, images. Suppose you're annotating a dusty old photo from the past. But you have limited knowledge of subject. With Genai in labellerrs classification section, you just ask what's this all about? And you'll see the instant result. Obviously, there is need for humans to verify, but it definitely reduces the load and saves time for annotators. Geni in video annotation. Why watch 15 minutes video when Genai can tell you the story? Genai has its application in video annotations, too. It could ease out event tagging problems, can give transcripts for your video, and provide help in other ways, too. Geni and text annotation. For annotators having text data, Genai has got your back. You don't need to waste time thinking about the summaries or finding nouns in the text. Genai eases the load and now you just need to read and verify. So, how many more use cases came up",
      "to your mind while seeing this video? As I said earlier, labellerr is your one-stop solution for all annotation tasks. labellerr's Gen AI is more than just a feature. It's your digital assistant that simplifies complex annotation tasks. Book a demo with labellerr to know more about annotations. Link in the description."
    ],
    "transcript_word_count": 352,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "Fx6s0xRgUNE",
    "title": "Choosing the Right LLM: The Best AI for Every Task",
    "description": "Struggling with coding errors, math headaches, or research dilemmas? In this video, we break down why one size doesn't fit all when it comes to large language models. Discover how different LLMs excel in distinct domains—Claude for coding challenges, GPT-4.5 for deep research and general knowledge, Gemini 2.0 for logical reasoning, and DeepSeek-R1 for advanced mathematics. We’ll share real-world examples, benchmark scores, and practical insights to help you choose the best tool for your specific task. Remember, there is no perfect LLM, but selecting the right one is key to success. \nFor additional insights, visit our blog: https://www.labellerr.com/blog/chosing-the-right-llm/\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=Fx6s0xRgUNE",
    "embed_url": "https://www.youtube.com/embed/Fx6s0xRgUNE",
    "duration": 103,
    "view_count": 211,
    "upload_date": "20250407",
    "uploader": "Labellerr",
    "tags": [
      "LLM comparison",
      "best LLM 2025",
      "ChatGPT alternatives",
      "Claude AI",
      "GPT 4.5",
      "Gemini 2.0",
      "Deepseek R1",
      "coding help AI",
      "math problem solver AI",
      "AI for coding",
      "large language models",
      "AI tools 2025",
      "AI for philosophy",
      "debugging with AI",
      "pick the best LLM",
      "LLM review",
      "machine learning models",
      "deep learning",
      "nlp",
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "annotation software",
      "dataset creation",
      "image datasets"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Gh. Are you stuck debugging coding error or having another math headache looking put it on chat GPT? What if I told you there's a better LLM for each task? In this video, you'll get to know the best suited LLM for your task. Hopefully, I have covered your domain, too. Spoiler, it's not chat GPT for everything. Our team was working on a project where we needed to calculate OKS from two JSON file. After testing multiple LLMs, Claude outperformed them all. One thing we observed that it justified the 70.3% score on the S.WE benchmark. Got a random question? Need deep science facts? GPT 4.5 is your brainiac. With an 85.1% MMLU score, it's like a living encyclopedia. Always fresh, always spoton. Ask anything. Gemini 2.0 doesn't guess, it reasons. Need to crack competitive exams or philosophy debates? It's 64.7% better at structured thinking than rivals? Struggling with calculus that keeps you up at night? Tangled in geometry that just won't click? I've been there until Deepseek R1 swooped in. Welcome to the LLM Fight Club. Claude's coding up a storm, GPT 4.5's dropping knowledge bombs, Geminis's outsmarting puzzles, and Deepseek R1's knocking out equations. Who's your champ? Pick your fighter for the win. Because, as they say, there is no perfect LLM. But choosing the right tool for the job is what matters. Just snapshot this cheat sheet and choose your tool wisely. Don't forget to visit our blog on this topic to get more insights. The link is provided in the description.",
    "transcript_chunks": [
      "Gh. Are you stuck debugging coding error or having another math headache looking put it on chat GPT? What if I told you there's a better LLM for each task? In this video, you'll get to know the best suited LLM for your task. Hopefully, I have covered your domain, too. Spoiler, it's not chat GPT for everything. Our team was working on a project where we needed to calculate OKS from two JSON file. After testing multiple LLMs, Claude outperformed them all. One thing we observed that it justified the 70.3% score on the S.WE benchmark. Got a random question? Need deep science facts? GPT 4.5 is your brainiac. With an 85.1% MMLU score, it's like a living encyclopedia. Always fresh, always spoton. Ask anything. Gemini 2.0 doesn't guess, it reasons. Need to crack competitive exams or philosophy debates? It's 64.7% better at structured thinking than rivals? Struggling with calculus that keeps you up at night? Tangled in geometry that just won't click? I've been there until Deepseek R1 swooped in. Welcome to the LLM Fight Club. Claude's coding up a storm, GPT 4.5's dropping knowledge bombs, Geminis's outsmarting puzzles, and Deepseek R1's knocking out equations. Who's your champ? Pick your fighter for the win. Because, as they say, there is no perfect LLM. But choosing the right tool for the job is what matters. Just snapshot this cheat sheet and choose your tool wisely. Don't forget to visit our blog on this topic to get more insights. The link is provided in the description."
    ],
    "transcript_word_count": 256,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "U6XImdGgRa4",
    "title": "Enhance Team Collaboration with Labellerr’s Notification Bar",
    "description": "Need a second opinion while annotating? Labellerr’s notification bar lets you tag team members directly, ensuring quick responses without delays. A red dot alerts the tagged person, while a blue dot shows unread messages. Click the notification to jump straight to the labeling window and resolve queries instantly. Filtering options keep your inbox organized for a smooth workflow.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=U6XImdGgRa4",
    "embed_url": "https://www.youtube.com/embed/U6XImdGgRa4",
    "duration": 68,
    "view_count": 29,
    "upload_date": "20250404",
    "uploader": "Labellerr",
    "tags": [
      "image annotation tool",
      "image datasets",
      "dataset creation",
      "annotation software",
      "annotation tips",
      "annotation demo",
      "annotation features",
      "data labeling",
      "AI annotation",
      "annotation platform",
      "image labeling",
      "annotation best practices",
      "Encord annotation",
      "video annotation",
      "image annotation",
      "medical imaging annotation",
      "machine learning annotation",
      "AI data labeling",
      "healthcare annotation",
      "vehicle annotation",
      "annotation integration",
      "AI annotation software",
      "annotation workflow"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Imagine you're in the middle of annotating a complex image. For example, here you've come across a jacket that needs a second opinion on whether to annotate it under the man's class or not. Instead of wasting time, simply open the remarks from here. Highlight the jacket in this case and tag a team member directly in the tool. You can also highlight that this inbox could be linked to Slack, too. Then over here in the dashboard, you can see a red dot. This would be shown to the tagged person who can be anyone from the annotator's team or reviewers from either side. The blue dot indicates that the specific message has not been read yet. Now clicking this button will take us to the labeling window where the tagged person can respond to the query. That's it. Now clicking on mark as read updates the message status. Here we also have filtering options for our inbox. The notification bar feature in Labaler's tool will streamline your annotation workflow and boost team collaboration. Book a demo with labellerr to learn more about annotations. The link is provided in the description.",
    "transcript_chunks": [
      "Imagine you're in the middle of annotating a complex image. For example, here you've come across a jacket that needs a second opinion on whether to annotate it under the man's class or not. Instead of wasting time, simply open the remarks from here. Highlight the jacket in this case and tag a team member directly in the tool. You can also highlight that this inbox could be linked to Slack, too. Then over here in the dashboard, you can see a red dot. This would be shown to the tagged person who can be anyone from the annotator's team or reviewers from either side. The blue dot indicates that the specific message has not been read yet. Now clicking this button will take us to the labeling window where the tagged person can respond to the query. That's it. Now clicking on mark as read updates the message status. Here we also have filtering options for our inbox. The notification bar feature in Labaler's tool will streamline your annotation workflow and boost team collaboration. Book a demo with labellerr to learn more about annotations. The link is provided in the description."
    ],
    "transcript_word_count": 192,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "b6pxZZiV9jk",
    "title": "Gap Filler - Your Solution to Faster Gap Filling",
    "description": "Ever wished your annotation gaps could fill effortlessly? Labellerr’s Gap Filler and Clip Mode make precise segmentation faster than ever! In this demo, watch how we can use it.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=b6pxZZiV9jk",
    "embed_url": "https://www.youtube.com/embed/b6pxZZiV9jk",
    "duration": 60,
    "view_count": 66,
    "upload_date": "20250403",
    "uploader": "Labellerr",
    "tags": [
      "annotation workflow",
      "annotation features",
      "annotation software",
      "segmentation mask",
      "annotation tutorial",
      "annotation best practices",
      "annotation for machine learning",
      "image segmentation",
      "polygon annotation",
      "semantic segmentation",
      "data annotation",
      "image annotation tool",
      "v7labs alternative",
      "dataset creation",
      "image datasets",
      "gap filler",
      "image annotation",
      "annotation tool",
      "precise annotation",
      "segmentation tool",
      "annotation demo",
      "image labeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "ever wished your gaps could fill without much of hard work Gap filler by labellerr is your solution for perfect precise annotations let's quickly see the demo here I have a family group picture let's quickly label it through Sam quick right okay so now we see the gaps between The annotation let's fill it through magic filler do watch our video on image annotation to understand this feature now we see a common border here simply activate wife annotation enable clip mode and press C to adjust the image along the common border and boom you're done imagine the time it would have taken to manually get such precise annotation for this part two combine Gap filler with clip mode for Ultra fast precise segmentation don't forget to book a demo with us to explore more about annotation link is provided in description",
    "transcript_chunks": [
      "ever wished your gaps could fill without much of hard work Gap filler by labellerr is your solution for perfect precise annotations let's quickly see the demo here I have a family group picture let's quickly label it through Sam quick right okay so now we see the gaps between The annotation let's fill it through magic filler do watch our video on image annotation to understand this feature now we see a common border here simply activate wife annotation enable clip mode and press C to adjust the image along the common border and boom you're done imagine the time it would have taken to manually get such precise annotation for this part two combine Gap filler with clip mode for Ultra fast precise segmentation don't forget to book a demo with us to explore more about annotation link is provided in description"
    ],
    "transcript_word_count": 144,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "D9JT1IEaGN0",
    "title": "Effortless Image Annotation with Labellerr’s Polygon Eraser & CLIP Mode",
    "description": "In this video, we demonstrate how Labellerr's powerful new features—Polygon Eraser and CLIP Mode—can transform your image annotation workflow. By combining these tools with SAM and other existing features, you can tackle even the most complex annotation challenges with ease. Watch as we show you how to refine and perfect annotations in fewer steps, ensuring accuracy and flexibility in your labeling tasks.\n\nChapters\n0:00 Introduction to Labellerr's Image Annotation Tool\n0:05 Overview of Polygon Eraser and CLIP Mode Features\n0:18 Annotating Ocean and Person with SAM\n0:24 Identifying Annotation Challenges\n0:43 Using Polygon Eraser for Precise Labeling\n0:55 Refining Annotations with Polygon Adjustments\n1:02 Streamlining Annotations with CLIP Mode\n1:30 Benefits of Labellerr’s Annotation Tools\n1:40 Book a Demo with Labellerr\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=D9JT1IEaGN0",
    "embed_url": "https://www.youtube.com/embed/D9JT1IEaGN0",
    "duration": 104,
    "view_count": 52,
    "upload_date": "20250402",
    "uploader": "Labellerr",
    "tags": [
      "labellerr platform",
      "annotation workflow",
      "annotation software",
      "annotation productivity",
      "annotation best practices",
      "computer vision annotation",
      "flexible annotation",
      "precise annotation",
      "image segmentation",
      "annotation tips",
      "annotation for AI",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "image datasets",
      "polygon eraser",
      "image annotation",
      "annotation tool",
      "smart annotation",
      "segmentation tool",
      "Sam annotation",
      "annotation features",
      "annotation demo"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "labeling complex images just got a whole lot smarter with labellerrs two new features polygon eraser and clip mode these tools can effectively tackle any annotation challenge when combined with Sam and other existing features so after uploading this picture in our tool I'll quickly annotate the ocean and the person using Sam so how many of you noticed that the area right here is not labeled as an ocean some of you might think of selecting ocean as an object and annotating this region separately but there is a problem after hiding this annotation we can see that this area still falls under the man's label so to tackle such similar problems we have introduced the two new features firstly let's understand the polygon eraser as you just saw it removes the sketched area's annotation need to perfect the shape no problem just drag the key points to adjust the polygon until it fits perfectly now I can use the previous method and label the area as an ocean now I'll show you how we can do the same annotation in fewer steps firstly activate clip mode oh and one more thing you can even delete your annotation made using the polygon eraser so coming back to it after activating clip mode and selecting the object again sketch the desired area so here it is we can clearly see a void is created here and this area now doesn't belong to the man but to the ocean only with labellerr polygon eraser and clip mode as I described earlier image annotation has become more flexible and faster book a demo with labellerr to know more about annotation Link in the description",
    "transcript_chunks": [
      "labeling complex images just got a whole lot smarter with labellerrs two new features polygon eraser and clip mode these tools can effectively tackle any annotation challenge when combined with Sam and other existing features so after uploading this picture in our tool I'll quickly annotate the ocean and the person using Sam so how many of you noticed that the area right here is not labeled as an ocean some of you might think of selecting ocean as an object and annotating this region separately but there is a problem after hiding this annotation we can see that this area still falls under the man's label so to tackle such similar problems we have introduced the two new features firstly let's understand the polygon eraser as you just saw it removes the sketched area's annotation need to perfect the shape no problem just drag the key points to adjust the polygon until it fits perfectly now I can use the previous method and label the area as an ocean now I'll show you how we can do the same annotation in fewer steps firstly activate clip mode oh and one more thing you can even delete your annotation made using the polygon eraser so coming back to it after activating clip mode and selecting the object again sketch the desired area so here it is we can clearly see a void is created here and this area now doesn't belong to the man but to the ocean only with labellerr polygon eraser and clip mode as I described earlier image annotation has become more flexible and faster book a demo with labellerr to know more about annotation Link in the description"
    ],
    "transcript_word_count": 282,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "Rqwwnq1GfBU",
    "title": "MCP - Your Solution to Build an AI Agent",
    "description": "Building AI agents can be complicated, with endless debugging and API issues. MCP (Model Context Protocol) simplifies this by providing pre-built tools that connect AI models with real-world applications.\nIn this video, we explain how MCP works, from managing tools and resources to automating LinkedIn posts. With MCP, scaling to platforms like Twitter and Instagram is as easy as updating a single line of code.\nWatch to learn how MCP can speed up AI automation\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=Rqwwnq1GfBU",
    "embed_url": "https://www.youtube.com/embed/Rqwwnq1GfBU",
    "duration": 143,
    "view_count": 127,
    "upload_date": "20250331",
    "uploader": "Labellerr",
    "tags": [
      "AI agent demo",
      "cross-platform AI automation",
      "AI application integration",
      "AI agent best practices",
      "MCP explained",
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "annotation software",
      "dataset creation",
      "image datasets",
      "Model Context Protocol",
      "MCP AI",
      "AI agent building",
      "LLM tools integration",
      "AI agent workflow",
      "MCP server",
      "MCP client",
      "AI agent protocol",
      "AI tools 2025",
      "large language models",
      "AI agent tutorial"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Hey everyone, today we're diving into something that can speed up AI agent building by 100x. Yes, you heard that right. So, assuming you have seen our video on AI agents before, in short, AI agents are a combination of LLM plus some tools. Now, imagine you've built an AI agent managing each of these tools individually. It can turn your project into a debugging nightmare. I'm sure many of you have felt that pain. Our team definitely felt that. Endless code fixes, broken UI selectors, and constant OOTH headaches. That's where MCP comes into the picture. MCP stands for model context protocol, and it's designed to bridge the gap between large language models and real world applications. In short, it's the source for all the tools required by LLM to operate as an AI agent. Last week, our team was automating our social media posting on LinkedIn. Let me explain the components of MCP for that scenario. This is your AI like GPT4 or Claude. It writes your post and knows how to tell LinkedIn what to do. It acts like a smart manager, giving clear instructions. The AI needs information to work well. MCP gives it tools like get old posts or upload a new one. Resources, your LinkedIn content files or templates, prompts, pre-written formats for posting. Everything the AI needs is ready and easy to use. This part handles how the AI and LinkedIn talk. MCP ensures the AI knows what tools it can use, follows simple rules to get things done. Just say, \"Post this on LinkedIn,\" and MCP makes it happen. You don't need to build anything. MCP servers offer tools like post to LinkedIn or get stats. MCP clients send your AI's requests. Your AI writes the post. MCP clicks the buttons. That's it. Your AI agent just mastered LinkedIn. Great. But now your boss wants to automate Twitter, Instagram, and Tik Tok. With MCP, scaling is easy. It's just changing a line of code on the required MCP server. MCP's secret, pre-built tools for every platform. No new code, no API hurdles. Just select the right MCP server for your chatbot, which acts as an MCP client, and your AI agent can start posting crossplatform in under an hour, and your boss is a happy man. Now, that's it. If you want to dive deeper into MCP, don't forget to visit our blog. Link is provided in the description.",
    "transcript_chunks": [
      "Hey everyone, today we're diving into something that can speed up AI agent building by 100x. Yes, you heard that right. So, assuming you have seen our video on AI agents before, in short, AI agents are a combination of LLM plus some tools. Now, imagine you've built an AI agent managing each of these tools individually. It can turn your project into a debugging nightmare. I'm sure many of you have felt that pain. Our team definitely felt that. Endless code fixes, broken UI selectors, and constant OOTH headaches. That's where MCP comes into the picture. MCP stands for model context protocol, and it's designed to bridge the gap between large language models and real world applications. In short, it's the source for all the tools required by LLM to operate as an AI agent. Last week, our team was automating our social media posting on LinkedIn. Let me explain the components of MCP for that scenario. This is your AI like GPT4 or Claude. It writes your post and knows how to tell LinkedIn what to do. It acts like a smart manager, giving clear instructions. The AI needs information to work well. MCP gives it tools like get old posts or upload a new one. Resources, your LinkedIn content files or templates, prompts, pre-written formats for posting. Everything the AI needs is ready and easy to use. This part handles how the AI and LinkedIn talk. MCP ensures the AI knows what tools it can use, follows simple rules to get things done. Just say, \"Post this on LinkedIn,\" and MCP makes it happen. You don't need to build anything. MCP servers offer tools like post to LinkedIn or get stats. MCP clients send your AI's requests. Your AI writes the post. MCP clicks the buttons. That's",
      "it. Your AI agent just mastered LinkedIn. Great. But now your boss wants to automate Twitter, Instagram, and Tik Tok. With MCP, scaling is easy. It's just changing a line of code on the required MCP server. MCP's secret, pre-built tools for every platform. No new code, no API hurdles. Just select the right MCP server for your chatbot, which acts as an MCP client, and your AI agent can start posting crossplatform in under an hour, and your boss is a happy man. Now, that's it. If you want to dive deeper into MCP, don't forget to visit our blog. Link is provided in the description."
    ],
    "transcript_word_count": 407,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "y3dvDFEfhhU",
    "title": "Are Vision AI Agents replacing Humans?",
    "description": "Vision AI Agents are redefining how machines perceive and interact with the world. Unlike traditional computer vision, these agents don’t just recognize objects—they analyze, reason, and act in real-time. From enhancing public safety by scanning thousands of surveillance cameras to detecting tumors invisible to the human eye, their impact spans across industries.\nBut what makes them different from standard AI models? What are their core components, and what challenges must they overcome to reach their full potential? \nFind your answers in the video.\n\nLink for the blog: https://www.labellerr.com/blog/vision-ai-agents-how-they-work-examples/\nInterested in learning more about our services?\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\nFind us on Social Media Platforms: LinkedIn:   / labellerr  \nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=y3dvDFEfhhU",
    "embed_url": "https://www.youtube.com/embed/y3dvDFEfhhU",
    "duration": 168,
    "view_count": 239,
    "upload_date": "20250328",
    "uploader": "Labellerr",
    "tags": [
      "SAM segmentation",
      "AI in transportation",
      "AI in healthcare",
      "AI in manufacturing",
      "AI agents",
      "AI decision making",
      "machine learning vision",
      "object recognition",
      "AI for surveillance",
      "visual data labeling",
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "dataset creation",
      "annotation software",
      "image datasets",
      "vision AI agents",
      "computer vision",
      "self-driving cars",
      "autonomous vehicles",
      "real-time object detection",
      "YOLOv11"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "This is how we see the street while sitting in our car, effortlessly glancing at it and immediately recognizing cars, pedestrians, and traffic lights. We don't think about it. It just happens. But this this is how the self-driving car you're sitting in sees the same street. It doesn't just look. It analyzes, labels, and understands everything in real time. By the way, did you notice the number plate in this picture at first sight? No. Right. But your car noticed it. That's what sets computer vision apart from human vision. Now, imagine something that goes beyond just recognizing objects. One that can reason, make decisions, and interact intelligently with the world in real time. That's the power vision AI agents hold. But what sets them apart from traditional computer vision models? What are the components operating them? And where are they making an impact? More importantly, what challenges must they overcome to truly revolutionize industries? Let's find out. No doubt traditional computer vision is powerful but it's limited. The moment it encounters something outside its training data, it fails. Also, it is not suited for tasks that require speed since labeling millions of images manually is difficult. But vision AI agents, they're adaptive learning from real-time data. They contain three main components. Eyes vision models like YOLO V11 and SAM that detect objects in any condition. Rain, glare, even through walls. Brains. Use of language model like GPT4 that understands text commands. Find every SUV with outofstate plates. Action engines that do. Alerting cops, locking down streets, all in real time. And if they mess up, you just tell them it's three, not eight. And it automatically fixes this error everywhere. Here's where it gets insane. In New York, cops used to spend days scrubbing footage for stolen cars. Now, vision AI agents scan 10,000 cameras at once, spotting even scratched off plates and send alerts before the thief finishes their coffee. And it's not just streets. In hospitals, these agents spot tumors invisible to the human eye. In factories, they predict machine failures by seeing the details the human eye can't detect. This is how a vision agent works. Just give the prompt and leave every other task to your visual AI pilot. But as transformative as these systems are to our society, they must navigate significant challenges like data privacy concerns, high computational costs, and biases from training data to achieve seamless real world performance. So I'm leaving you with a question. How far is the time where vision AI agents complement human vision? For the answer and other insights, do check our blog on vision AI agents. The link is provided in the description.",
    "transcript_chunks": [
      "This is how we see the street while sitting in our car, effortlessly glancing at it and immediately recognizing cars, pedestrians, and traffic lights. We don't think about it. It just happens. But this this is how the self-driving car you're sitting in sees the same street. It doesn't just look. It analyzes, labels, and understands everything in real time. By the way, did you notice the number plate in this picture at first sight? No. Right. But your car noticed it. That's what sets computer vision apart from human vision. Now, imagine something that goes beyond just recognizing objects. One that can reason, make decisions, and interact intelligently with the world in real time. That's the power vision AI agents hold. But what sets them apart from traditional computer vision models? What are the components operating them? And where are they making an impact? More importantly, what challenges must they overcome to truly revolutionize industries? Let's find out. No doubt traditional computer vision is powerful but it's limited. The moment it encounters something outside its training data, it fails. Also, it is not suited for tasks that require speed since labeling millions of images manually is difficult. But vision AI agents, they're adaptive learning from real-time data. They contain three main components. Eyes vision models like YOLO V11 and SAM that detect objects in any condition. Rain, glare, even through walls. Brains. Use of language model like GPT4 that understands text commands. Find every SUV with outofstate plates. Action engines that do. Alerting cops, locking down streets, all in real time. And if they mess up, you just tell them it's three, not eight. And it automatically fixes this error everywhere. Here's where it gets insane. In New York, cops used to spend days scrubbing footage for stolen cars.",
      "Now, vision AI agents scan 10,000 cameras at once, spotting even scratched off plates and send alerts before the thief finishes their coffee. And it's not just streets. In hospitals, these agents spot tumors invisible to the human eye. In factories, they predict machine failures by seeing the details the human eye can't detect. This is how a vision agent works. Just give the prompt and leave every other task to your visual AI pilot. But as transformative as these systems are to our society, they must navigate significant challenges like data privacy concerns, high computational costs, and biases from training data to achieve seamless real world performance. So I'm leaving you with a question. How far is the time where vision AI agents complement human vision? For the answer and other insights, do check our blog on vision AI agents. The link is provided in the description."
    ],
    "transcript_word_count": 448,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "iaYX6BCZgG4",
    "title": "How Labellerr provided personalized Solution I Jordan, Senior AI Software Engineer At Spot AI",
    "description": "Discover how Spot AI improved their object detection for people and vehicles with Labellerr's high-quality data annotation services!\n\nIn this video, Jordan, an AI engineer at Spot AI, shares how their team struggled with model evaluation and improvement before finding Labellerr. After trying multiple providers, they chose Labellerr for its deep labeling expertise, affordability, and excellent support.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=iaYX6BCZgG4",
    "embed_url": "https://www.youtube.com/embed/iaYX6BCZgG4",
    "duration": 62,
    "view_count": 42,
    "upload_date": "20250325",
    "uploader": "Labellerr",
    "tags": [
      "annotation support",
      "annotation workflow",
      "annotation provider",
      "affordable annotation",
      "annotation testimonial",
      "annotation for machine learning",
      "annotation for startups",
      "AI engineer testimonial",
      "SpotAI",
      "data labeling platform",
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "annotation software",
      "dataset creation",
      "image datasets",
      "labellerr review",
      "AI annotation tool",
      "vehicle detection"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Hey, my name is Jordan. I am a AI engineer at Spotai. So, our main challenge before using labellerr was determining how well our model is doing as in coming up with metrics and to improve our models. In our particular case for object detection on people and vehicles. So, when we contacted a lot of different providers and labellerr was one of the few we talked to that clearly understood uh labeling and they were simply cheaper than the competition. So when we signed up uh they were very helpful in providing pretty personal support on actually getting our thing working. Uh we have a group Slack chat with uh them for uploading data and all our problems and we pretty impressed with both all the help we got and the quality of the annotations. We're a pretty lean team. We don't actually have much bandwidth to figure all this out, but yet they kind of all fell together. And yeah, that's our experience. I would highly recommend labellerr for anyone else.",
    "transcript_chunks": [
      "Hey, my name is Jordan. I am a AI engineer at Spotai. So, our main challenge before using labellerr was determining how well our model is doing as in coming up with metrics and to improve our models. In our particular case for object detection on people and vehicles. So, when we contacted a lot of different providers and labellerr was one of the few we talked to that clearly understood uh labeling and they were simply cheaper than the competition. So when we signed up uh they were very helpful in providing pretty personal support on actually getting our thing working. Uh we have a group Slack chat with uh them for uploading data and all our problems and we pretty impressed with both all the help we got and the quality of the annotations. We're a pretty lean team. We don't actually have much bandwidth to figure all this out, but yet they kind of all fell together. And yeah, that's our experience. I would highly recommend labellerr for anyone else."
    ],
    "transcript_word_count": 173,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "pFZUIHf8I0I",
    "title": "Boost Barcode Accuracy with Labellerr’s AI",
    "description": "Barcodes are everywhere—on products, tickets, medical records, and shipping labels. But traditional barcode scanners often fail due to faded labels, poor lighting, or different barcode standards across industries. In this video, we explore how computer vision is transforming barcode verification, making it faster, smarter, and more reliable.\n\nAlso Learn how Labellerr simplifies barcode annotation, making AI models more accurate and efficient in reading 1D and 2D barcodes. Watch as we demonstrate how to annotate barcodes, classify components, and improve scanning accuracy using Labellerr’s powerful tools.\n\nChapters\n0:00 Introduction to Barcodes\n0:37 Understanding Barcodes\n0:57 Challenges with Traditional Barcode Scanners\n1:27 Computer Vision Solutions for Barcode Scanning\n1:58 Introduction to Labellerr for Barcode Annotation\n2:04 Annotating 1D Barcodes\n2:46 Classifying Barcode Components\n3:05 Adding Start and Stop Characters\n3:12 Using OCR for Check Digits\n3:26 Simplifying Global Barcode Classification\n3:40 Conclusion and Demo Booking\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=pFZUIHf8I0I",
    "embed_url": "https://www.youtube.com/embed/pFZUIHf8I0I",
    "duration": 227,
    "view_count": 96,
    "upload_date": "20250324",
    "uploader": "Labellerr",
    "tags": [
      "barcode recognition",
      "scanner troubleshooting",
      "annotation tools",
      "image annotation",
      "AI annotation",
      "computer vision AI",
      "object detection",
      "barcode accuracy",
      "annotation quality control",
      "bounding box annotation",
      "semantic segmentation",
      "barcode OCR annotation",
      "barcode annotation tools",
      "barcode annotation for manufacturing",
      "data annotation",
      "image annotation tool",
      "dataset creation",
      "annotation software",
      "image datasets",
      "barcode scanning",
      "computer vision barcode",
      "barcode annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Imagine you're at your favorite restaurant scanning some lines and you are provided with complete menu instantly. Or if you're a lab professional, a quick scan brings up a patient's complete medical history on your screen in seconds. What's the tiny tech hero behind so many of these tasks? Barcodes. These little black and white lines are not as simple as they look. Today we're diving into the world of barcodes and why barcode scanners sometimes fail and how computer vision is providing the solutions with the help of tools like labellerr. Stick around because this is going to get exciting. Understanding barcode. A barcode is a secret code for machines made from lines, dots, and spaces. From grocery stores to airports, barcodes are everywhere. On products, tickets, books, and even the laptop you're watching this video on. But they don't always work perfectly. And that's where the trouble starts. Imagine a warehouse worker shipping a package, but the labels faded from rain. Scanner fails to read the code. Even if the barcode stays intact, the scanner still fails if used under improper light conditions. And sometimes if if both these conditions are met, we see that scanners cannot interpret the barcode due to varying standards of barcodes in different industries. These problems cost time, money, and patience, causing lost shipments or annoyed customers waiting in line. And that's where computer vision comes as a solution. With computer vision, blurry or damaged barcodes can be read. It also detects barcode errors before they cause problems, which is a complex task when done manually. And the best part, scanner using computer vision works in any condition. Whether it's low light, curved surfaces, or different scanning angles, it adapts to every challenge. All this is possible because of the learning that these models go through from thousands of annotated images. And to make this task faster and scalable, labellerr steps in. First, let's annotate a 1D barcode. Clearly recognizable as it's made from vertical lines and spaces. But how will the system recognize it? For that, I'll annotate the whole barcode region for the system to locate it. I have already added all the 1D types using attributes. We already have a video on it and other features that I'll be using on our channel. Do check that out. So yes, coming back choosing UPCA since there were 12 check digits beneath the lines. See the 10 digits here represent the UPCA type. To get more precision, adjustments can be made to your annotation too. This way, since we now know the type, labellerr also makes it easy to add a new object at any point in your annotation. Let's quickly create some of the components for the UPCA type barcode. Start character and stop character help the model recognize the beginning and end of the barcode. For check digits, I'll use OCR and add the number in the input field. This is how easy it is to classify all the barcodes used worldwide and mark their components using our tool. Now there's no need for separate scanners to read different barcode types. With the power of computer vision and annotation tools, a single system can accurately interpret both 1D and 2D barcodes along with their different versions. Book a demo with labellerr to explore more about annotation. Link is provided in the description.",
    "transcript_chunks": [
      "Imagine you're at your favorite restaurant scanning some lines and you are provided with complete menu instantly. Or if you're a lab professional, a quick scan brings up a patient's complete medical history on your screen in seconds. What's the tiny tech hero behind so many of these tasks? Barcodes. These little black and white lines are not as simple as they look. Today we're diving into the world of barcodes and why barcode scanners sometimes fail and how computer vision is providing the solutions with the help of tools like labellerr. Stick around because this is going to get exciting. Understanding barcode. A barcode is a secret code for machines made from lines, dots, and spaces. From grocery stores to airports, barcodes are everywhere. On products, tickets, books, and even the laptop you're watching this video on. But they don't always work perfectly. And that's where the trouble starts. Imagine a warehouse worker shipping a package, but the labels faded from rain. Scanner fails to read the code. Even if the barcode stays intact, the scanner still fails if used under improper light conditions. And sometimes if if both these conditions are met, we see that scanners cannot interpret the barcode due to varying standards of barcodes in different industries. These problems cost time, money, and patience, causing lost shipments or annoyed customers waiting in line. And that's where computer vision comes as a solution. With computer vision, blurry or damaged barcodes can be read. It also detects barcode errors before they cause problems, which is a complex task when done manually. And the best part, scanner using computer vision works in any condition. Whether it's low light, curved surfaces, or different scanning angles, it adapts to every challenge. All this is possible because of the learning that these",
      "models go through from thousands of annotated images. And to make this task faster and scalable, labellerr steps in. First, let's annotate a 1D barcode. Clearly recognizable as it's made from vertical lines and spaces. But how will the system recognize it? For that, I'll annotate the whole barcode region for the system to locate it. I have already added all the 1D types using attributes. We already have a video on it and other features that I'll be using on our channel. Do check that out. So yes, coming back choosing UPCA since there were 12 check digits beneath the lines. See the 10 digits here represent the UPCA type. To get more precision, adjustments can be made to your annotation too. This way, since we now know the type, labellerr also makes it easy to add a new object at any point in your annotation. Let's quickly create some of the components for the UPCA type barcode. Start character and stop character help the model recognize the beginning and end of the barcode. For check digits, I'll use OCR and add the number in the input field. This is how easy it is to classify all the barcodes used worldwide and mark their components using our tool. Now there's no need for separate scanners to read different barcode types. With the power of computer vision and annotation tools, a single system can accurately interpret both 1D and 2D barcodes along with their different versions. Book a demo with labellerr to explore more about annotation. Link is provided in the description."
    ],
    "transcript_word_count": 562,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "pY_o4EvYMz8",
    "title": "Boost AI Image Annotation 10X with Labellerr’s CLIP Mode",
    "description": "Tired of struggling with manual stretching keypoints and overlapping areas to fix your annotations? Labellerr’s CLIP mode solves this problem in seconds! In this video, we’ll show you how CLIP mode refines annotations with precision and speed. Watch a step-by-step demo and explore real-world use cases in medical imaging, autonomous driving, retail, and agriculture.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=pY_o4EvYMz8",
    "embed_url": "https://www.youtube.com/embed/pY_o4EvYMz8",
    "duration": 89,
    "view_count": 47,
    "upload_date": "20250324",
    "uploader": "Labellerr",
    "tags": [
      "medical image annotation",
      "agricultural annotation",
      "retail image annotation",
      "image segmentation tool",
      "annotation for industry",
      "annotation demo",
      "annotation use cases",
      "advanced annotation tools",
      "annotation guidelines",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "dataset creation",
      "image datasets",
      "clip mode annotation",
      "annotation precision",
      "annotation efficiency",
      "overlapping annotations",
      "SAM annotation",
      "segmentation accuracy",
      "annotation boundaries"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Tired of stretching out the key points for setting up precise boundaries or having trouble in adjusting the overlapped area? Imagine a feature that could do it within seconds? Clip is a feature designed for such problems in labellerrs tool. Stick around because by the end of this video, you'll see exactly how it can save you time and improve your workflow. Here I have annotated both these objects using SAM and still we see some missing areas. Starting from this key point to this, let's make boundaries sharp and precise using clip mode. Firstly, using magic filler, we will stretch the dog's annotation area outside this common border. Clearly, we see the overlapping region. Now, simply activate the polygon we want to use clip mode on the dog in this case. And turn on the clip button from here. Then press. See how accurately and quickly we have solved the problem. Now, you might be wondering where can I use this? The possibilities are endless. In medical imaging, segmenting organs or tumors with pinpoint accuracy. For autonomous driving, it's perfect for cleanly segmenting pedestrians, cars, or road signs. In agriculture, segmenting healthy and diseased crops. No mix-ups, just clear data for farmers. Even in retail, segmenting products on a shelf becomes effortless. labellerr's clip feature is flexible enough to handle complexities in data from different industries. Book a demo with Lavaler to explore more about annotations. Link is provided in the description.",
    "transcript_chunks": [
      "Tired of stretching out the key points for setting up precise boundaries or having trouble in adjusting the overlapped area? Imagine a feature that could do it within seconds? Clip is a feature designed for such problems in labellerrs tool. Stick around because by the end of this video, you'll see exactly how it can save you time and improve your workflow. Here I have annotated both these objects using SAM and still we see some missing areas. Starting from this key point to this, let's make boundaries sharp and precise using clip mode. Firstly, using magic filler, we will stretch the dog's annotation area outside this common border. Clearly, we see the overlapping region. Now, simply activate the polygon we want to use clip mode on the dog in this case. And turn on the clip button from here. Then press. See how accurately and quickly we have solved the problem. Now, you might be wondering where can I use this? The possibilities are endless. In medical imaging, segmenting organs or tumors with pinpoint accuracy. For autonomous driving, it's perfect for cleanly segmenting pedestrians, cars, or road signs. In agriculture, segmenting healthy and diseased crops. No mix-ups, just clear data for farmers. Even in retail, segmenting products on a shelf becomes effortless. labellerr's clip feature is flexible enough to handle complexities in data from different industries. Book a demo with Lavaler to explore more about annotations. Link is provided in the description."
    ],
    "transcript_word_count": 243,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "EviGR_XMTfQ",
    "title": "Enhancing Annotation with Classification Feature",
    "description": "Learn how to classify images for self-driving cars using Labellerr’s powerful annotation tools. From identifying traffic lights and speed limit signs to handling complex real-world scenarios, this video covers key classification techniques—radio button, input field, boolean, and multi-select classification. Discover how AI models learn from precise annotations to make informed decisions on the road.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=EviGR_XMTfQ",
    "embed_url": "https://www.youtube.com/embed/EviGR_XMTfQ",
    "duration": 122,
    "view_count": 54,
    "upload_date": "20250320",
    "uploader": "Labellerr",
    "tags": [
      "annotation for computer vision",
      "annotation tools 2025",
      "annotation best practices",
      "annotation demo",
      "precise annotation",
      "sensor fusion annotation",
      "LIDAR annotation",
      "annotation for traffic analysis",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "dataset creation",
      "image datasets",
      "classification annotation",
      "self-driving car dataset",
      "traffic light annotation",
      "speed limit annotation",
      "boolean annotation",
      "annotation for AI models",
      "annotation for autonomous vehicles"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Imagine transforming unstructured data into meaningful categories. That's what classification does. Whether you're working with videos, images, or text, labellerr's classification feature works in all of them. Here I have an image from a self-driving car data set. Our goal is to classify it based on different situations. Here is the traffic light in this picture red, yellow, or green. With radio button classification, annotators pick one clear option, teaching the AI to recognize traffic signals with confidence. A faraway speed limit sign? No problem. We type the exact number into an input field, turning ambiguity into precise data for the machine, definitely helping the machine understand that it is 30 and not 80. Is there a pedestrian crossing the road? Clearly the answer is no. In such situations we can prefer a boolean mine. It helps for a yes no decision helping the model to learn to prioritize safety in this case. At last, we'll explore the multi select classification type which is essential for handling the complexities of real world driving. Unlike simple yes, no, or single option classifications, real world scenarios often involve multiple factors at the same time. These are some of the factors required in knowledge for self-driving cars. This approach helps build a more adaptable and intelligent model that can make informed decisions in diverse road situations. So, we see that in one image there are countless details. Behind every trusted self-driving car are millions of perfectly annotated frames. labellerr's tool helps you annotate your data for different industries, offering you the best features required for faster and precise results. Book a demo to explore more about annotation. The link is provided in the description.",
    "transcript_chunks": [
      "Imagine transforming unstructured data into meaningful categories. That's what classification does. Whether you're working with videos, images, or text, labellerr's classification feature works in all of them. Here I have an image from a self-driving car data set. Our goal is to classify it based on different situations. Here is the traffic light in this picture red, yellow, or green. With radio button classification, annotators pick one clear option, teaching the AI to recognize traffic signals with confidence. A faraway speed limit sign? No problem. We type the exact number into an input field, turning ambiguity into precise data for the machine, definitely helping the machine understand that it is 30 and not 80. Is there a pedestrian crossing the road? Clearly the answer is no. In such situations we can prefer a boolean mine. It helps for a yes no decision helping the model to learn to prioritize safety in this case. At last, we'll explore the multi select classification type which is essential for handling the complexities of real world driving. Unlike simple yes, no, or single option classifications, real world scenarios often involve multiple factors at the same time. These are some of the factors required in knowledge for self-driving cars. This approach helps build a more adaptable and intelligent model that can make informed decisions in diverse road situations. So, we see that in one image there are countless details. Behind every trusted self-driving car are millions of perfectly annotated frames. labellerr's tool helps you annotate your data for different industries, offering you the best features required for faster and precise results. Book a demo to explore more about annotation. The link is provided in the description."
    ],
    "transcript_word_count": 281,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "5Do6wvpANlU",
    "title": "Boost Video Annotation with Labellerr’s Attribute Tool",
    "description": "Ever wondered how an NBA scoreboard updates instantly without human effort? It all comes down to video annotation—and tools like Labellerr make it happen.\n\nIn this video, we demonstrate how attribute annotation helps machines recognize teams in an NBA game. Using our SAM and SAM-2 features, we annotate players and then assign attributes to classify them by team. The result? The machine can now identify which team scored in real time!\n\nBeyond sports, attributes are essential for AI models in various industries. With Labellerr, adding attributes is fast, seamless, and scalable—helping you build smarter AI models with accurate, structured data.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=5Do6wvpANlU",
    "embed_url": "https://www.youtube.com/embed/5Do6wvpANlU",
    "duration": 138,
    "view_count": 32,
    "upload_date": "20250319",
    "uploader": "Labellerr",
    "tags": [
      "sports data labeling",
      "computer vision sports",
      "AI in basketball",
      "player segmentation",
      "video annotation workflow",
      "AI-powered sports analytics",
      "data annotation",
      "image annotation tool",
      "v7labs alternative",
      "aws sagemaker alternative",
      "annotation software",
      "dataset creation",
      "image datasets",
      "video annotation",
      "sports video annotation",
      "basketball analysis",
      "NBA analytics",
      "player tracking",
      "attribute annotation",
      "SAM annotation",
      "sports AI",
      "real-time analytics",
      "AI model training"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "imagine you're watching an NBA game players sprinting passing slamming down jaw-dropping shots and digital scoreboard moving instantly and then you start wondering how the digital scorecard instantly gets updated when a player scores a basket without human effort the base for all this is video annotation and tools like leeler make it happen stick with me you're about to see this in action with a real NBA clip I have this 2cond clip of an NBA game between the Boston Celtics and Oklahoma City Thunder after uploading the video to our tool I annotated the players using Sam and with Sam 2 I annotated all the frames we already have videos about them on our channel do check them out cool now I'll show you how to add attributes we'll choose radio here and add the two options for the two teams I'll quickly do it for all the other nine players we as humans can anytime tell which team scored maybe by the jersey or because we know the player but what about a machine how does it know who's on which team just to add this extra context we are using attributes well we are done with adding attributes for each player I've annotated I'll add an attribute team Boston for this guy team Thunder for this one and quickly do it for others too now now I have added attributes to each of the Players let's play the video again basket boom I'll pause here and click on the player who scored look at this not only do we see the player's name but the team too team Boston the machine now knows exactly which team scored all thanks to that attribute we added and it's not just Sports think autonomous cars figure fing out road signs or security systems tracking activity with lebell adding attributes is fast and easy leveling up your annotation game no matter what you're working on book a demo to learn more about annotations with labellerr link is provided in the description",
    "transcript_chunks": [
      "imagine you're watching an NBA game players sprinting passing slamming down jaw-dropping shots and digital scoreboard moving instantly and then you start wondering how the digital scorecard instantly gets updated when a player scores a basket without human effort the base for all this is video annotation and tools like leeler make it happen stick with me you're about to see this in action with a real NBA clip I have this 2cond clip of an NBA game between the Boston Celtics and Oklahoma City Thunder after uploading the video to our tool I annotated the players using Sam and with Sam 2 I annotated all the frames we already have videos about them on our channel do check them out cool now I'll show you how to add attributes we'll choose radio here and add the two options for the two teams I'll quickly do it for all the other nine players we as humans can anytime tell which team scored maybe by the jersey or because we know the player but what about a machine how does it know who's on which team just to add this extra context we are using attributes well we are done with adding attributes for each player I've annotated I'll add an attribute team Boston for this guy team Thunder for this one and quickly do it for others too now now I have added attributes to each of the Players let's play the video again basket boom I'll pause here and click on the player who scored look at this not only do we see the player's name but the team too team Boston the machine now knows exactly which team scored all thanks to that attribute we added and it's not just Sports think autonomous cars figure fing out road",
      "signs or security systems tracking activity with lebell adding attributes is fast and easy leveling up your annotation game no matter what you're working on book a demo to learn more about annotations with labellerr link is provided in the description"
    ],
    "transcript_word_count": 341,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "qR7HeVluz-g",
    "title": "AI-Powered Home Service Annotation with Labellerr",
    "description": "Imagine inspecting properties for estimates and measurements without the hassle of multiple site visits — just a single photograph is enough. This is made possible through computer vision technology.\n\nLabellerr’s tool streamlines the annotation process, offering both manual and automated labeling options to suit diverse needs. While manual annotation can be time-consuming and complex, especially for intricate layouts and structural elements, Labellerr’s SAM (Segment Anything Model) feature automates the process with just one click.\n\nBy using Labellerr’s platform, home service providers, insurance companies, and moving businesses can achieve faster estimates, accurate measurements, and more efficient property assessments.\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/generate-super-fast-annotations-for-virtual-surveys-model/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=qR7HeVluz-g",
    "embed_url": "https://www.youtube.com/embed/qR7HeVluz-g",
    "duration": 66,
    "view_count": 34,
    "upload_date": "20250313",
    "uploader": "Labellerr",
    "tags": [
      "AI for real estate",
      "annotation for insurance",
      "AI defect detection",
      "structural damage detection",
      "building maintenance AI",
      "AI-powered property estimates",
      "property image analysis",
      "real estate data labeling",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "dataset creation",
      "image datasets",
      "computer vision annotation",
      "automated labeling",
      "manual annotation",
      "SAM annotation",
      "structural element detection",
      "layout annotation",
      "annotation platform",
      "Labellerr tool"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "imagine inspecting properties for estimates and measurements without the hassle of multiple site visits do it with just a photograph this is achieved by computer vision technology labellerr tool is designed to streamline this process offering both automated and manual labeling firstly we'll annotate the lounge of course manual annotation presents lots of challenges due to labeling complex layouts measuring spaces or identifying structural elements accurately above all it is timec consuming we have a solution for this Sam a feature for automated labeling here's how it is done with just one click your annotations are made using labellerrs annotation platform in the home service industry Home Service Providers Insurance companies and moving businesses benefit from precise annotations enabling faster estimates accurate measurements and more efficient property assessments read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "imagine inspecting properties for estimates and measurements without the hassle of multiple site visits do it with just a photograph this is achieved by computer vision technology labellerr tool is designed to streamline this process offering both automated and manual labeling firstly we'll annotate the lounge of course manual annotation presents lots of challenges due to labeling complex layouts measuring spaces or identifying structural elements accurately above all it is timec consuming we have a solution for this Sam a feature for automated labeling here's how it is done with just one click your annotations are made using labellerrs annotation platform in the home service industry Home Service Providers Insurance companies and moving businesses benefit from precise annotations enabling faster estimates accurate measurements and more efficient property assessments read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 158,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "nw4GTygX3Jc",
    "title": "One-Click AI Wound Detection with Labellerr’s SAM: Recover Fast",
    "description": "Imagine a system that can instantly analyze wound images, identifying affected areas and even predicting potential causes. Wound image segmentation, powered by computer vision, helps healthcare providers track healing progress, assess severity, and monitor infection risks.\nIn this video, we demonstrate the manual annotation process for diabetic wounds, highlighting the challenges of irregular shapes, varying wound sizes, and subtle tissue differences - all of which make manual labeling time-consuming and complex.\nWe then introduce an efficient solution: Labellerr's automated annotation platform with SAM (Segment Anything Model). With just one click, annotations are generated.\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/enhancing-wound-image-segmentation/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=nw4GTygX3Jc",
    "embed_url": "https://www.youtube.com/embed/nw4GTygX3Jc",
    "duration": 72,
    "view_count": 82,
    "upload_date": "20250313",
    "uploader": "Labellerr",
    "tags": [
      "wound healing tracking",
      "tissue segmentation",
      "annotation for diagnosis",
      "healthcare data labeling",
      "medical dataset annotation",
      "annotation for researchers",
      "annotation for healthcare providers",
      "medical annotation tools",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "dataset creation",
      "image datasets",
      "wound image segmentation",
      "medical image annotation",
      "wound annotation",
      "computer vision healthcare",
      "medical image labeling",
      "SAM annotation",
      "automated medical annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Imagine A system that can instantly analyze wound images identifying affected areas or tell the cause of the wound wound image segmentation uses computer vision to help healthc care providers track healing assess severity and monitor infection risks with greater accuracy and efficiency label's tool is designed to streamline this process offering both automated and manual labeling firstly we'll annotate this diabetic wound manually of course manual annotation presents lots of challenges due to irregular shapes varying wound sizes subtle tissue differences and of course the timeconsuming nature of labeling large data sets we have a solution for this Sam a feature for automated labeling here's how it works with just one click your annotations are made using labellerrs annotation platform for wound annotation Healthcare Providers researchers and medical AI developers can benefit to improve diagnosis read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "Imagine A system that can instantly analyze wound images identifying affected areas or tell the cause of the wound wound image segmentation uses computer vision to help healthc care providers track healing assess severity and monitor infection risks with greater accuracy and efficiency label's tool is designed to streamline this process offering both automated and manual labeling firstly we'll annotate this diabetic wound manually of course manual annotation presents lots of challenges due to irregular shapes varying wound sizes subtle tissue differences and of course the timeconsuming nature of labeling large data sets we have a solution for this Sam a feature for automated labeling here's how it works with just one click your annotations are made using labellerrs annotation platform for wound annotation Healthcare Providers researchers and medical AI developers can benefit to improve diagnosis read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 166,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "YTpBxQ9wQN8",
    "title": "AI-Powered Fruit Disease Segmentation with Labellerr’s Tool",
    "description": "Discover how AI-powered fruit disease segmentation helps detect infections early, protecting crops and increasing yields. Labellerr’s platform streamlines the process with both manual and automated labeling options, including the SAM feature for one-click annotations. Learn how farmers, researchers, and agri-tech companies can benefit from faster, more accurate disease detection\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/precise-fruit-disease-segmentation/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=YTpBxQ9wQN8",
    "embed_url": "https://www.youtube.com/embed/YTpBxQ9wQN8",
    "duration": 73,
    "view_count": 38,
    "upload_date": "20250313",
    "uploader": "Labellerr",
    "tags": [
      "AI in agriculture",
      "crop disease detection",
      "agricultural computer vision",
      "automated fruit labeling",
      "SAM annotation agriculture",
      "annotation for farmers",
      "annotation for agronomists",
      "agricultural AI tools",
      "disease segmentation model",
      "agricultural image segmentation",
      "deep learning agriculture",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "dataset creation",
      "image datasets",
      "fruit disease segmentation",
      "fruit disease annotation",
      "plant disease detection"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "imagine spotting fruit diseases with pinpoint accuracy before they ruin a harvest precise fruit disease segmentation uses AI to detect infections early empowering Farmers to protect crops reduce losses and maximize yields with ease labellerrs tools streamlines fruit disease annotation with automated and manual labeling options ensuring accuracy and efficiency for diverse agricultural needs first we'll manually annotate infected areas a process that comes with challenges like subtle symptoms overlapping infections and the time intensive task of handling large data sets we have a solution for this Sam an automated labeling feature with just one click it instantly annotates the infected area saving time and boosting accuracy using labellerrs annotation platform for fruit disease segmentation Farmers agronomists researchers and agretech companies can enhance disease detection optimize treatments and maximize crop yields read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "imagine spotting fruit diseases with pinpoint accuracy before they ruin a harvest precise fruit disease segmentation uses AI to detect infections early empowering Farmers to protect crops reduce losses and maximize yields with ease labellerrs tools streamlines fruit disease annotation with automated and manual labeling options ensuring accuracy and efficiency for diverse agricultural needs first we'll manually annotate infected areas a process that comes with challenges like subtle symptoms overlapping infections and the time intensive task of handling large data sets we have a solution for this Sam an automated labeling feature with just one click it instantly annotates the infected area saving time and boosting accuracy using labellerrs annotation platform for fruit disease segmentation Farmers agronomists researchers and agretech companies can enhance disease detection optimize treatments and maximize crop yields read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 161,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "1VZy1Z2QoMY",
    "title": "Spot Animal Diseases Fast with AI-Powered Labellerr Tool",
    "description": "Imagine a farm where cameras monitor animals 24/7, detecting signs of illness or stress early. With Labellerr’s annotation platform, farmers, researchers, and agritech companies can streamline livestock monitoring and management. While manual annotation can be time-consuming and challenging, Labellerr's SAM feature automates the process, making annotations with just a few clicks. Learn how computer vision and AI-powered tools simplify livestock tracking, improve animal welfare, and optimize farm operations.\n\nInterested in learning more?\n\nhttps://www.labellerr.com/blog/how-labellerr-support-livestock-monitoring-to-ensure-healthy-cattle/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=1VZy1Z2QoMY",
    "embed_url": "https://www.youtube.com/embed/1VZy1Z2QoMY",
    "duration": 66,
    "view_count": 62,
    "upload_date": "20250313",
    "uploader": "Labellerr",
    "tags": [
      "AI animal tracking",
      "automated livestock annotation",
      "SAM annotation livestock",
      "animal welfare AI",
      "precision livestock farming",
      "pose estimation livestock",
      "object detection livestock",
      "animal stress detection",
      "movement tracking animals",
      "deep learning agriculture",
      "farm management AI",
      "veterinary AI",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "dataset creation",
      "image datasets",
      "livestock monitoring",
      "animal behavior analysis",
      "computer vision livestock"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "imagine a farm where cameras watch over animals 24/7 spotting signs of illness or stress before they become serious with computer vision farmers can track each animal's movement feeding habits and even emotional state labellerrs tool is designed to streamline this process offering both automated and manual labeling options firstly we'll annotate the cow manually of course manual annotation comes with many challenges handling large dynamic herds inconsistent animal postures and poor environmental conditions above all it's timec consuming we have a solution for this Sam a feature for automated labeling here's how it works with just a few clicks your annotations are done using labellerrs annotation platform in livestock monitoring Farmers agretech companies researchers and veterinarians can benefit from efficient livestock monitoring and management read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "imagine a farm where cameras watch over animals 24/7 spotting signs of illness or stress before they become serious with computer vision farmers can track each animal's movement feeding habits and even emotional state labellerrs tool is designed to streamline this process offering both automated and manual labeling options firstly we'll annotate the cow manually of course manual annotation comes with many challenges handling large dynamic herds inconsistent animal postures and poor environmental conditions above all it's timec consuming we have a solution for this Sam a feature for automated labeling here's how it works with just a few clicks your annotations are done using labellerrs annotation platform in livestock monitoring Farmers agretech companies researchers and veterinarians can benefit from efficient livestock monitoring and management read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 155,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "iz0RMdYTJ8M",
    "title": "Boost Food Grain Segmentation Efficiency with AI-Powered Labellerr",
    "description": "Discover how AI-powered fruit disease segmentation helps detect infections early, protecting crops and increasing yields. Labellerr’s platform streamlines the process with both manual and automated labeling options, including the SAM feature for one-click annotations. Learn how farmers, researchers, and agri-tech companies can benefit from faster, more accurate disease detection. Read our blog or book a demo to explore the future of agricultural annotation\n\nChapters\n0:00 Introduction to Grain Segmentation\n0:05 How Computer Vision Enhances Grain Labeling\n0:14 Labellerr’s Annotation Tools Overview\n0:21 Challenges in Annotating Small Grains\n0:36 SAM: One-Click Automated Labeling Feature\n0:48 Benefits for Farmers and Researchers\n1:01 Next Steps: Blog and Demo Links\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/food-grain-segmentation-labellerr/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=iz0RMdYTJ8M",
    "embed_url": "https://www.youtube.com/embed/iz0RMdYTJ8M",
    "duration": 70,
    "view_count": 49,
    "upload_date": "20250313",
    "uploader": "Labellerr",
    "tags": [
      "grain defect detection",
      "precision agriculture",
      "grain sorting automation",
      "agricultural AI tools",
      "crop analysis AI",
      "image segmentation agriculture",
      "smart farming solutions",
      "agritech annotation",
      "seed counting AI",
      "grain sorting machine vision",
      "data annotation",
      "image annotation tool",
      "deep learning",
      "data preprocessing",
      "image segmentation",
      "food grain segmentation",
      "grain classification",
      "computer vision agriculture",
      "AI grain sorting",
      "automated grain labeling",
      "SAM annotation grains"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "imagine accurately distinguishing every grain in a massive Harvest with just an image food grain segmentation uses computer vision to label and classify grains enhancing quality control and optimizing sorting processes labellerr tool streamlines this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate a single barley of course it presents lots of challenges due to their small size overlapping instances and subtle shape or color variations making it hard to label each grain accurately and consistently above all it is timec consuming we have a solution for this Sam a feature for automated labeling here's how it works with just one click your annotations are done using labellerrs annotation platform for food grain segmentation benefits Farmers food processors and researchers by improving sorting accuracy detecting defects and enhancing crop analysis for better yield management read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "imagine accurately distinguishing every grain in a massive Harvest with just an image food grain segmentation uses computer vision to label and classify grains enhancing quality control and optimizing sorting processes labellerr tool streamlines this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate a single barley of course it presents lots of challenges due to their small size overlapping instances and subtle shape or color variations making it hard to label each grain accurately and consistently above all it is timec consuming we have a solution for this Sam a feature for automated labeling here's how it works with just one click your annotations are done using labellerrs annotation platform for food grain segmentation benefits Farmers food processors and researchers by improving sorting accuracy detecting defects and enhancing crop analysis for better yield management read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 171,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "ALR3HYwQP2o",
    "title": "Revolutionizing Healthcare: Manual vs. Automated of Scans with Labellerr",
    "description": "Unlock the power of computer vision in the field of Healthcare with Labellerr! In this video, we showcase how our tool simplifies Image Annotation for organs and tissues.\n\nWe start with manual annotation, highlighting the time and precision challenges. Then, we introduce SAM, Labellerr's automated labeling feature, which annotates the organ with just few clicks.\n\nUsing Labellerr's annotation platform in healthcare benefits doctors, researchers, and medical device companies by training AI for faster, more accurate diagnoses.\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/faster-ultrasound-imaging-annotation/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=ALR3HYwQP2o",
    "embed_url": "https://www.youtube.com/embed/ALR3HYwQP2o",
    "duration": 71,
    "view_count": 40,
    "upload_date": "20250311",
    "uploader": "Labellerr",
    "tags": [
      "SAM annotation healthcare",
      "medical AI training",
      "diagnostic imaging AI",
      "healthcare annotation tools",
      "medical device AI",
      "image segmentation healthcare",
      "radiology AI",
      "deep learning medical imaging",
      "medical data labeling",
      "annotation for diagnosis",
      "data annotation",
      "image annotation tool",
      "deep learning",
      "data preprocessing",
      "image segmentation",
      "medical image annotation",
      "tissue annotation",
      "AI in healthcare",
      "CT scan annotation",
      "ultrasound annotation",
      "automated medical labeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "wonder how machines detect abnormalities in scans like spotting tumor growths in CT scans identifying organs in ultrasounds or mapping cells in pathology slides this is accomplished by image labeling that helps train AI to identify organs tissues and potential issues with Precision speeding up diagnosis and improving patient care labellerrs tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate the kidney manually of course this presents lots of challenges due to unclear boundaries and complex structures slowing the process and increasing errors we have a solution for this Sam a feature for automated labeling here's how it works with just two clicks your annotations are made using labellerrs annotation platform in healthcare benefits doctors researchers and medical device companies by training AI for faster more accurate diagnoses read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "wonder how machines detect abnormalities in scans like spotting tumor growths in CT scans identifying organs in ultrasounds or mapping cells in pathology slides this is accomplished by image labeling that helps train AI to identify organs tissues and potential issues with Precision speeding up diagnosis and improving patient care labellerrs tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate the kidney manually of course this presents lots of challenges due to unclear boundaries and complex structures slowing the process and increasing errors we have a solution for this Sam a feature for automated labeling here's how it works with just two clicks your annotations are made using labellerrs annotation platform in healthcare benefits doctors researchers and medical device companies by training AI for faster more accurate diagnoses read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 170,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "dfSI8zZjaoM",
    "title": "Revolutionizing Sports Annotation: Manual vs. Automated Player Segmentation with Labellerr",
    "description": "Unlock the power of computer vision for sports analytics with Labellerr! In this video, we showcase how our tool simplifies player and object segmentation — even in fast-paced games like basketball or soccer.\n\nWe start with manual annotation, highlighting the time and precision challenges. Then, we introduce SAM , Labellerr's automated labeling feature, which segments players in just a few clicks and labels a ball with a single click.\n\nWith Labellerr, companies and researchers can create high-quality training datasets for improved performance analysis, strategic insights, and game optimization.\n\nChapters\n0:00 Introduction to Computer Vision in Sports\n0:09 Overview of Labellerr's Annotation Tool\n0:20 Manual Annotation Challenges in Sports\n0:40 Using Labellerr's Magic Editor for Refinement\n0:47 Automated Labeling with SAM Feature\n0:58 Creating Training Datasets for Sports Analytics\n1:07 Strategic Insights for Teams and Coaches\n1:12 Learn More and Book a Demo\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/transforming-sports-analysis-with-player-segmentation-using-labellerr/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=dfSI8zZjaoM",
    "embed_url": "https://www.youtube.com/embed/dfSI8zZjaoM",
    "duration": 80,
    "view_count": 13,
    "upload_date": "20250311",
    "uploader": "Labellerr",
    "tags": [
      "basketball player tracking",
      "soccer player segmentation",
      "AI sports analytics",
      "automated player labeling",
      "SAM annotation sports",
      "player pose estimation",
      "object detection sports",
      "sports strategy AI",
      "video annotation sports",
      "ball detection",
      "sports dataset creation",
      "athlete identification",
      "player movement analysis",
      "data annotation",
      "image annotation tool",
      "deep learning",
      "data preprocessing",
      "image segmentation",
      "player segmentation",
      "sports image annotation",
      "computer vision sports"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "with computer vision technology particularly in sports like soccer or basketball the possibilities for in-depth player segmentation have increased exponentially labellerr tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate this Lakers versus Miami picture manually the task of player segmentation in the sports industry especially in Dynamic Sports comes with challenges like precision accuracy and time management clearly it's time consuming taking more than 50 clicks still the picture is not precisely labeled we'll use labellerr magic Editor to refine and improve The annotation we have a solution for this Sam a feature for automated labeling here's how it works with just a few clicks the Miami player is annotated and with one click the ball is perfectly labeled using labellerrs annotation platform companies and res Searchers can create training data sets contributing to improved Sports analytics player performance evaluation and strategic insights for teams and coaches read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "with computer vision technology particularly in sports like soccer or basketball the possibilities for in-depth player segmentation have increased exponentially labellerr tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate this Lakers versus Miami picture manually the task of player segmentation in the sports industry especially in Dynamic Sports comes with challenges like precision accuracy and time management clearly it's time consuming taking more than 50 clicks still the picture is not precisely labeled we'll use labellerr magic Editor to refine and improve The annotation we have a solution for this Sam a feature for automated labeling here's how it works with just a few clicks the Miami player is annotated and with one click the ball is perfectly labeled using labellerrs annotation platform companies and res Searchers can create training data sets contributing to improved Sports analytics player performance evaluation and strategic insights for teams and coaches read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 190,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "8T6eyuylFw0",
    "title": "How Vehicle Damage Annotation Speeds Up Insurance Claims — Manual vs Automated Labeling",
    "description": "Imagine capturing a photo of a damaged vehicle and instantly receiving an accurate assessment. With Labellerr's annotation platform, powered by advanced computer vision, car damage labeling becomes faster and more reliable — streamlining insurance claims, repair estimates, and fleet maintenance.\n\nIn this video, we demonstrate manual annotation for broken glass, showcasing the challenges of handling complex damage types. Then, we introduce Labellerr’s automated labeling feature, SAM, which reduces human effort and speeds up the process with just one click.\n\nBy using Labellerr’s platform, insurance companies, repair centers, and fleet operators can accelerate damage assessment, minimize errors, and optimize claims processing.\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/accelerate-car-damage-annotation-with-labellerr-auto-labeling/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=8T6eyuylFw0",
    "embed_url": "https://www.youtube.com/embed/8T6eyuylFw0",
    "duration": 71,
    "view_count": 46,
    "upload_date": "20250311",
    "uploader": "Labellerr",
    "tags": [
      "car damage segmentation",
      "vehicle inspection AI",
      "car damage dataset",
      "accident image analysis",
      "automotive annotation tools",
      "damage severity classification",
      "broken glass annotation",
      "deep learning car damage",
      "data annotation",
      "image annotation tool",
      "data preprocessing",
      "deep learning",
      "image segmentation",
      "car damage annotation",
      "computer vision insurance",
      "vehicle damage detection",
      "automated damage assessment",
      "SAM annotation car",
      "repair estimate AI",
      "fleet maintenance AI"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "imagine capturing a photo of a damaged vehicle and instantly receiving an accurate assessment with Advanced computer vision car damage annotation automates insurance claims speeds up repair estimates and enhances Fleet Maintenance reducing time and human error labella's tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate broken glass manually of course this presents many challenges due to the various types of damage slowing the process and increasing errors we have a solution for this Sam a feature for automated labeling here's how it is done with just one click here your annotations are made annotating broken window with one click again using labellerrs annotation platform in vehicle damage insurance companies repair centers and Fleet operators benefit from faster damage assessment reduced errors and streamlined claims proc processing read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "imagine capturing a photo of a damaged vehicle and instantly receiving an accurate assessment with Advanced computer vision car damage annotation automates insurance claims speeds up repair estimates and enhances Fleet Maintenance reducing time and human error labella's tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate broken glass manually of course this presents many challenges due to the various types of damage slowing the process and increasing errors we have a solution for this Sam a feature for automated labeling here's how it is done with just one click here your annotations are made annotating broken window with one click again using labellerrs annotation platform in vehicle damage insurance companies repair centers and Fleet operators benefit from faster damage assessment reduced errors and streamlined claims proc processing read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 170,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "wsc0vBaOyBE",
    "title": "Effortless Waste Segmentation Annotation with Labellerr",
    "description": "Discover how Labellerr makes waste segmentation faster and smarter! In this video, we walk you through both manual and automated annotation methods using our powerful SAM feature. See how manual labeling can take up to 20 seconds for a single item — while SAM identifies and outlines waste with just 2 dots in seconds.\n\nWhether you're creating training datasets for recycling or optimizing waste management workflows, Labellerr adapts to your needs with precision and speed.\n\nChapters\n0:00 Introduction to Waste Segmentation Annotation\n0:31 Introducing SAM Feature for Automated Labeling\n0:42 Manual vs. Automated Annotation Comparison\n0:47 Adjusting Annotations with SAM and Magic Editor\n0:52 Creating Training Datasets with Labellerr\n1:00 Additional Resources and Blog Link\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/navigating-waste-sorting-labellerr/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=wsc0vBaOyBE",
    "embed_url": "https://www.youtube.com/embed/wsc0vBaOyBE",
    "duration": 65,
    "view_count": 24,
    "upload_date": "20250311",
    "uploader": "Labellerr",
    "tags": [
      "computer vision waste management",
      "automated waste sorting",
      "SAM annotation waste",
      "recycling AI",
      "instance segmentation waste",
      "semantic segmentation waste",
      "waste management AI",
      "recycling dataset",
      "smart waste bins",
      "waste sorting robots",
      "deep learning waste analysis",
      "waste detection AI",
      "smart recycling solutions",
      "garbage sorting automation",
      "data annotation",
      "image annotation tool",
      "image segmentation",
      "data preprocessing",
      "deep learning",
      "waste segmentation",
      "waste annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "waste segmentation involves identifying and categorizing different types of waste such as plastic paper or metal in images to support Recycling and waste management labellerrs tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs first we'll annotate manually clearly it is timec consuming like a small cup taking around 20 seconds to get annotated imagine doing this for thousands of items timec consuming right we have a solution for this Sam a feature for automated labeling that instantly identifies an outlines waste items with just two dots it obviously saves your valuable time compared to manual methods we can also adjust our annotations in Sam manually or by using the magic editor we've covered this in our earlier videos using labellerrs annotation platform companies and researchers can create training data sets specifically tailored for Waste sorting and segregation read more about this in our blog Link in the description thank you",
    "transcript_chunks": [
      "waste segmentation involves identifying and categorizing different types of waste such as plastic paper or metal in images to support Recycling and waste management labellerrs tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs first we'll annotate manually clearly it is timec consuming like a small cup taking around 20 seconds to get annotated imagine doing this for thousands of items timec consuming right we have a solution for this Sam a feature for automated labeling that instantly identifies an outlines waste items with just two dots it obviously saves your valuable time compared to manual methods we can also adjust our annotations in Sam manually or by using the magic editor we've covered this in our earlier videos using labellerrs annotation platform companies and researchers can create training data sets specifically tailored for Waste sorting and segregation read more about this in our blog Link in the description thank you"
    ],
    "transcript_word_count": 160,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "gBAtATndxpw",
    "title": "Revolutionizing Environmental Monitoring: Manual vs. Automated Land Cover Segmentation",
    "description": "Unlock the power of computer vision for environmental monitoring with Labellerr! In this video, we showcase how our tool simplifies Land Cover Segmentation — even in diverse conditions.\n\nWe start with manual annotation, highlighting the time and precision challenges. Then, we introduce SAM , Labellerr's automated labeling feature, which segments the land area within few clicks.\n\nUsing Labellerr's annotation platform, urban planners, farmers, environmentalists, disaster response teams, and researchers can benefit from land cover segmentation.\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/land-cover-classification-with-labellerr/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=gBAtATndxpw",
    "embed_url": "https://www.youtube.com/embed/gBAtATndxpw",
    "duration": 80,
    "view_count": 29,
    "upload_date": "20250311",
    "uploader": "Labellerr",
    "tags": [
      "agricultural land mapping",
      "forest cover detection",
      "glacier segmentation",
      "multi-class segmentation",
      "pixel-level classification",
      "environmental monitoring AI",
      "urban planning AI",
      "hyperspectral image segmentation",
      "bareland segmentation",
      "remote sensing deep learning",
      "post-processing segmentation",
      "pixel-wise classification",
      "data annotation",
      "image annotation tool",
      "image segmentation",
      "data preprocessing",
      "deep learning",
      "satellite image segmentation",
      "remote sensing annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "wondered how researchers come to know about shifting land patterns Glacier melt or desertification over time or how land patterns are recognized by investors or real estate this is accomplished by segmentation of satellite images into multiple classes such as urban areas agriculture forests water bodies and more labellerr tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate the land area manually of course it presents lots of challenges due to similar spectral signatures spatial variations class imbalances and temporal changes and yes above all it is timec consuming we have a solution for this Sam a feature for automated labeling here's how it works with just a few clicks your annotations are made and you can even adjust them manually afterward using labellerrs anation platform urban planners Farmers environmentalists Disaster Response teams and researchers can benefit from land cover segmentation read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "wondered how researchers come to know about shifting land patterns Glacier melt or desertification over time or how land patterns are recognized by investors or real estate this is accomplished by segmentation of satellite images into multiple classes such as urban areas agriculture forests water bodies and more labellerr tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate the land area manually of course it presents lots of challenges due to similar spectral signatures spatial variations class imbalances and temporal changes and yes above all it is timec consuming we have a solution for this Sam a feature for automated labeling here's how it works with just a few clicks your annotations are made and you can even adjust them manually afterward using labellerrs anation platform urban planners Farmers environmentalists Disaster Response teams and researchers can benefit from land cover segmentation read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 183,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "OkeYnngGLmY",
    "title": "How Self-Driving Cars Use Road Annotations — Manual vs Automated Labeling with Labellerr",
    "description": "Ever wondered how self-driving cars adjust to road lanes without human intervention? They rely on computer vision algorithms to detect lane markings from camera footage — but labeling this data can be time-consuming and complex.\n\nIn this video, we explore the challenges of manual annotation (precision, accuracy, and time management).\n\nHow Labellerr’s SAM 2 feature automates labeling with a single click — 11 annotations in just 11 seconds!\n\nHow automated annotation boosts capabilities in vehicle navigation, lane-keeping assistance, and traffic management for safer roads and enhanced mobility\n\nChapters\n0:00 Introduction to Self-Driving Car Lane Detection\n0:08 Computer Vision in Autonomous Vehicles\n0:17 Labellerr’s Annotation Tool Overview\n0:27 Challenges of Manual Annotation\n0:40 Automated Labeling with SAM 2 Feature\n0:53 Benefits for Vehicle Navigation and Safety\n1:06 Additional Resources and Demo Booking\n\nInterested in learning more?\nhttps://www.labellerr.com/blog/lane-detection-using-labellerr/\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=OkeYnngGLmY",
    "embed_url": "https://www.youtube.com/embed/OkeYnngGLmY",
    "duration": 75,
    "view_count": 111,
    "upload_date": "20250310",
    "uploader": "Labellerr",
    "tags": [
      "vehicle navigation AI",
      "traffic management AI",
      "real-time lane detection",
      "lane boundary detection",
      "deep learning lane detection",
      "lane detection dataset",
      "real-time lane tracking",
      "robust lane detection",
      "road marking recognition",
      "data annotation",
      "image annotation tool",
      "deep learning",
      "data preprocessing",
      "image segmentation",
      "lane detection",
      "lane marking annotation",
      "self-driving car AI",
      "autonomous vehicle computer vision",
      "road lane segmentation",
      "OpenCV lane detection"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "wondered how self-driving car make adjustments according to Road Lanes I mean how can they identify the markings without any human intervention use computer vision algorithms to analyze visual data captured by cameras mounted on them and identify Lane markings on the road surface label's tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate the markings manually of course it presents lots of challenges es like Precision accuracy and time management clearly it is timec consuming and takes around 8 to 10 clicks for just one marking we have a solution for this Sam a feature for automated labeling here's how it is done with just one click the marking is labeled automatically see 11 annotations with one click within 11 seconds using labellerrs annotation platform organizations can unlock new capabilities in vehicle navigation Lane keeping assistance and traffic management ultimately contributing to safer roads and enhanced Mobility for all read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description",
    "transcript_chunks": [
      "wondered how self-driving car make adjustments according to Road Lanes I mean how can they identify the markings without any human intervention use computer vision algorithms to analyze visual data captured by cameras mounted on them and identify Lane markings on the road surface label's tool is designed to streamline this process offering both automated and manual labeling methods to meet diverse needs firstly we'll annotate the markings manually of course it presents lots of challenges es like Precision accuracy and time management clearly it is timec consuming and takes around 8 to 10 clicks for just one marking we have a solution for this Sam a feature for automated labeling here's how it is done with just one click the marking is labeled automatically see 11 annotations with one click within 11 seconds using labellerrs annotation platform organizations can unlock new capabilities in vehicle navigation Lane keeping assistance and traffic management ultimately contributing to safer roads and enhanced Mobility for all read more about this in our blog Link in the description interested to know more about annotations book a demo with us reach out through the link in the description"
    ],
    "transcript_word_count": 192,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "47WGJJu_XQU",
    "title": "Master Event Tagging with Labellerr: Effortlessly Organize & Analyze Video Content",
    "description": "In today’s fast-paced world, we crave quick, impactful moments — whether it's a game-winning dunk or a critical security incident. But how do you find those moments in hours of footage?\n\nIn this video, we break down the power of event tagging and show you how Labellerr’s tool makes it simple to pinpoint, organize, and leverage key moments in videos. From tagging Goran Dragic’s clutch 2-pointer to mapping out the NBA Finals’ hottest plays, you’ll see how event tagging turns raw footage into structured data.\n\nWhy does this matter?\nEvery tagged event contributes to creating datasets that train machine learning models, helping businesses recognize patterns, predict outcomes, and automate video analysis. Whether you’re a content creator, security expert, or AI innovator — event tagging can revolutionize your workflow.\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nInterested in learning more?\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms: \nLinkedIn: https://www.linkedin.com/company/labellerr\n\nTwitter: https://www.x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=47WGJJu_XQU",
    "embed_url": "https://www.youtube.com/embed/47WGJJu_XQU",
    "duration": 141,
    "view_count": 25,
    "upload_date": "20250310",
    "uploader": "Labellerr",
    "tags": [
      "video timeline tagging",
      "security footage tagging",
      "educational video tagging",
      "content curation AI",
      "Labellerr annotation tool",
      "sports video segmentation",
      "video metadata creation",
      "video summary automation",
      "video labeling software",
      "data annotation",
      "image annotation tool",
      "deep learning",
      "data preprocessing",
      "image segmentation",
      "event tagging",
      "video annotation",
      "sports highlights tagging",
      "AI video analysis",
      "automated event detection",
      "machine learning dataset",
      "event detection AI"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "in today's world everyone loves short form content whether it's Instagram reels YouTube shorts or skipping a full soccer match or let's say an NBA game for the highlights we just don't have the patience for the whole game anymore we want the dunks the buzzer beers the big steals served up fast that's where event tagging steps in pinpointing those can't miss Clips with Precision let's dive in here I've uploaded this clip of a classic Heat versus Lakers second quarter of game 5 in the NBA Finals oh that's gor drags two-pointer let's tag this in the following manner simple keep the timeline head at the starting point of the event and then click on the event now imagine a fan searching for it in approximately 50 minutes of play but with this no more manual searching just precise organized and accessible content ready when you need it now here's a foul on LeBron James we all know him right let's tag this passage of event to firstly add the name of the event from here I'll quickly add it with foul on LeBron as the name just repeat the same steps keep the timeline head at the correct Tim stamp and you're good to go we can also adjust the Tag's length and move it around as [Music] needed see in seconds I've mapped out the game's hottest plays but but it's not just about highlights it's about turning raw footage into structured data that powers Innovation every tagged Moment Like a dunk a foul or a steel creates a data set that trains machine learning models by precisely annotating key moments businesses can train models to recognize patterns predict outcomes or automate analysis and it's not just for sport fans whether you're a security expert who wants to tag suspicious activities or specific incidents to streamline footage review or online educator willing to highlight critical Concepts practical demos or chapter breaks event tagging has its application in all of them plus many more and labellerr makes event tagging simple for you book a demo with lailler to explore more about annotations link is provided in the description thank you",
    "transcript_chunks": [
      "in today's world everyone loves short form content whether it's Instagram reels YouTube shorts or skipping a full soccer match or let's say an NBA game for the highlights we just don't have the patience for the whole game anymore we want the dunks the buzzer beers the big steals served up fast that's where event tagging steps in pinpointing those can't miss Clips with Precision let's dive in here I've uploaded this clip of a classic Heat versus Lakers second quarter of game 5 in the NBA Finals oh that's gor drags two-pointer let's tag this in the following manner simple keep the timeline head at the starting point of the event and then click on the event now imagine a fan searching for it in approximately 50 minutes of play but with this no more manual searching just precise organized and accessible content ready when you need it now here's a foul on LeBron James we all know him right let's tag this passage of event to firstly add the name of the event from here I'll quickly add it with foul on LeBron as the name just repeat the same steps keep the timeline head at the correct Tim stamp and you're good to go we can also adjust the Tag's length and move it around as [Music] needed see in seconds I've mapped out the game's hottest plays but but it's not just about highlights it's about turning raw footage into structured data that powers Innovation every tagged Moment Like a dunk a foul or a steel creates a data set that trains machine learning models by precisely annotating key moments businesses can train models to recognize patterns predict outcomes or automate analysis and it's not just for sport fans whether you're a security expert who",
      "wants to tag suspicious activities or specific incidents to streamline footage review or online educator willing to highlight critical Concepts practical demos or chapter breaks event tagging has its application in all of them plus many more and labellerr makes event tagging simple for you book a demo with lailler to explore more about annotations link is provided in the description thank you"
    ],
    "transcript_word_count": 363,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "co70NzWpr74",
    "title": "Faster precised Image Annotation using Labellerr's Magic Editor",
    "description": "Struggling with sharp edges or missing labels in your image annotations? In this quick demo, discover how Labellerr’s Magic Editor perfects your AI training data effortlessly. Whether you’re annotating people, objects, or reviewing SAM-labeled images (our one-dot labeling feature), Magic Editor fixes irregularities without disrupting your work. Learn to smooth boundaries, merge unlabeled areas, and refine pixels with simple shortcuts—ideal for retail, medical imaging, and more\n\nBook a Demo: https://www.labellerr.com/book-a-demo  \n\nInterested in learning more?\nWebsite: https://www.labellerr.com/\n\nFind us on Social Media Platforms: \nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=co70NzWpr74",
    "embed_url": "https://www.youtube.com/embed/co70NzWpr74",
    "duration": 98,
    "view_count": 83,
    "upload_date": "20250228",
    "uploader": "Labellerr",
    "tags": [
      "pixel-level annotation",
      "annotation correction",
      "annotation tools",
      "computer vision annotation",
      "SAM annotation",
      "annotation workflow",
      "annotation for AI",
      "data labeling",
      "annotation review",
      "bounding box annotation",
      "polygon annotation",
      "tracking annotation",
      "annotation for retail",
      "annotation data standards",
      "image annotation tool",
      "data preprocessing",
      "deep learning",
      "image segmentation",
      "image annotation",
      "annotation refinement",
      "AI training data",
      "object segmentation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hey everyone imagine you're training an AI to detect objects like a person in an image or you are reviewing any annotated image you've got annotations but look sharp edges and missing labels around the hair and clothes these errors can lower your model's accuracy that's where labellerr magic editor comes in as you can see I have manually annotated the image and irregularities can be seen throughout to fix this one way is by dragging key points like this but this could disturb the overall shape of your annotation but yes this can be a good option for Sharp edges and to make them curvy wherever required let's get to our main feature for such problems that is Magic editor now as we have already covered on our YouTube Sam feature labels images with just one dot but if it misses pixels reviewers can refine it in seconds now let's merge this part to our object man just press the starting key Point holding shift and cover the unlabeled area by simply right clicking and at last press the ending key Point by holding shift also to have continuous key points just hold control key and guide your pointer on the desired boundary that's it easy right correct any imperfection from retail product outlines to Medical scans with Precision using labellerrs magic editor wanting to learn more about image annotation we''ve already cover covered a demo on our Channel showcasing other annotation types too the link is provided in the description thank you",
    "transcript_chunks": [
      "hey everyone imagine you're training an AI to detect objects like a person in an image or you are reviewing any annotated image you've got annotations but look sharp edges and missing labels around the hair and clothes these errors can lower your model's accuracy that's where labellerr magic editor comes in as you can see I have manually annotated the image and irregularities can be seen throughout to fix this one way is by dragging key points like this but this could disturb the overall shape of your annotation but yes this can be a good option for Sharp edges and to make them curvy wherever required let's get to our main feature for such problems that is Magic editor now as we have already covered on our YouTube Sam feature labels images with just one dot but if it misses pixels reviewers can refine it in seconds now let's merge this part to our object man just press the starting key Point holding shift and cover the unlabeled area by simply right clicking and at last press the ending key Point by holding shift also to have continuous key points just hold control key and guide your pointer on the desired boundary that's it easy right correct any imperfection from retail product outlines to Medical scans with Precision using labellerrs magic editor wanting to learn more about image annotation we''ve already cover covered a demo on our Channel showcasing other annotation types too the link is provided in the description thank you"
    ],
    "transcript_word_count": 254,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "ixbl7lT_Jo4",
    "title": "What Are AI Agents? A 6-Minute Guide to Their Power and Potential.",
    "description": "Discover the world of AI agents in this 6-minute guide from Labellerr. Learn what AI agents are, why they matter, and how they’re transforming industries like healthcare, finance, and e-commerce. Explore their types, core technologies, and how they work, with real-world examples like smart thermostats and travel assistants. Whether you’re building a final year project or automating your ML pipeline, this video offers insights to stay ahead. Book a demo with Labellerr to see how AI agents can power your next project. Subscribe for more tech insights!\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nInterested in learning more about - What Are AI Agents?\nWebsite: https://www.labellerr.com/blog/what-are-ai-agents-a-comprehensive-guide/\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=ixbl7lT_Jo4",
    "embed_url": "https://www.youtube.com/embed/ixbl7lT_Jo4",
    "duration": 415,
    "view_count": 319,
    "upload_date": "20250228",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hey everyone AI agents are making waves in the headlines today we're exploring what they are why they are a big deal we'll uncover their types and at last we'll see the tech powering them experts call them the next big thing from Healthcare to finance and Beyond so whether you're a student racing to finish your final year project or want to automate your ml pipeline this six-minute guide has something for you ready let's jump in What are AI agents so to understand a agents I want you to first know what they're not let's talk first about AI automation we all know what it is right so can it be called an AI agent if not so what's the distinguishing factor to figure this out let's take an example imagine You've Got A system that sends you a daily reminder email at 8:00 a.m. pretty handy right but is it an AI agent nope that system is just following orders it's like a system or software that does exactly what you tell it to no thinking no deciding just executing the set of tasks if you told it to send that email every day it'd keep going even if the world's on fire an AI agent though it' pause and say hm maybe today is not the day for this the big difference is decision-making automation lacks it but agents thrive on it so that's just a quick peek at how AI agents differ from automation now the Technologies went from automation to AI models that could think and generate text here's the thing AI models like llms are the brain or basically giant sets of numbers when you give them input like a question they go through some fancy math and other operations and spit out a response that's it they can't do anything beyond generating responses so if you're worried about an AI model taking over the world relax it's not pressing any buttons or launching anything it's just handing you a pile of numbers turned into responses AI agents though they're the real movers they take that AI model's brain power like llm and pair it with traditional software to interact with the world they can call apis tap into tools and make things happen for example say you ask your agent to schedule a meeting the AI model figures out what you mean and the agent uses an API to book it on your calendar models can only think and predict AI automated devices only act but agents think and then act one of the defining features of an AI agent is its ability to interact with the real world unlike Standalone models that rely solely on pre-existing data sets AI agents have internet access this enables them to stay updated and respond to new information as it happens we'll get to know more about the Technologies used in an AI agent later why should you know about AI agents it is not because they're a trend they're the future and it's coming fast experts predict the AI agent Market will soar from $5.1 billion in 2024 to a staggering 47.1 billion by 2030 that's almost a tenfold leap in just 6 years in finance AI agents spot fraudulent transactions in real time saving companies billions and protecting your money in healthcare they're teaming up with doctors analyzing complex scans like MRIs or x-rays faster and with pinpoint accuracy sometimes even catching things the human ey might miss in e-commerce they're the masterminds behind the recommendations learning your taste predicting your needs and making shopping feel personal in ways we've never seen before efficiency accuracy and speed that's the unbeatable promise of AI agents learning about them isn't just a good idea it's your chance to understand the tech that's reshaping how we work live and connect in a world where AI is accelerating every day knowing how these agents tick could be the edge you need to thrive Bill Gates nailed it when he said the advancement of technology is based on making it fit in so that you don't really even notice it so it's part of everyday life AI agents are becoming that invisible indispensable Force let us now know about its types there are different types of AI agents simple reflex agents react to inputs like a thermostat adjusting the AC model-based agents track patterns like self-driving cars reading traffic goal-based agents aim for outcomes like a delivery robot that plans the best route to deliver a package it considers it its current position obstacles and available paths to choose the quickest way utility based agents weigh options for the best result like trading Bots maximizing profits hierarchical agents for example AI systems in smart factories manage multiple production lines each with a different automation level and learning agents they get smarter over time adapting to new data like recommendation algorithms tailoring playlists to your taste done with the types now we'll see the Technologies behind a general AI agent imagine you've got an AI travel assistant that reschedules your flights finds car rentals and snags discounts all on its own how does it do that let's break down the Technologies behind this first up large language models or llms these are the brains that let your AI understand your request like change my flight timings to later and reply with humanlike answers trained on massive data sets llms power chat Bots and more making your assistant sharp and talkative next memory systems short-term memory tracks what you just said like remembering you want a later flight not an earlier one while long-term memory recalls your past trips knowing you prefer evening flights that's how it personalizes your travel then there's planning and reasoning modules these help your AI Scout new flight options check car rental needs and pick the best deals within your schedule it's like a mini strategist figuring things out just like self-driving driving cars plan routes or doctors choose treatments now action execution and Tool integration this is where your AI takes charge booking that later flight reserving a car and texting you a discount code it links to airline apps rental sites and databases to make it all happen smoothly finally learning module through reinforcement learning your AI improves by trial and error learning you love cheap rentals human feedback like I want Budget Cars refines it further and as travel Trends shift it adapts spotting deals or quieter flight times to summarize AI agents are smart systems that sense analyze and act from chatbots to self-driving cars they are reshaping our world and the future it's full of possibilities maybe one day you'll see a startup team of AI agents book a demo with labellerr to explore more about how AI agents can power your projects thank you",
    "transcript_chunks": [
      "hey everyone AI agents are making waves in the headlines today we're exploring what they are why they are a big deal we'll uncover their types and at last we'll see the tech powering them experts call them the next big thing from Healthcare to finance and Beyond so whether you're a student racing to finish your final year project or want to automate your ml pipeline this six-minute guide has something for you ready let's jump in What are AI agents so to understand a agents I want you to first know what they're not let's talk first about AI automation we all know what it is right so can it be called an AI agent if not so what's the distinguishing factor to figure this out let's take an example imagine You've Got A system that sends you a daily reminder email at 8:00 a.m. pretty handy right but is it an AI agent nope that system is just following orders it's like a system or software that does exactly what you tell it to no thinking no deciding just executing the set of tasks if you told it to send that email every day it'd keep going even if the world's on fire an AI agent though it' pause and say hm maybe today is not the day for this the big difference is decision-making automation lacks it but agents thrive on it so that's just a quick peek at how AI agents differ from automation now the Technologies went from automation to AI models that could think and generate text here's the thing AI models like llms are the brain or basically giant sets of numbers when you give them input like a question they go through some fancy math and other operations and spit out",
      "a response that's it they can't do anything beyond generating responses so if you're worried about an AI model taking over the world relax it's not pressing any buttons or launching anything it's just handing you a pile of numbers turned into responses AI agents though they're the real movers they take that AI model's brain power like llm and pair it with traditional software to interact with the world they can call apis tap into tools and make things happen for example say you ask your agent to schedule a meeting the AI model figures out what you mean and the agent uses an API to book it on your calendar models can only think and predict AI automated devices only act but agents think and then act one of the defining features of an AI agent is its ability to interact with the real world unlike Standalone models that rely solely on pre-existing data sets AI agents have internet access this enables them to stay updated and respond to new information as it happens we'll get to know more about the Technologies used in an AI agent later why should you know about AI agents it is not because they're a trend they're the future and it's coming fast experts predict the AI agent Market will soar from $5.1 billion in 2024 to a staggering 47.1 billion by 2030 that's almost a tenfold leap in just 6 years in finance AI agents spot fraudulent transactions in real time saving companies billions and protecting your money in healthcare they're teaming up with doctors analyzing complex scans like MRIs or x-rays faster and with pinpoint accuracy sometimes even catching things the human ey might miss in e-commerce they're the masterminds behind the recommendations learning your taste predicting your needs and",
      "making shopping feel personal in ways we've never seen before efficiency accuracy and speed that's the unbeatable promise of AI agents learning about them isn't just a good idea it's your chance to understand the tech that's reshaping how we work live and connect in a world where AI is accelerating every day knowing how these agents tick could be the edge you need to thrive Bill Gates nailed it when he said the advancement of technology is based on making it fit in so that you don't really even notice it so it's part of everyday life AI agents are becoming that invisible indispensable Force let us now know about its types there are different types of AI agents simple reflex agents react to inputs like a thermostat adjusting the AC model-based agents track patterns like self-driving cars reading traffic goal-based agents aim for outcomes like a delivery robot that plans the best route to deliver a package it considers it its current position obstacles and available paths to choose the quickest way utility based agents weigh options for the best result like trading Bots maximizing profits hierarchical agents for example AI systems in smart factories manage multiple production lines each with a different automation level and learning agents they get smarter over time adapting to new data like recommendation algorithms tailoring playlists to your taste done with the types now we'll see the Technologies behind a general AI agent imagine you've got an AI travel assistant that reschedules your flights finds car rentals and snags discounts all on its own how does it do that let's break down the Technologies behind this first up large language models or llms these are the brains that let your AI understand your request like change my flight timings to later and",
      "reply with humanlike answers trained on massive data sets llms power chat Bots and more making your assistant sharp and talkative next memory systems short-term memory tracks what you just said like remembering you want a later flight not an earlier one while long-term memory recalls your past trips knowing you prefer evening flights that's how it personalizes your travel then there's planning and reasoning modules these help your AI Scout new flight options check car rental needs and pick the best deals within your schedule it's like a mini strategist figuring things out just like self-driving driving cars plan routes or doctors choose treatments now action execution and Tool integration this is where your AI takes charge booking that later flight reserving a car and texting you a discount code it links to airline apps rental sites and databases to make it all happen smoothly finally learning module through reinforcement learning your AI improves by trial and error learning you love cheap rentals human feedback like I want Budget Cars refines it further and as travel Trends shift it adapts spotting deals or quieter flight times to summarize AI agents are smart systems that sense analyze and act from chatbots to self-driving cars they are reshaping our world and the future it's full of possibilities maybe one day you'll see a startup team of AI agents book a demo with labellerr to explore more about how AI agents can power your projects thank you"
    ],
    "transcript_word_count": 1145,
    "transcript_chunk_count": 4
  },
  {
    "video_id": "rpmKDXfn6dg",
    "title": "Get Annotations 10x Faster for Name Entity Recognition, Classification, Text Summarization & More",
    "description": "Save time for your text annotation project with Labellerr! In this tutorial, we walk you through the essentials of annotating text data, from named entity recognition (NER) to sentiment analysis, key phrase identification, attributes, and classification. You’ll also get to know how to merge GenAI in this process.\nWhether you're training AI models, analyzing large datasets, or categorizing content, this guide will help you understand the importance of structured text data for machine learning and natural language processing (NLP) applications.\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nInterested in learning more about our text annotation solution?\nWebsite: https://www.labellerr.com/text-annotation-platform\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=rpmKDXfn6dg",
    "embed_url": "https://www.youtube.com/embed/rpmKDXfn6dg",
    "duration": 379,
    "view_count": 156,
    "upload_date": "20250224",
    "uploader": "Labellerr",
    "tags": [
      "NLP annotation",
      "natural language processing",
      "object detection",
      "image segmentation",
      "cvat setup",
      "ner models",
      "GenAI annotation",
      "image labeling",
      "annotation strategies",
      "cvat features",
      "image classification",
      "computer vision",
      "image annotation",
      "labeling software",
      "data annotation",
      "image annotation tool",
      "machine learning",
      "cvat tool",
      "text annotation",
      "NER annotation",
      "annotation software",
      "deep learning",
      "dataset creation",
      "cvat tutorial",
      "annotation methods",
      "cvat guide"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "welcome to the text annotation tutorial while I create objects you'll understand why and how is this crucial text annotation helps AI identify key entities relationships and Concepts in text making it easier to analyze and understand large data sets this is useful where quick insights can save time and improve decision-making today we'll work with an excerpt from fictional story set in neotokyo where a young inventor named Aiko Nakamura unveils her latest creation a self aware AI called elesium even this short intro that I just said can be generated because of text annotation we'll explore how labellerr makes this process intuitive efficient and scalable let's dive in annotating named entities first we'll annotate named entities these are specific pieces of information like names locations and AI system for example let's label neotokyo as a location simply select location from object section and then choose the corresponding text next we'll label persons Aiko Nakamura and Dr Hiroshi Tanaka and alium as an AI system these annotations help AI models recognize and categorize similar entities in other texts which is essential for tasks like information extraction and Knowledge Graph creation we'll see how to add more details to these objects later on in this video side by side you may review your annotation by simply clicking on the labeled text total number of annotations can be viewed here along with their starting and ending index annotating sentiments next let's annotate sentiments for instance the phrase ieko remained optimistic clearly conveys a positive sentiment we'll select positive and label optimistic on the other hand Dr Tanaka's warning about existential risks carries a negative sentiment will label existential risks as negative by annotating sentiments we can train models to automatically detect emotions in customer feedback social media posts and more annotating key phrases finally let's annotate phrases for theme phrases like self-aware Ai and complex Global challenges capture the core ideas of the text we'll label these as key phrase similarly human machine collaboration is also an important phrase in our story annotating phrases helps in organizing and categorizing large data sets making it easier to analyze Trends and patterns adding attributes attributes and classification add depth and structure to your annotations let's assign attributes to object person here's how we'll create gender as an attribute here you'll see multiple options in this dropdown I'll come to captioning later for now I'll select radio and add options to it similarly adding an attribute to AI system which highlights its status if you want to get input from user you may go with input attribute type I'll be choosing radio again now it'll quickly add city and state options for locations attribute named type and relevance for key phrases you may add multiple attributes to single object like role for person and type for AI system Etc now here's how you can select attributes let's set the relevance of this key phrase as high similarly we'll select attributes for other objects too now that I have added attributes to all the objects let's review them you'll be seeing neotokyo as City here Aiko Nakamura as female and the doctor as male key phrases attributed high low medium on the basis of their relevance classification next let's classify the text I'll add classification in the form of theme and select captioning now this is an interesting feature we'll give a prompt like this classification allows us to assign broader categories or labels to the entire text and here we get our theme that's what I was talking about in the introduction part of the video you may edit it the way you want you may use it for translation summary factchecking and question answering Etc now some other features you can also add an attribute at any moment of your annotation let's add adjectives as an attribute adjectives help in understanding the emotional and descriptive tone of the text making the text more machine readable for Downstream tasks like summarization or sentiment analysis after annotating bustling and young we see self-aware as an adjective but the problem is it is labeled as a key phrase already so the question is can we annotate a single word multiple times so yes it is possible in our tool we'll label other adjectives quickly too reviewing annotations now that we've annotated the text let's review our work simply clicking on the label will show you its type above clicking the label would also highlight the object here where you could see the starting and ending index it can be used to easily search and retrieve specific annotations in large data sets also you can delete the object or edit the attribute from here if you'd like to dive deeper into reviewing annotations we already have a detailed video on our Channel you'll learn how to review large length of textual annotated data there link is provided in the description and that's it thank you for watching book a demo with labellerr now to explore more about different annotation types link is in the description",
    "transcript_chunks": [
      "welcome to the text annotation tutorial while I create objects you'll understand why and how is this crucial text annotation helps AI identify key entities relationships and Concepts in text making it easier to analyze and understand large data sets this is useful where quick insights can save time and improve decision-making today we'll work with an excerpt from fictional story set in neotokyo where a young inventor named Aiko Nakamura unveils her latest creation a self aware AI called elesium even this short intro that I just said can be generated because of text annotation we'll explore how labellerr makes this process intuitive efficient and scalable let's dive in annotating named entities first we'll annotate named entities these are specific pieces of information like names locations and AI system for example let's label neotokyo as a location simply select location from object section and then choose the corresponding text next we'll label persons Aiko Nakamura and Dr Hiroshi Tanaka and alium as an AI system these annotations help AI models recognize and categorize similar entities in other texts which is essential for tasks like information extraction and Knowledge Graph creation we'll see how to add more details to these objects later on in this video side by side you may review your annotation by simply clicking on the labeled text total number of annotations can be viewed here along with their starting and ending index annotating sentiments next let's annotate sentiments for instance the phrase ieko remained optimistic clearly conveys a positive sentiment we'll select positive and label optimistic on the other hand Dr Tanaka's warning about existential risks carries a negative sentiment will label existential risks as negative by annotating sentiments we can train models to automatically detect emotions in customer feedback social media posts and more annotating",
      "key phrases finally let's annotate phrases for theme phrases like self-aware Ai and complex Global challenges capture the core ideas of the text we'll label these as key phrase similarly human machine collaboration is also an important phrase in our story annotating phrases helps in organizing and categorizing large data sets making it easier to analyze Trends and patterns adding attributes attributes and classification add depth and structure to your annotations let's assign attributes to object person here's how we'll create gender as an attribute here you'll see multiple options in this dropdown I'll come to captioning later for now I'll select radio and add options to it similarly adding an attribute to AI system which highlights its status if you want to get input from user you may go with input attribute type I'll be choosing radio again now it'll quickly add city and state options for locations attribute named type and relevance for key phrases you may add multiple attributes to single object like role for person and type for AI system Etc now here's how you can select attributes let's set the relevance of this key phrase as high similarly we'll select attributes for other objects too now that I have added attributes to all the objects let's review them you'll be seeing neotokyo as City here Aiko Nakamura as female and the doctor as male key phrases attributed high low medium on the basis of their relevance classification next let's classify the text I'll add classification in the form of theme and select captioning now this is an interesting feature we'll give a prompt like this classification allows us to assign broader categories or labels to the entire text and here we get our theme that's what I was talking about in the introduction part of",
      "the video you may edit it the way you want you may use it for translation summary factchecking and question answering Etc now some other features you can also add an attribute at any moment of your annotation let's add adjectives as an attribute adjectives help in understanding the emotional and descriptive tone of the text making the text more machine readable for Downstream tasks like summarization or sentiment analysis after annotating bustling and young we see self-aware as an adjective but the problem is it is labeled as a key phrase already so the question is can we annotate a single word multiple times so yes it is possible in our tool we'll label other adjectives quickly too reviewing annotations now that we've annotated the text let's review our work simply clicking on the label will show you its type above clicking the label would also highlight the object here where you could see the starting and ending index it can be used to easily search and retrieve specific annotations in large data sets also you can delete the object or edit the attribute from here if you'd like to dive deeper into reviewing annotations we already have a detailed video on our Channel you'll learn how to review large length of textual annotated data there link is provided in the description and that's it thank you for watching book a demo with labellerr now to explore more about different annotation types link is in the description"
    ],
    "transcript_word_count": 848,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "3wyvMvg1u1g",
    "title": "Label 10x Faster: All-in-One Image Annotation Tool for Agriculture, Robotics & Surveillance",
    "description": "In this tutorial, we walk through the process of image annotation using Labellerr, covering different techniques to label objects accurately for AI training. \nLearn how bounding boxes, polygon annotation, and keypoints help create precise datasets for model training.\n\nThis video also explores advanced features such as SAM for efficient automated polygon annotation, the grouping tool for entity-based labeling, and attributes/classifications for better dataset organization.\n\nChapters\n0:00 Introduction to Image Annotation\n0:39 Overview of Labellerr Tool\n1:07 Bounding Box Annotation\n1:45 Polygon Annotation\n3:17 SAM for Automated Polygon Annotation\n4:04 Keypoint Annotation\n5:37 Attributes and Classifications\n7:20 Grouping Tool Feature\n9:39 Filtering Annotations\n10:01 Conclusion and Demo Booking\n\nAre you're looking to streamline your annotation workflow and improve dataset quality?\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nInterested in learning more? \nWebsite: https://www.labellerr.com  \n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr \nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=3wyvMvg1u1g",
    "embed_url": "https://www.youtube.com/embed/3wyvMvg1u1g",
    "duration": 605,
    "view_count": 367,
    "upload_date": "20250218",
    "uploader": "Labellerr",
    "tags": [
      "diet tracking AI",
      "annotation attributes",
      "SAM annotation",
      "automated image labeling",
      "annotation workflow",
      "image classification",
      "dataset preparation",
      "instance segmentation",
      "annotation tutorial",
      "Labellerr annotation tool",
      "data annotation",
      "machine learning",
      "cvat tool",
      "annotation software",
      "image annotation",
      "bounding box annotation",
      "polygon annotation",
      "keypoint annotation",
      "object labeling",
      "AI training data",
      "computer vision annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hello everyone and welcome to today's tutorial on image annotation in this session we'll be working with this kitchen scene featuring strawberries a knife yogurt honey apple and granola while I am adding objects and their annotation type I'll also explain why annotating these objects is crucial accurate annotations help AI models recognize everyday objects imagine using this for a smart grocery checkout system that identifies fruits automatically or maybe for a cooking assistant robot that picks out ingredients it could even assist diet tracking apps by detecting what you're eating cool right now let's see how to actually label these objects using labellerrs tool you see multiple objects here you may switch between them using numbers or by directly right-clicking the pan tool allows us to adjust the image the quality of the image can also be enhanced from this option we'll talk about transparency later and these these two features can be used to adjust the dimensions of our image we'll get to the grouping tool feature as well in the latter part of the video first up bounding box annotation this is the simplest method where we draw rectangles around objects I'll start by drawing a BB box around the knife here then the bowl and lastly the strawberries apple and granola bounding boxes are perfect for Clear simple shapes and are commonly used in object detection tasks but as you'll notice there they're not super precise for objects with irregular shapes like the strawberries just view all your annotation from here the 10 here highlights total number of annotations now coming to polygon annotation polygon annotation lets us outline objects more precisely let's annotate honey first now how to change The annotation type of the strawberry so that it can be more precisely labeled let's see the procedure to change The annotation type of the strawberry changing it to polygon just clear annotations that are not required you may use polygon annotation by right clicking at boundary points or just holding shift and dragging the pointer along the boundary to complete press n see how much cleaner that looks now annotating each of the five strawberries may require much more manual work for this labellerr has a feature called Sam with just a single dot placed on an object it automatically creates a polygon around it let me show you I'll place a DOT on this strawberry and boom it's perfectly outlined this is super handy for fast accurate labeling in a similar way let's annotate the other berries quickly adjustments can be made to your annotation from here see the changing width of the polygon now let's move to keypoint annotation this is great for identifying specific parts of an object I'll mark key points on the knife one on the handle and another on the blade this method is especially useful for things like pose estimation or tracking object Parts in motion like identifying where someone's joints are in a fitness app not so much for full objects but great for details you can view your annotated dots like this once you've labeled everything Precision is key in labellerr you can easily adjust the size of bounding boxes we now have a problem as you can clearly see this part of strawberry is left out so to include any specific part in your polygon firstly activate the key points now select the starting key Point while holding shift and then just rightclick on the point you want to include hold shift again and click on ending key point this ensures the data is clean before exporting it for AI training adjustments can also be made like this Beyond basic annotations labellerr offers additional features to enhance your data set's quality and structure attributes how do you distinguish between a ripe and unripe strawberry or a red and green apple using attributes you could do this let me demonstrate how to add them in the attribute dropdown you'll find multiple options I'll select the radio button option to categorize the ripeness of the strawberries and the color of the Apple adding green and red as two options to our attribute similarly adding options for the strawberries attribute now let's select the red option for the Apple here and ripe for the strawberry attributes allow you to add specific details to objects using this feature you can classify objects based on their characteristics classification unlike attributes which are object specific classifications apply a common label to the entire image for example if the whole image depicts a breakfast scene you can classify it accordingly this is particularly useful in large scale data sets where context matters grouping tool feature let's start by first creating cix and Barry as objects and annotating them separately the group grouping tool is used to assign a common ID to both of them indicating they belong to a common entity we'll have to remove the previous annotation on the strawberry to understand it better just click on The annotation and press delete on the keyboard or remove them from this section annotating calx using Sam and then the berry you can use Sam here too clicking right up here we activate the grouping tool and all the annotations inside it will be selected and assigned an ID as described earlier group one is assigned to the berry and cix moving on to the other strawberry now we have a feature for auto bordering here first move your pointer over the cix then press control key points will be activated then click on the key point where you want to start then clicking on the second key point would indicate the direction you want to move to and now Mark the last key point to end at last press see how smooth The annotation at the boundary looks now I'll apply the grouping tool to this and the others quickly this is how our final annotation looks like you may view all group annotations by clicking here the filter section helps in filtering out specific annotations objects not designated to any group can be viewed too and that's it bounding boxes for quick tasks polygons for precision and key points for detailed Parts which method do you think works best for this image are you planning to cover more advanced annotation techniques or automation features in future tutorials then book a demo with lober today",
    "transcript_chunks": [
      "hello everyone and welcome to today's tutorial on image annotation in this session we'll be working with this kitchen scene featuring strawberries a knife yogurt honey apple and granola while I am adding objects and their annotation type I'll also explain why annotating these objects is crucial accurate annotations help AI models recognize everyday objects imagine using this for a smart grocery checkout system that identifies fruits automatically or maybe for a cooking assistant robot that picks out ingredients it could even assist diet tracking apps by detecting what you're eating cool right now let's see how to actually label these objects using labellerrs tool you see multiple objects here you may switch between them using numbers or by directly right-clicking the pan tool allows us to adjust the image the quality of the image can also be enhanced from this option we'll talk about transparency later and these these two features can be used to adjust the dimensions of our image we'll get to the grouping tool feature as well in the latter part of the video first up bounding box annotation this is the simplest method where we draw rectangles around objects I'll start by drawing a BB box around the knife here then the bowl and lastly the strawberries apple and granola bounding boxes are perfect for Clear simple shapes and are commonly used in object detection tasks but as you'll notice there they're not super precise for objects with irregular shapes like the strawberries just view all your annotation from here the 10 here highlights total number of annotations now coming to polygon annotation polygon annotation lets us outline objects more precisely let's annotate honey first now how to change The annotation type of the strawberry so that it can be more precisely labeled let's see",
      "the procedure to change The annotation type of the strawberry changing it to polygon just clear annotations that are not required you may use polygon annotation by right clicking at boundary points or just holding shift and dragging the pointer along the boundary to complete press n see how much cleaner that looks now annotating each of the five strawberries may require much more manual work for this labellerr has a feature called Sam with just a single dot placed on an object it automatically creates a polygon around it let me show you I'll place a DOT on this strawberry and boom it's perfectly outlined this is super handy for fast accurate labeling in a similar way let's annotate the other berries quickly adjustments can be made to your annotation from here see the changing width of the polygon now let's move to keypoint annotation this is great for identifying specific parts of an object I'll mark key points on the knife one on the handle and another on the blade this method is especially useful for things like pose estimation or tracking object Parts in motion like identifying where someone's joints are in a fitness app not so much for full objects but great for details you can view your annotated dots like this once you've labeled everything Precision is key in labellerr you can easily adjust the size of bounding boxes we now have a problem as you can clearly see this part of strawberry is left out so to include any specific part in your polygon firstly activate the key points now select the starting key Point while holding shift and then just rightclick on the point you want to include hold shift again and click on ending key point this ensures the data is clean",
      "before exporting it for AI training adjustments can also be made like this Beyond basic annotations labellerr offers additional features to enhance your data set's quality and structure attributes how do you distinguish between a ripe and unripe strawberry or a red and green apple using attributes you could do this let me demonstrate how to add them in the attribute dropdown you'll find multiple options I'll select the radio button option to categorize the ripeness of the strawberries and the color of the Apple adding green and red as two options to our attribute similarly adding options for the strawberries attribute now let's select the red option for the Apple here and ripe for the strawberry attributes allow you to add specific details to objects using this feature you can classify objects based on their characteristics classification unlike attributes which are object specific classifications apply a common label to the entire image for example if the whole image depicts a breakfast scene you can classify it accordingly this is particularly useful in large scale data sets where context matters grouping tool feature let's start by first creating cix and Barry as objects and annotating them separately the group grouping tool is used to assign a common ID to both of them indicating they belong to a common entity we'll have to remove the previous annotation on the strawberry to understand it better just click on The annotation and press delete on the keyboard or remove them from this section annotating calx using Sam and then the berry you can use Sam here too clicking right up here we activate the grouping tool and all the annotations inside it will be selected and assigned an ID as described earlier group one is assigned to the berry and cix moving on",
      "to the other strawberry now we have a feature for auto bordering here first move your pointer over the cix then press control key points will be activated then click on the key point where you want to start then clicking on the second key point would indicate the direction you want to move to and now Mark the last key point to end at last press see how smooth The annotation at the boundary looks now I'll apply the grouping tool to this and the others quickly this is how our final annotation looks like you may view all group annotations by clicking here the filter section helps in filtering out specific annotations objects not designated to any group can be viewed too and that's it bounding boxes for quick tasks polygons for precision and key points for detailed Parts which method do you think works best for this image are you planning to cover more advanced annotation techniques or automation features in future tutorials then book a demo with lober today"
    ],
    "transcript_word_count": 1073,
    "transcript_chunk_count": 4
  },
  {
    "video_id": "87m3L22J1aM",
    "title": "How Data Annotation Enhances Retail AI Performance",
    "description": "In this video, we explore how data annotation enhances retail AI performance, enabling innovations like automated checkouts, real-time inventory management, and personalized shopping experiences. We’ll dive into the role of computer vision in retail, from identifying products on shelves to tracking customer behavior and ensuring planogram compliance.\nYou’ll also learn how high-quality data annotation improves AI accuracy, scalability, and efficiency, while poorly annotated data can lead to errors like mislabeled products and facial recognition mistakes.\n\nChapters\n0:00 Introduction to Retail AI Innovations\n0:14 What is Data Annotation?\n0:41 Computer Vision in Retail\n0:55 Automated Checkout Systems\n1:03 Customer Behavior Analysis\n1:15 Inventory Management with AI\n1:23 Planogram Compliance\n1:44 How Data Annotation Enhances Retail AI\n2:07 Consequences of Poor Data Annotation\n2:47 Labellerr as a Data Annotation Solution\n3:08 Conclusion\n\nInterested in learning more about our services? Follow the links below:\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=87m3L22J1aM",
    "embed_url": "https://www.youtube.com/embed/87m3L22J1aM",
    "duration": 201,
    "view_count": 116,
    "upload_date": "20250212",
    "uploader": "Labellerr",
    "tags": [
      "object detection retail",
      "shelf monitoring AI",
      "retail data labeling",
      "product tagging AI",
      "smart shelves",
      "customer journey mapping",
      "retail shelf analytics",
      "real-time shelf tracking",
      "digital signage AI",
      "data annotation",
      "image annotation tool",
      "cvat tool",
      "computer vision retail",
      "data annotation retail",
      "inventory management AI",
      "machine learning",
      "annotation software",
      "product recognition AI",
      "facial keypoint annotation",
      "retail computer vision use cases",
      "retail security AI"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "imagine walking into a store where shelves are always stocked checkout lines are a thing of the past and every shopping experience feels personalized just for you even trying on glasses or makeup is done virtually from your phone the technology behind all this is computer vision and the foundation of computer vision is data annotation let's Deep dive into it data annotation is the process of labeling images videos audio or text to train AI systems to recognize and understand visual information for example in retail data annotation can be done by drawing bounding boxes around products on a shelf or mark key points on a customer's face how computer vision is used in retail retailers using computer vision in Inventory management have seen a 20 to 30% reduction in outof stock situations let's look at some more real life use cases of computer vision automated checkout stores like Amazon go use computer vision to track items customers pick up enabling a seamless checkout experience customer Behavior Analysis heat Maps generated from video feeds help retailers understand how customers move through stores optimizing layouts for better Sales Inventory management AI powered systems monitor shelves in real time ensuring products are always in stock planogram compliance a planogram is a visual blueprint that shows how products should be arranged on shelves studies show that 70% of purchasing decisions are made in store making planogram compliance critical for driving sales and customer satisfaction but none of this would be possible without data annotation how data annotation enhances retail AI performance highquality annotations improve AI accuracy enabling precise object recognition scalable Solutions across stores and efficient automation of tasks like inventory checks retailers using well-annotated data sets have seen a full 40 to 50% boost in AI performance enhancing customer satisfaction and operations let's look at some cases when data annotation goes wrong mislabeled products imagine a customer uses a visual search to find guava but AI recommends green apple instead this could happen if the data set was poorly annotated planogram failures if a shelf monitoring AI is trained on incorrectly annotated images it might fail to detect outof stock items or placed products facial recognition errors in personalized shopping if facial key points are inaccurately annotated the AI might misidentify customers leading to awkward or even offensive recommendations how can labellerr be a solution you can solve these challenges with a data annotation tool like labellerr it simplifies the process by automating annotations for products on shelves facial key points and customer Behavior patterns using bounding boxes polygons and key key points you can clearly see in this demo how the lady her facial key points and the orange in her hand are annotated let's get to the conclusion so the next time you walk into a futuristic retail store remember behind every AI driven Innovation is a team of annotators teaching machines to see the World As We Do",
    "transcript_chunks": [
      "imagine walking into a store where shelves are always stocked checkout lines are a thing of the past and every shopping experience feels personalized just for you even trying on glasses or makeup is done virtually from your phone the technology behind all this is computer vision and the foundation of computer vision is data annotation let's Deep dive into it data annotation is the process of labeling images videos audio or text to train AI systems to recognize and understand visual information for example in retail data annotation can be done by drawing bounding boxes around products on a shelf or mark key points on a customer's face how computer vision is used in retail retailers using computer vision in Inventory management have seen a 20 to 30% reduction in outof stock situations let's look at some more real life use cases of computer vision automated checkout stores like Amazon go use computer vision to track items customers pick up enabling a seamless checkout experience customer Behavior Analysis heat Maps generated from video feeds help retailers understand how customers move through stores optimizing layouts for better Sales Inventory management AI powered systems monitor shelves in real time ensuring products are always in stock planogram compliance a planogram is a visual blueprint that shows how products should be arranged on shelves studies show that 70% of purchasing decisions are made in store making planogram compliance critical for driving sales and customer satisfaction but none of this would be possible without data annotation how data annotation enhances retail AI performance highquality annotations improve AI accuracy enabling precise object recognition scalable Solutions across stores and efficient automation of tasks like inventory checks retailers using well-annotated data sets have seen a full 40 to 50% boost in AI performance enhancing customer satisfaction and",
      "operations let's look at some cases when data annotation goes wrong mislabeled products imagine a customer uses a visual search to find guava but AI recommends green apple instead this could happen if the data set was poorly annotated planogram failures if a shelf monitoring AI is trained on incorrectly annotated images it might fail to detect outof stock items or placed products facial recognition errors in personalized shopping if facial key points are inaccurately annotated the AI might misidentify customers leading to awkward or even offensive recommendations how can labellerr be a solution you can solve these challenges with a data annotation tool like labellerr it simplifies the process by automating annotations for products on shelves facial key points and customer Behavior patterns using bounding boxes polygons and key key points you can clearly see in this demo how the lady her facial key points and the orange in her hand are annotated let's get to the conclusion so the next time you walk into a futuristic retail store remember behind every AI driven Innovation is a team of annotators teaching machines to see the World As We Do"
    ],
    "transcript_word_count": 490,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "vTyl2syis28",
    "title": "Why AI Teams Are Switching from Open-Source Annotation Tools",
    "description": "Are open-source data labeling tools slowing down your AI projects? \nIn this video, we explain why many AI teams are moving from free annotation tools to paid platforms.You’ll learn what data labeling is, the benefits and struggles of using open-source tools.\nWe also compare features like automation, scalability, and support to help you choose the right tool for your project.If you’re working on small projects, free tools might be enough. But for large-scale AI projects, investing in a paid tool can save time and improve the quality of your data.\n\nChapters\n0:00 Introduction to Data Annotation\n0:08 Importance of High-Quality Labeled Data\n0:20 Open-Source Annotation Tools: Pros and Cons\n0:38 Challenges with Open-Source Tools\n1:28 Benefits of Paid Data Annotation Platforms\n1:47 Features of Paid Platforms\n2:31 Choosing the Right Tool for Your AI Project\n2:52 How Labellerr Supports AI Data Annotation\n\nInterested in learning more about services? \nWebsite: https://www.labellerr.com\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=vTyl2syis28",
    "embed_url": "https://www.youtube.com/embed/vTyl2syis28",
    "duration": 178,
    "view_count": 62,
    "upload_date": "20250207",
    "uploader": "Labellerr",
    "tags": [
      "machine learning annotation",
      "Labellerr annotation tool",
      "annotation tool scalability",
      "annotation tool limitations",
      "annotation tool benefits",
      "annotation tool for enterprises",
      "annotation tool for startups",
      "data annotation",
      "image annotation tool",
      "machine learning",
      "cvat tool",
      "annotation software",
      "data annotation tools",
      "paid annotation platforms",
      "AI data labeling",
      "annotation platform features",
      "quality control annotation",
      "annotation tool integration"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "data annotation or labeling is the process of tagging raw data like images videos or text to make it understandable for AI without highquality labeled data AI models can fail imagine training a self-driving car on inaccurate labels it could be a disaster so choosing the right type of annotation tool is important open- Source labeling tools look like a great choice because they are free flexible and customizable but AI teams soon face problems slow setup too much manual work security risks and poor scalability that's why many teams are making a switch to paid ones let's break it down complex setup first you spend hours or even days configuring the software need a new feature you have to code it yourself as minimal collaboration features are provided AI automation mainly they support manual labeling annotators waste time on repetitive tasks instead of focusing on AI model improvements limited documentation it can make users rely on community forums or issue threads for support these sources might be inconsistent or unclear which makes it harder to solve problems scalability issues when project size grows open- Source tools often struggle with performance annotation speed drops team productivity takes a hit integration issues open- Source platforms offer limited API support for ML integration so due to these limitations most AI teams are switching from open source tools to PID platforms these platforms remove inefficiencies making labeling faster easier and scalable let's see the features of paid platforms over open source sites quality control with built-in review Systems you can ensure the highest standards of accuracy and consistency across your data set improving the quality of your AI models scalable for any team paid tools also offer team collaboration this means you can assign tasks track progress and manage multiple labellerrs simultaneously making it perfect for larger teams or distributed workforces support and Innovation with regular updates 24/7 support and industry-leading features paid platforms grow with your AI needs for example label air is one such paid tool we introduced the group Tool feature to meet the specific requirements of our customer oi we already have a detailed video on it on our Channel AI powered automation Smart Tools like AI assisted annotations and active learning red reduce manual effort by up to 70% the smart choice so if you're a beginner or working on a small project free tools might be enough but if you're a company needing large scale data annotation investing in a paid tool can save time and improve quality and that's what most companies today are doing want to learn more get in touch with us at labellerr to explore how our platform can support your AI projects",
    "transcript_chunks": [
      "data annotation or labeling is the process of tagging raw data like images videos or text to make it understandable for AI without highquality labeled data AI models can fail imagine training a self-driving car on inaccurate labels it could be a disaster so choosing the right type of annotation tool is important open- Source labeling tools look like a great choice because they are free flexible and customizable but AI teams soon face problems slow setup too much manual work security risks and poor scalability that's why many teams are making a switch to paid ones let's break it down complex setup first you spend hours or even days configuring the software need a new feature you have to code it yourself as minimal collaboration features are provided AI automation mainly they support manual labeling annotators waste time on repetitive tasks instead of focusing on AI model improvements limited documentation it can make users rely on community forums or issue threads for support these sources might be inconsistent or unclear which makes it harder to solve problems scalability issues when project size grows open- Source tools often struggle with performance annotation speed drops team productivity takes a hit integration issues open- Source platforms offer limited API support for ML integration so due to these limitations most AI teams are switching from open source tools to PID platforms these platforms remove inefficiencies making labeling faster easier and scalable let's see the features of paid platforms over open source sites quality control with built-in review Systems you can ensure the highest standards of accuracy and consistency across your data set improving the quality of your AI models scalable for any team paid tools also offer team collaboration this means you can assign tasks track progress and manage multiple labellerrs simultaneously making",
      "it perfect for larger teams or distributed workforces support and Innovation with regular updates 24/7 support and industry-leading features paid platforms grow with your AI needs for example label air is one such paid tool we introduced the group Tool feature to meet the specific requirements of our customer oi we already have a detailed video on it on our Channel AI powered automation Smart Tools like AI assisted annotations and active learning red reduce manual effort by up to 70% the smart choice so if you're a beginner or working on a small project free tools might be enough but if you're a company needing large scale data annotation investing in a paid tool can save time and improve quality and that's what most companies today are doing want to learn more get in touch with us at labellerr to explore how our platform can support your AI projects"
    ],
    "transcript_word_count": 450,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "TyxP59p65WY",
    "title": "How Oishii Revolutionized Automated Harvesting with Labellerr…",
    "description": "Discover how Oishii, a leader in vertical farming, tackled the challenges of labeling complex image data for robotic strawberry harvesting. \nLearn how Labellerr's advanced Group Tool transformed their AI model, enhancing accuracy and efficiency in detecting ripeness and precise cutting points. \nWatch this video to see how data labeling is revolutionizing agriculture and how industries like automotive, security, and sports are also leveraging this game-changing technology.\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn:   / labellerr  \nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=TyxP59p65WY",
    "embed_url": "https://www.youtube.com/embed/TyxP59p65WY",
    "duration": 170,
    "view_count": 96,
    "upload_date": "20250129",
    "uploader": "Labellerr",
    "tags": [
      "crop monitoring AI",
      "precision agriculture",
      "AI crop detection",
      "yield optimization AI",
      "AI-powered drones farming",
      "data annotation",
      "DataLabeling",
      "VerticalFarming",
      "Labellerr",
      "AgricultureInnovation",
      "data science",
      "annotation software",
      "data annotation tool",
      "deep learning",
      "machine learning",
      "labeling software",
      "visual data",
      "image labeling",
      "labeling solutions",
      "dataset creation",
      "data collection",
      "AI in agriculture",
      "vertical farming AI",
      "computer vision farming",
      "robotic harvesting"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "artificial intelligence is transforming Industries across the globe from improving Healthcare Diagnostics to automating retail and streamlining Logistics AI is reshaping how we work and live ai's impact in agriculture AI is making its impact in agriculture consistently AI powered tools are analyzing weather patterns soil conditions and crop Health robots and drones powered by AI help monitor Fields spray crops and even harvest in vertical farming AI takes efficiency to the next level crops are grown in stacked layers indoors with AI controlling light water and nutrients this Innovative method uses less space and water introduction to oi oi is an organization in the vertical farming sector focused on using robots to make farming smarter and more efficient challenges faced by oosi as they scaled their operations they encountered challenges in labeling large volumes of image data they needed to train AI models to detect ripe strawberries and identify precise cutting points on the stems the complexity of distinguishing between the berry Kix and stem along with varying ripeness levels made the process both timec consuming and prone to errors for instance overlapping stems could be mistakenly labeled as one leading the robot to cut the wrong stem this leads to inefficient harvesting labellerrs group Tool solution to address these challenges Oshi par partnered with labellerr labellerr introduced a grouping tool to streamline The annotation process first we labeled each part of the strawberry the berry kaix and stem using polygon annotations these annotations helped differentiate the three parts then bounding boxes were used to annotate the entire Berry focusing on the ripeness levels as part of the model's training data the group Tool allowed to group The Berry kaix and stem together treating them as a single unit and gave them a common ID for easy tracking this made it easier for the AI model to understand how these parts interact improving the accuracy of the model's predictions results and impacts now oi's ai models became much more accurate in detecting ripeness and identifying precise cutting points this helped speed up their harvesting process and reduced errors conclusion lebel's automated annotation and quality control capabilities significantly enhanced the customer's ability to develop a reliable AI model for robotic strawberry harvesting in vertical farming book a demo with leler now to explore how different Industries like healthcare Sports security and surveillance Etc are using computer vision technology",
    "transcript_chunks": [
      "artificial intelligence is transforming Industries across the globe from improving Healthcare Diagnostics to automating retail and streamlining Logistics AI is reshaping how we work and live ai's impact in agriculture AI is making its impact in agriculture consistently AI powered tools are analyzing weather patterns soil conditions and crop Health robots and drones powered by AI help monitor Fields spray crops and even harvest in vertical farming AI takes efficiency to the next level crops are grown in stacked layers indoors with AI controlling light water and nutrients this Innovative method uses less space and water introduction to oi oi is an organization in the vertical farming sector focused on using robots to make farming smarter and more efficient challenges faced by oosi as they scaled their operations they encountered challenges in labeling large volumes of image data they needed to train AI models to detect ripe strawberries and identify precise cutting points on the stems the complexity of distinguishing between the berry Kix and stem along with varying ripeness levels made the process both timec consuming and prone to errors for instance overlapping stems could be mistakenly labeled as one leading the robot to cut the wrong stem this leads to inefficient harvesting labellerrs group Tool solution to address these challenges Oshi par partnered with labellerr labellerr introduced a grouping tool to streamline The annotation process first we labeled each part of the strawberry the berry kaix and stem using polygon annotations these annotations helped differentiate the three parts then bounding boxes were used to annotate the entire Berry focusing on the ripeness levels as part of the model's training data the group Tool allowed to group The Berry kaix and stem together treating them as a single unit and gave them a common ID for easy tracking this",
      "made it easier for the AI model to understand how these parts interact improving the accuracy of the model's predictions results and impacts now oi's ai models became much more accurate in detecting ripeness and identifying precise cutting points this helped speed up their harvesting process and reduced errors conclusion lebel's automated annotation and quality control capabilities significantly enhanced the customer's ability to develop a reliable AI model for robotic strawberry harvesting in vertical farming book a demo with leler now to explore how different Industries like healthcare Sports security and surveillance Etc are using computer vision technology"
    ],
    "transcript_word_count": 398,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "gOeu5hjzuew",
    "title": "Speed Up AI Training with SAM 2 for Quick Video Annotation | Sports, Robotics & Security",
    "description": "Learn how to annotate video data like a pro!\nIn this step-by-step guide, we demonstrate video annotation using a tennis match.\nExplore how to label players, the ball, and even add attributes and classifications to enhance your dataset.\n\nHighlights:\n- Bounding boxes for players\n- Polygon annotations for precision labeling of the ball\n- Adding attributes like player activity and jersey color\n- Using SAM 2(Segment Anything Model) for faster annotation\n\nChapters\n0:00 Introduction to Video Annotation\n0:20 Getting Familiar with the Video\n0:29 Choosing Annotation Types\n0:48 Bounding Boxes for Players\n1:02 Polygon Annotation for Tennis Ball\n1:14 Adding Attributes and Classifications\n2:08 Main Labeling Interface\n2:13 Labeling Players and Balls\n3:08 Adjusting Bounding Boxes\n3:20 Polygon Annotation Process\n3:47 Using SAM 2 for Faster Annotation\n4:28 Setting Video Classifications\n4:44 Automating Annotations with SAM 2\n5:43 Reviewing Annotations\n6:07 Playing the Labeled Video\n6:10 Conclusion and Next Steps\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nFind us on Social Media Platforms:\nLinkedIn: https://www.linkedin.com/company/labellerr\nTwitter: https://x.com/Labellerr1",
    "video_url": "https://www.youtube.com/watch?v=gOeu5hjzuew",
    "embed_url": "https://www.youtube.com/embed/gOeu5hjzuew",
    "duration": 390,
    "view_count": 216,
    "upload_date": "20250121",
    "uploader": "Labellerr",
    "tags": [
      "sports video labeling",
      "bounding box annotation",
      "polygon annotation",
      "tennis match annotation",
      "ball tracking AI",
      "player labeling",
      "attribute annotation",
      "frame-by-frame annotation",
      "SAM annotation",
      "automated video labeling",
      "annotation workflow",
      "object detection video",
      "annotation best practices",
      "multi-object annotation",
      "Labellerr annotation tool",
      "video annotation software",
      "data annotation",
      "image annotation tool",
      "machine learning",
      "annotation software",
      "cvat tool",
      "video annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hi everyone welcome to this guide on video annotation today we'll learn how to annotate a short video of a tennis game using bounding boxes and polygon annotations by the end of this video you'll know how to label players the ball and other objects while adding useful attributes and classifications to enhance your data set let's get familiar with the video before we start labeling after uploading the video to the tool the first task is to choose the appropriate annotation type for each object we identify two objects here the players and the ball as seen in the video preview earlier you could make your labeling work even more precise by annotating additional elements like Shadows buildings Etc we'll use bounding boxes to label the players a bounding box surrounds the object with a simple rectangular shape making it ideal for players since they out lines don't require High Precision for the tennis ball let's use polygon annotation while a bounding box is equally good for this purpose I'll use a polygon for demonstration purposes to make our data set even more informative we can add attributes and classifications let's add an attribute to the player that highlights their activity similarly attributes like Jersey color position Etc ET could be added as you can see we have various types of attributes to choose from I'll be going with the radio button and add two options to it classification can also be made for the video as a whole here's how so after adding playing conditions as classification with open arena and closed Arena radio options for it we will move to main labeling interface by clicking save and next up here let's get to the labeling work now our video has 54 frames so we'll annotate after every four frames you can adjust your labeling methods accordingly let's first label the player by clicking on it from the object section as we saw in the preview of the video earlier the players in the court were playing while the players standing outside it were idle so let's add more detail to the objects by adding attributes to them we can adjust the dimensions of the bounding box Anytime by clicking on it now after selecting the ball we draw a polygon around it in this way after drawing a single dot use shift and guide your pointer around the ball to precisely label it at the end press n on your keyboard to finish it now the ball is precisely labeled return to the original zoomed out view by clicking up here now in the same fashion we have to label the other balls too timec consuming task right to save time we use the Sam segment anything model feature which effortlessly annotates the tennis ball with just a single Dot We Now quickly label all the balls present here to wrap up labeling the first frame we select the playing conditions as as open arena this serves as the classification for the video as a whole we can directly select objects by pressing the numbers present in front of them for the ball in this instance we press one annotating all 54 frames manually by repeating the same bounding boxes and polygons can be a tedious and timeconsuming task thankfully we have a solution for this Sam 2 the segment anything model to save time we'll use Sam 2 for subsequent frames Sam 2 leverages AI to predict object boundaries based on previous annotations automatically adjusting the bounding boxes and polygons for the next frame with ease and precision for a detailed explanation of how Sam 2 Works check out our video on the channel we apply Sam to all the objects here once you've annotated all the frames and added attributes review your work to ensure accuracy and consistency notice how our video annotations create a clear and detailed data set now that we've applied Sam 2 to all our objects let's let's play our labeled video that's it for this guide on video annotation we've covered bounding boxes polygons Sam attributes and classifications with these techniques you can create detailed annotations for any video data set if you found this guide helpful be sure to like And subscribe for more informative content See you in the next video",
    "transcript_chunks": [
      "hi everyone welcome to this guide on video annotation today we'll learn how to annotate a short video of a tennis game using bounding boxes and polygon annotations by the end of this video you'll know how to label players the ball and other objects while adding useful attributes and classifications to enhance your data set let's get familiar with the video before we start labeling after uploading the video to the tool the first task is to choose the appropriate annotation type for each object we identify two objects here the players and the ball as seen in the video preview earlier you could make your labeling work even more precise by annotating additional elements like Shadows buildings Etc we'll use bounding boxes to label the players a bounding box surrounds the object with a simple rectangular shape making it ideal for players since they out lines don't require High Precision for the tennis ball let's use polygon annotation while a bounding box is equally good for this purpose I'll use a polygon for demonstration purposes to make our data set even more informative we can add attributes and classifications let's add an attribute to the player that highlights their activity similarly attributes like Jersey color position Etc ET could be added as you can see we have various types of attributes to choose from I'll be going with the radio button and add two options to it classification can also be made for the video as a whole here's how so after adding playing conditions as classification with open arena and closed Arena radio options for it we will move to main labeling interface by clicking save and next up here let's get to the labeling work now our video has 54 frames so we'll annotate after every",
      "four frames you can adjust your labeling methods accordingly let's first label the player by clicking on it from the object section as we saw in the preview of the video earlier the players in the court were playing while the players standing outside it were idle so let's add more detail to the objects by adding attributes to them we can adjust the dimensions of the bounding box Anytime by clicking on it now after selecting the ball we draw a polygon around it in this way after drawing a single dot use shift and guide your pointer around the ball to precisely label it at the end press n on your keyboard to finish it now the ball is precisely labeled return to the original zoomed out view by clicking up here now in the same fashion we have to label the other balls too timec consuming task right to save time we use the Sam segment anything model feature which effortlessly annotates the tennis ball with just a single Dot We Now quickly label all the balls present here to wrap up labeling the first frame we select the playing conditions as as open arena this serves as the classification for the video as a whole we can directly select objects by pressing the numbers present in front of them for the ball in this instance we press one annotating all 54 frames manually by repeating the same bounding boxes and polygons can be a tedious and timeconsuming task thankfully we have a solution for this Sam 2 the segment anything model to save time we'll use Sam 2 for subsequent frames Sam 2 leverages AI to predict object boundaries based on previous annotations automatically adjusting the bounding boxes and polygons for the next frame with ease",
      "and precision for a detailed explanation of how Sam 2 Works check out our video on the channel we apply Sam to all the objects here once you've annotated all the frames and added attributes review your work to ensure accuracy and consistency notice how our video annotations create a clear and detailed data set now that we've applied Sam 2 to all our objects let's let's play our labeled video that's it for this guide on video annotation we've covered bounding boxes polygons Sam attributes and classifications with these techniques you can create detailed annotations for any video data set if you found this guide helpful be sure to like And subscribe for more informative content See you in the next video"
    ],
    "transcript_word_count": 723,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "HG7HjcpsxGs",
    "title": "Track Objects 10x Faster in Videos with Labellerr’s SAM 2 Annotation Tool",
    "description": "Discover how SAM 2 makes video object tracking up to 10x faster on Labellerr. Track players, balls, or any moving objects with just a few clicks. Save hours of manual work and get accurate results in no time.\n\nUse Labellerr for sports analytics, robotics, agriculture, retail, and more. Simplify your workflow and improve efficiency today!\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nChapters\n0:00 Introduction to Labellerr’s SAM2 Tool\n0:12 Uploading a Football Video for Tracking\n0:24 Selecting Objects to Track (Ball and Player)\n0:35 Using the Magic Brush and SAM2 Model\n0:48 Adding Point Prompts for Segmentation\n1:01 Visualizing Tracked Objects in the Timeline\n1:11 Tracking the Ball with SAM2\n1:18 Tracking the Player with SAM2\n1:42 Previewing Seamless Object Tracking\n1:50 Handling Occlusions and Out-of-Frame Objects\n2:09 Conclusion: Easy Object Tracking with SAM2\n\nInterested in learning more about our services? \nWebsite: https://www.labellerr.com\n\nFind us on Social Media Platforms:\nLinkedIn: https://linkedin.com/company/labellerr\nTwitter: https://twitter.com/Labellerr1/",
    "video_url": "https://www.youtube.com/watch?v=HG7HjcpsxGs",
    "embed_url": "https://www.youtube.com/embed/HG7HjcpsxGs",
    "duration": 137,
    "view_count": 236,
    "upload_date": "20250114",
    "uploader": "Labellerr",
    "tags": [
      "video object tracking",
      "SAM 2",
      "segment anything model",
      "labeller demo",
      "AI video segmentation",
      "tracking moving objects",
      "player tracking AI",
      "ball tracking AI",
      "video annotation tool",
      "object tracking software",
      "video labeling automation",
      "frame-by-frame tracking",
      "computer vision tracking",
      "real-time object tracking",
      "deep learning video tracking",
      "machine learning sports",
      "sports data annotation",
      "AI for sports",
      "data annotation",
      "machine learning",
      "cvat tool",
      "annotation software"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hey everyone welcome to this demo of our labellerr tool featuring The segment anything model 2 Sam 2 today I'm going to show you how simple it is to track objects and videos using our platform for this demo I've uploaded a video of people playing football let's Jump Right In First we'll go to the label tab let me play the video briefly so you can see what we're working with in this video I want to track two things one the ball two the player dribbling the ball I've already created two labels for these objects player and ball now let's select the ball label I'll click on the Magic Brush icon and choose segment anything model from the dropdown menu oh looks like it's already selected perfect next I'll click on the interact icon to get started using the tool I'll add Point prompts to help the model identify and segment the ball click on the tick icon to confirm the segmentation of the object notice the pink timeline below it matches the label color and you'll also see the label name with a red dot this is a visual representation of the object we're tracking then I'll rightclick and select Sam 2 track this will automatically track the ball across the entire video similarly I'll follow the same process for the player object I'll select the player label provide the necessary prompts confirm the segmentation using the tick icon and then use the Sam 2 track option to track the player throughout the video Let's Play the video so you can see it in action both the ball and the player are now being tracked seamlessly now you see the gaps between the red dots it's because the object is either occluded or out of frame if the model predicts it wrong and the object is actually in frame or out of frame you can fix this by right clicking and selecting Mark out of view or Markin view respectively and that's it object tracking Made Easy with Sam 2 on labellerr thanks for watching and I hope you'll give it a try",
    "transcript_chunks": [
      "hey everyone welcome to this demo of our labellerr tool featuring The segment anything model 2 Sam 2 today I'm going to show you how simple it is to track objects and videos using our platform for this demo I've uploaded a video of people playing football let's Jump Right In First we'll go to the label tab let me play the video briefly so you can see what we're working with in this video I want to track two things one the ball two the player dribbling the ball I've already created two labels for these objects player and ball now let's select the ball label I'll click on the Magic Brush icon and choose segment anything model from the dropdown menu oh looks like it's already selected perfect next I'll click on the interact icon to get started using the tool I'll add Point prompts to help the model identify and segment the ball click on the tick icon to confirm the segmentation of the object notice the pink timeline below it matches the label color and you'll also see the label name with a red dot this is a visual representation of the object we're tracking then I'll rightclick and select Sam 2 track this will automatically track the ball across the entire video similarly I'll follow the same process for the player object I'll select the player label provide the necessary prompts confirm the segmentation using the tick icon and then use the Sam 2 track option to track the player throughout the video Let's Play the video so you can see it in action both the ball and the player are now being tracked seamlessly now you see the gaps between the red dots it's because the object is either occluded or out of frame",
      "if the model predicts it wrong and the object is actually in frame or out of frame you can fix this by right clicking and selecting Mark out of view or Markin view respectively and that's it object tracking Made Easy with Sam 2 on labellerr thanks for watching and I hope you'll give it a try"
    ],
    "transcript_word_count": 357,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "6aMl9759er8",
    "title": "Annotate Audio 5x Faster with Labellerr’s Tool | Audio Annotation, Transcription, speech Agent",
    "description": "Learn how to efficiently annotate audio files using the Labellerr Tool! \nIt provides a simple guide to features like attribute tagging and classification, perfect for individuals or teams aiming to simplify and improve audio labeling.\n\nChapters\n0:00 Introduction to Audio Annotation Challenges\n0:03 Meet Labellerr: Your Audio Annotation Solution\n0:06 Getting Started with Labellerr’s Workspace\n0:12 Exploring the Main Labeling Window\n0:16 Adding Objects for Annotation\n0:23 Naming Audio Objects (Husband and Wife)\n0:30 Adding Attributes to Describe Audio\n0:42 Exploring Attribute Options in Dropdown Menu\n1:05 Classifying Audio Based on Quality\n1:24 Starting the Labeling Process\n1:41 Labeling Speakers with Attributes\n1:53 Setting Tone Attributes (Questioning)\n2:06 Labeling Husband’s Segment (Apologetic Tone)\n2:17 Adjusting Segment Dimensions\n2:54 Completing All Segment Annotations\n2:59 Using Zoom for Precision Labeling\n3:08 Reviewing and Editing Annotations\n3:28 Viewing Segment Timelines\n3:35 Classifying the Entire Audio Segment\n3:39 Submitting the Labeling Process\n3:41 Conclusion: Transform Your Audio Annotation with Labellerr\n\nExplore more About audio Annotation Using Labellerr!\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=6aMl9759er8",
    "embed_url": "https://www.youtube.com/embed/6aMl9759er8",
    "duration": 230,
    "view_count": 530,
    "upload_date": "20250110",
    "uploader": "Labellerr",
    "tags": [
      "speech annotation",
      "audio segmentation",
      "attribute tagging",
      "audio classification",
      "annotation workflow",
      "audio labeling software",
      "audio transcription annotation",
      "audio model training",
      "audio data labeling",
      "segment annotation",
      "audio annotation tutorial",
      "audio tagging",
      "sound event detection",
      "multi-speaker annotation",
      "audio quality labeling",
      "data annotation",
      "image annotation tool",
      "annotation software",
      "machine learning",
      "cvat tool",
      "audio annotation",
      "audio labeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "ever struggled with annotating audio for your machine learning projects meet labellerr your ultimate solution now that we've logged into our workspace created the domain and uploaded the audio file for annotation we've arrived at the main labeling window this is where the real magic begins now let's add the objects by clicking on the add object button in the audio I've uploaded there are two distinct sounds one male and one female I'll name them husband and wife for this annotation next we will add attributes to describe the audio here is the procedure to it as you can see there's a wide range of attributes provided in the drop- down menu offering users the flexibility to choose the most relevant ones for their annotation needs this ensures that every detail of the audio is captured accurately and efficiently you can even classify your audio let's in this case classify on the basis of audio quality now let's get to the labeling work and start by playing the audio hello hi honey where are you I was just wondering when you'll be home hey I'm still at work things got a bit hectic today it'll take me a little longer to get back maybe an hour or so oh I next we'll label the speakers here's how let's add the attribute on the right side of the window setting the tone to be questioning while there are multiple ways to use attributes as shown earlier I've chosen to use the multi- select option for this task moving on to the next segment of the husband by clicking on it you can easily adjust the dimensions of the segment at any time by sliding the vertical margins following the same procedure as for segment one we set the tone of the husband attribute to apologetic label all the other segments in the same manner with all segments labeled we've completed the process this ensures accurate annotations for Effective model training the zoom feature can be used on selected segments to achieve greater Precision after labeling the entire audio file review your annotations to ensure accuracy you can edit any segment by selecting it on the waveform and updating the tags or attributes the timeline for each segment can also be viewed from here to wrap up we can classify the entire audio segment from here and then clicking on submit will complete your labeling process try labellerr today and transform your audio annotation process click the link in the description to get started",
    "transcript_chunks": [
      "ever struggled with annotating audio for your machine learning projects meet labellerr your ultimate solution now that we've logged into our workspace created the domain and uploaded the audio file for annotation we've arrived at the main labeling window this is where the real magic begins now let's add the objects by clicking on the add object button in the audio I've uploaded there are two distinct sounds one male and one female I'll name them husband and wife for this annotation next we will add attributes to describe the audio here is the procedure to it as you can see there's a wide range of attributes provided in the drop- down menu offering users the flexibility to choose the most relevant ones for their annotation needs this ensures that every detail of the audio is captured accurately and efficiently you can even classify your audio let's in this case classify on the basis of audio quality now let's get to the labeling work and start by playing the audio hello hi honey where are you I was just wondering when you'll be home hey I'm still at work things got a bit hectic today it'll take me a little longer to get back maybe an hour or so oh I next we'll label the speakers here's how let's add the attribute on the right side of the window setting the tone to be questioning while there are multiple ways to use attributes as shown earlier I've chosen to use the multi- select option for this task moving on to the next segment of the husband by clicking on it you can easily adjust the dimensions of the segment at any time by sliding the vertical margins following the same procedure as for segment one we set the tone of",
      "the husband attribute to apologetic label all the other segments in the same manner with all segments labeled we've completed the process this ensures accurate annotations for Effective model training the zoom feature can be used on selected segments to achieve greater Precision after labeling the entire audio file review your annotations to ensure accuracy you can edit any segment by selecting it on the waveform and updating the tags or attributes the timeline for each segment can also be viewed from here to wrap up we can classify the entire audio segment from here and then clicking on submit will complete your labeling process try labellerr today and transform your audio annotation process click the link in the description to get started"
    ],
    "transcript_word_count": 423,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "G9TM8hvgVk4",
    "title": "Introducing Label GPT: Revolutionize Data Annotation and Accelerate Your Vision AI Projects",
    "description": "Say goodbye to hours of manual data labeling. Introducing Label GPT by Labellerr, the ultimate auto annotation tool designed to revolutionize your AI workflows.\n\nWith Label GPT, you can pre-label large datasets of visual data in minutes, saving your team days of effort. Integrated seamlessly into the Labellerr platform, it helps you speed up workflows, scale your Vision AI projects, and achieve more—faster.\n\nTry Label GPT today and see the difference. \n\nWebsite: https://www.labellerr.com\nLabel GPT: https://www.labellerr.com/labelgpt\nBook a Demo: https://www.labellerr.com/book-a-demo\n\n#LabelGPT #AutoAnnotation #VisionAI #AIDataLabeling #Labellerr #MachineLearning #AIWorkflow",
    "video_url": "https://www.youtube.com/watch?v=G9TM8hvgVk4",
    "embed_url": "https://www.youtube.com/embed/G9TM8hvgVk4",
    "duration": 35,
    "view_count": 75,
    "upload_date": "20241205",
    "uploader": "Labellerr",
    "tags": [
      "data annotation using transformers",
      "annotation software",
      "image annotation",
      "annotation workflow",
      "manual annotation",
      "text tagging",
      "data labeling jobs",
      "image labeling",
      "label studio guide",
      "training dataset",
      "dataset creation",
      "text annotation tool",
      "quality assurance",
      "annotation platforms",
      "image tagging",
      "data annotation",
      "data annotation tutorial",
      "data annotation python",
      "training data annotation",
      "data annotation companies",
      "annotation",
      "data science",
      "annotating data"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "introducing label GPT the Ultimate Auto annotation tool built by labellerr labeling large data sets manually can take hours sometimes even days it's exhausting and slows down your AI projects but with label GPT you can pre-label a huge amount of visual data in just a few minutes integrated directly into labellerrs platform your team can save days of manual effort speeding up workflows and scaling your vision AI projects effortlessly thank you for watching see for yourself try label GPT Now by clicking the link in the description",
    "transcript_chunks": [
      "introducing label GPT the Ultimate Auto annotation tool built by labellerr labeling large data sets manually can take hours sometimes even days it's exhausting and slows down your AI projects but with label GPT you can pre-label a huge amount of visual data in just a few minutes integrated directly into labellerrs platform your team can save days of manual effort speeding up workflows and scaling your vision AI projects effortlessly thank you for watching see for yourself try label GPT Now by clicking the link in the description"
    ],
    "transcript_word_count": 89,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "O0q_5sUMncI",
    "title": "Annotate 10,000+ Images 10x Faster with Labellerr’s SAM & Negative Prompt Tool",
    "description": "Discover how object segmentation with SAM revolutionizes image annotation. With SAM and negative point prompts integrated into Labellerr's annotation pipeline, you can create precise masks for images in under a minute. This groundbreaking technology accelerates workflows across various industries, including virtual try-ons, automobile damage detection, retail product categorization, precision agriculture, robotics navigation, and sports analytics. Save hours of manual work and boost your efficiency. \n\nExplore how object segmentation with SAM can transform your workflow today!\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=O0q_5sUMncI",
    "embed_url": "https://www.youtube.com/embed/O0q_5sUMncI",
    "duration": 100,
    "view_count": 120,
    "upload_date": "20241122",
    "uploader": "Labellerr",
    "tags": [
      "image segmentation",
      "object segmentation",
      "object detection",
      "segmentation",
      "object segmentation with florence 2",
      "image segmentation and object detection",
      "video segmentation",
      "instance segmentation",
      "object segmentation in videos",
      "semantic segmentation",
      "custom image segmentation",
      "ai segmentation",
      "segmentation mask",
      "segmentation model",
      "segmentation models",
      "florence 2 segmentation",
      "deep learning segmentation",
      "florence 2 tutorial",
      "data annotation",
      "video ai",
      "image masking"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Effortless Image Segmentation with SAM and Negative Point Prompts. Manually drawing masks for every object in an image can take hours. I looked into how much time it would take to manually segment everything in this image. The answer? Around 6 to 7 hours. Now, imagine if I had more than 10,000 images to work on. Let’s skip the math and instead, see how we can speed up the process. SAM, or the Segment Anything Model, is a powerful tool that quickly finds and separates all objects in an image. With just one click, it creates masks for everything—clothes, faces, backgrounds, and more. And when SAM is integrated into Labellerr’s annotation pipeline, you can complete this task in under a minute! But what if you don’t want everything? For example, I only want the clothes, not the face or background. This is where Negative Point Prompts come in.With a few clicks, you can remove the parts of the mask you don’t need. It’s fast, simple, and highly accurate. In less than a minute, we have a perfect mask of the clothes—ready to use for various applications. Now, imagine applying this segmented clothing to virtual try-ons like LV or Gucci. Customers can preview how outfits look on them without needing to try them on physically. With SAM and Negative Point Prompts, what used to take hours can now be done in minutes. It’s efficient, accurate, and perfect for large datasets. Thank you for watching, and start exploring how Labellerr can help boost your workflow today!Book a Demo from the link in t",
    "transcript_chunks": [
      "Effortless Image Segmentation with SAM and Negative Point Prompts. Manually drawing masks for every object in an image can take hours. I looked into how much time it would take to manually segment everything in this image. The answer? Around 6 to 7 hours. Now, imagine if I had more than 10,000 images to work on. Let’s skip the math and instead, see how we can speed up the process. SAM, or the Segment Anything Model, is a powerful tool that quickly finds and separates all objects in an image. With just one click, it creates masks for everything—clothes, faces, backgrounds, and more. And when SAM is integrated into Labellerr’s annotation pipeline, you can complete this task in under a minute! But what if you don’t want everything? For example, I only want the clothes, not the face or background. This is where Negative Point Prompts come in.With a few clicks, you can remove the parts of the mask you don’t need. It’s fast, simple, and highly accurate. In less than a minute, we have a perfect mask of the clothes—ready to use for various applications. Now, imagine applying this segmented clothing to virtual try-ons like LV or Gucci. Customers can preview how outfits look on them without needing to try them on physically. With SAM and Negative Point Prompts, what used to take hours can now be done in minutes. It’s efficient, accurate, and perfect for large datasets. Thank you for watching, and start exploring how Labellerr can help boost your workflow today!Book a Demo from the link in t"
    ],
    "transcript_word_count": 261,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "PZSbnSm8lc4",
    "title": "Annotate Video Objects with Labellerr’s Bounding Box Tool Easily",
    "description": "Discover how Labellerr simplifies bounding box annotation for videos. With easy-to-follow steps, you can track and mark moving objects like cars, ensuring accurate video annotation. Our intuitive platform helps you quickly adjust bounding boxes to follow object movement frame by frame.\n\nChapters\n0:00 Introduction to Video Annotation\n0:03 Starting Bounding Box Annotation\n0:07 Marking Objects in Video\n0:12 Adjusting Bounding Boxes for Movement\n1:14 Interpolating Bounding Boxes\n\nReady to streamline your video annotation process?\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nThis is the perfect solution for projects requiring detailed, automated video annotation.",
    "video_url": "https://www.youtube.com/watch?v=PZSbnSm8lc4",
    "embed_url": "https://www.youtube.com/embed/PZSbnSm8lc4",
    "duration": 80,
    "view_count": 163,
    "upload_date": "20241122",
    "uploader": "Labellerr",
    "tags": [
      "bounding box annotation",
      "bounding box image annotation",
      "image annotation",
      "bounding box object detection",
      "video annotation",
      "data annotation",
      "2d bounding box annotation",
      "bounding box annotation services",
      "bounding box annotations",
      "bounding box annotation linkedin",
      "2d bounding box image annotation",
      "bounding box deep learning",
      "bounding box machine learning",
      "cvat object tracking for video annotation",
      "cvat annotations for video datasets",
      "annotation software",
      "video tracking"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "here we demonstrate how you can annotate your videos with labellerr we are performing bounding box annotation for that we first Mark the object in the video which is car in this case and then slightly adjust the B box accordingly to the movement of the object rinse and repeat the process to completely annotate your video e after that we interpolate the bounding boxes",
    "transcript_chunks": [
      "here we demonstrate how you can annotate your videos with labellerr we are performing bounding box annotation for that we first Mark the object in the video which is car in this case and then slightly adjust the B box accordingly to the movement of the object rinse and repeat the process to completely annotate your video e after that we interpolate the bounding boxes"
    ],
    "transcript_word_count": 65,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "mQNQl1Ph7Dg",
    "title": "How to Easily Remove Objects In Seconds Using Labellerr's Magic Eraser",
    "description": "Discover how you can create pixel-perfect masks with Labellerr’s Segmentation Tool! In this quick tutorial, learn how to effectively mask walls while excluding windows and highlight them in red.\nLearn about the eraser tool for selective masking and remove the masks which are not required. Perfect for beginners and pros alike, this guide will enhance your image annotation techniques. \n\nExplore Labellerr for seamless data annotation solutions! Enhance your AI projects with expert tools for image, video, text, and audio labeling.\nWebsite: https://www.labellerr.com\n\nBook a Demo: https://www.labellerr.com/book-a-demo\n\n#SelectiveMasking \n#labellerr \n#SegmentationTool\n#maskcreation \n#EraserTool \n#imagemanipulation \n#editingtutorial \n#pixelperfect \n#graphicdesign \n#imagesegmentation",
    "video_url": "https://www.youtube.com/watch?v=mQNQl1Ph7Dg",
    "embed_url": "https://www.youtube.com/embed/mQNQl1Ph7Dg",
    "duration": 73,
    "view_count": 20,
    "upload_date": "20241122",
    "uploader": "Labellerr",
    "tags": [
      "annotation software",
      "image segmentation",
      "segmentation mask",
      "pixel-perfect annotation",
      "labellerr segmentation",
      "image annotation tutorial",
      "annotation accuracy",
      "AI-assisted segmentation",
      "semantic segmentation",
      "image labeling tool",
      "annotation workflow",
      "instance segmentation",
      "object-based segmentation",
      "wound segmentation",
      "data annotation",
      "image annotation tool",
      "cvat alternative",
      "v7labs alternative",
      "aws sagemaker alternative",
      "cvat tool",
      "machine learning"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "here we want mask of the wall but not the windows so we create the mask of the wall first now we use the Eraser tool to mark off the windows from the mask now we select the windows to mask them separately by color red now that we are done let's finalize and preview the masks",
    "transcript_chunks": [
      "here we want mask of the wall but not the windows so we create the mask of the wall first now we use the Eraser tool to mark off the windows from the mask now we select the windows to mask them separately by color red now that we are done let's finalize and preview the masks"
    ],
    "transcript_word_count": 57,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "p385gRkV6EM",
    "title": "How Labellerr Scaled Our Image Annotation Operations – Eric, Senior Scientist at Foss #Startup",
    "description": "Discover how Labellerr empowers customers to overcome challenges in preparing training datasets for vision, NLP, and LLM model training.\n\nIn this testimonial, Eric, a Senior Scientist at Foss Analytics, shares his experience using Labellerr's platform to tackle the complexities of image instance segmentation annotation. Eric highlights how Labellerr helped his team achieve scalable annotation throughput that was previously unattainable.\n\nLearn how our innovative tools and collaborative approach enable teams to streamline their workflows and achieve exceptional results in data annotation.\n\n#ComputerVision #InstanceSegmentation #DataAnnotation #ImageAnalysis #AI #MachineLearning #DeepLearning #DataScience #TechInnovation #AICommunity #FutureTech #Automation #NLP #Labellerr",
    "video_url": "https://www.youtube.com/watch?v=p385gRkV6EM",
    "embed_url": "https://www.youtube.com/embed/p385gRkV6EM",
    "duration": 67,
    "view_count": 46,
    "upload_date": "20240705",
    "uploader": "Labellerr",
    "tags": [
      "image annotation tool",
      "image annotation",
      "image annotation tutorial",
      "data annotation",
      "best image annotation tools",
      "image annotation tools",
      "encord image annotation tool",
      "ai data annotation",
      "auto image annotation using autodistill",
      "video annotation",
      "image collection",
      "insert object annotation in image processing project",
      "image processing",
      "medical data annotation",
      "annotations",
      "product operations manager",
      "annotation testimonial",
      "object detection",
      "annotation software"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "we thought that labellerr did a great job we have used labellerr for helping us with instant segmentation of images uh which had a number of objects that we needed to segment for further processing label really helped us scale this process and allowed us to get much more data than we would have been possible within our own organization we label by having this iterative approach when defining what objects what were objects and what were not objects uh with a quick turnaround time between reviews and annotation really quickly helped us get established a good annotation Baseline uh so we've been very happy with the quick reply time we had when we experienced problems in the output formats that were available and the quick turnaround time on feedbacks when we had reviews as well as the flexibility in scaling up and down workloads and yeah we would recommend label of all companies that need adapation services",
    "transcript_chunks": [
      "we thought that labellerr did a great job we have used labellerr for helping us with instant segmentation of images uh which had a number of objects that we needed to segment for further processing label really helped us scale this process and allowed us to get much more data than we would have been possible within our own organization we label by having this iterative approach when defining what objects what were objects and what were not objects uh with a quick turnaround time between reviews and annotation really quickly helped us get established a good annotation Baseline uh so we've been very happy with the quick reply time we had when we experienced problems in the output formats that were available and the quick turnaround time on feedbacks when we had reviews as well as the flexibility in scaling up and down workloads and yeah we would recommend label of all companies that need adapation services"
    ],
    "transcript_word_count": 158,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "lAYu-ewIhTE",
    "title": "Boost Data Annotation Accuracy and Efficiency with Labellerr's Active Learning Feature",
    "description": "Discover how Labellerr's Active Learning feature revolutionizes data annotation by significantly reducing annotation time per file.\n\nThis video provides a step-by-step demo, showcasing how to annotate files using Labellerr’s automated and manual labeling options. Learn to create an Active Learning job, select relevant projects, and choose the appropriate labels for training your model. Once trained, the model seamlessly integrates into your data-labeling workflow, delivering precise predictions and boosting efficiency.\n\nExplore more at: www.labellerr.com\nBook a personalized demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=lAYu-ewIhTE",
    "embed_url": "https://www.youtube.com/embed/lAYu-ewIhTE",
    "duration": 199,
    "view_count": 164,
    "upload_date": "20240628",
    "uploader": "Labellerr",
    "tags": [
      "image annotation",
      "video annotation",
      "annotation",
      "image annotation tool",
      "best data annotation companies",
      "data annotation companies",
      "data annotation for ai",
      "ai data annotation",
      "data annotation service",
      "data annotation company",
      "surge ai data annotation",
      "best data annotation services",
      "data annotation services",
      "training data annotation",
      "data annotation tech",
      "data annotation for ml",
      "text annotation",
      "data labeling workflow",
      "annotation model integration",
      "annotation platform demo"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hi this is laer a data annotation platform that streamlines your ml workflows I'll be introducing you to Active Learning introduced inside laer which has reduced The annotation time per file by a substantial amount for that let's go to the label screen and label some of the images to create our Active Learning job so let's get started let's label some images and yeah that's done so I've already labeled some images for creating my job click on settings go to auto Lael jobs this is the job listing page now click on create new job to create a new Active Learning job now select the use case that is most applicable to your project your data set I'll go for image segmentation click on next now already selected the projects where the model would be deployed I'll select the classes which would be used to train my active learning job now click on next and enter the job details like job name job description and the number of apop aop is the number of iterative cycles of training that a model goes through now click on start job to start your Active Learning job you can see that the modelbased intelligence model has been created inside the model listing page now this model has to be attached with the project where I want to use this model so there's a toggle that is present in the model row and I'll click on that toggle to attach this model to my project now switch to the labeling screen to access the model now inside the labeling screen click on use Auto Lael select the model select the get prediction button corresponding to the model selected and you'll see that you'll get all your predictions in one go it is as easy as that you can check and review all the labels that you have received so here I did add some new labels and I have received prediction corresponding to them and I can see that all the annotations are quite Pixel Perfect I hope you enjoyed this explanatory video thank you so much thank you for being a patient list now",
    "transcript_chunks": [
      "hi this is laer a data annotation platform that streamlines your ml workflows I'll be introducing you to Active Learning introduced inside laer which has reduced The annotation time per file by a substantial amount for that let's go to the label screen and label some of the images to create our Active Learning job so let's get started let's label some images and yeah that's done so I've already labeled some images for creating my job click on settings go to auto Lael jobs this is the job listing page now click on create new job to create a new Active Learning job now select the use case that is most applicable to your project your data set I'll go for image segmentation click on next now already selected the projects where the model would be deployed I'll select the classes which would be used to train my active learning job now click on next and enter the job details like job name job description and the number of apop aop is the number of iterative cycles of training that a model goes through now click on start job to start your Active Learning job you can see that the modelbased intelligence model has been created inside the model listing page now this model has to be attached with the project where I want to use this model so there's a toggle that is present in the model row and I'll click on that toggle to attach this model to my project now switch to the labeling screen to access the model now inside the labeling screen click on use Auto Lael select the model select the get prediction button corresponding to the model selected and you'll see that you'll get all your predictions in one go it is",
      "as easy as that you can check and review all the labels that you have received so here I did add some new labels and I have received prediction corresponding to them and I can see that all the annotations are quite Pixel Perfect I hope you enjoyed this explanatory video thank you so much thank you for being a patient list now"
    ],
    "transcript_word_count": 363,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "kysWjXMVS5E",
    "title": "Labellerr AI Data Podcast: Garbage In, Garbage Out to Insights ft. Simon, Strategy Director @ IB",
    "description": "In this episode, we welcome Cho Yan (Simon) Yam, Director of Strategy and Operations at Internet Brands, to share his journey from corporate strategy to driving analytics in startups. Simon discusses the critical role of data-driven decisions, automation, and AI in transforming businesses.\n\nWhat You’ll Learn:\nHow Simon helped achieve a 43% revenue increase in a single year through data-centric strategies.\nThe power of analytics in streamlining operations and saving thousands of hours annually.\nInsights on using Python, SQL, and Tableau to turn data into actionable insights.\nHow Simon adapted pricing and product strategies during COVID-19 to enhance profitability.\nLessons from working with Fortune 500 companies and scaling startups.\nAbout the Host:\nPuneet Jindal, an AI/ML expert with 12+ years of experience, is the founder of Labellerr, an automated data labeling platform designed to create high-quality training data for machine learning models.\n\nExplore more at: www.labellerr.com\nBook a personalized demo: https://www.labellerr.com/book-a-demo\n\n#AI #DataAnalytics #MachineLearning #StartupJourney #Automation #Labellerr #Strategy #Podcast",
    "video_url": "https://www.youtube.com/watch?v=kysWjXMVS5E",
    "embed_url": "https://www.youtube.com/embed/kysWjXMVS5E",
    "duration": 2594,
    "view_count": 90,
    "upload_date": "20240628",
    "uploader": "Labellerr",
    "tags": [
      "ai podcast",
      "artificial intelligence podcast",
      "podcast",
      "ai podcast clips",
      "ai podcasts",
      "data podcast",
      "podcast ai",
      "data science podcast",
      "data analyst podcast",
      "seo podcast",
      "tech podcast",
      "coding podcast",
      "podcast coding",
      "academic podcast",
      "software podcast",
      "developer podcast",
      "ai-generated podcasts",
      "programmer podcast",
      "data analytics",
      "machine learning",
      "data storytelling",
      "ai trends",
      "artificial intelligence",
      "software engineering",
      "data visualization"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] hello everyone good morning good evening to all the folks lovely folks out there uh so this is Punit uh I'm your host for this today's AI data podcast uh and uh a quick introduction about myself uh so I run labellerr uh where we provide high quality data uh for computer vision NLP and generative VI teams help them uh with rlf and so on so forth uh so that they can focus on training their AI models fine tun their generative AI models and um so so as you know in this AI data podcast series we talk about we talk to um some wonderful guests uh who are working in the space of AI automation data analytics and so and so forth and and talk to them about their interesting stories and anecdotes uh which usually you would not find on the internet easily right so it's an opportunity for them to kind of share with the whole audience and that's what the motivation that they bring why they are on the podcast right and and today we have Simon uh who who is currently the analytics uh and data director at Internet Brand it's a 12b value technology and online Media company uh interestingly in his case he's worked with forun 500 Enterprises to fastp startups uh and even more interesting his his his journey started as a in from psychology and now he's you know getting into the he's got into the AI and data automation uh space uh and helping these com internet companies uh build their competitive advantage so uh welcome Simon uh welcome to the podcast thank you for having me cool so so Simon what we will do is uh let's let's uh first uh we would want to have a brief introduction about yourself uh for the audience uh understanding and uh we'll take it from there yeah sure absolutely so um like prit said is like I am the oper analytics and operation director in um internet brands so I came from a a typical background from as a analyst or engineer so it's like I I start my career as an analyst and also like just I work in a lot of work as a data scientist and also as like engineer through um Fortune 500 company startup and mids corporate company work with executive on the business side and also as I work with their um tag team sometimes it's actually like just around the globe it's like in India and also like in US learn a lot of things so I'm I'm very fortunate cool Simon would you elaborate on like um what what are the companies that you worked with and and what these industries have been uh just in brief for the audience sure so um I start my first job after so I got a bachelor degree in Psychology and a Master Degree in Psychology and after my master I my first job was in the Walt Disney Company so it's like as an analyst and then I work with their um gram holding company used to be known as The Washington Post company and then spend majority of my time time with a um big Hardware retailer a five bilon Runway Hardware retailer called Harbor fre to in the corporate strategy team which is their internal Consulting team so mainly work um with their corporate executive so it's like on the key decisions crunching data doing different analysis to drive actionable decisions and also like on the other side like work with different functional team to do different key uh business initiatives as like work with like operations um inventory marketing category management so it's like doing lots of um data analytics project analysis project also is like Predictive Analytics customer segmentation so all range of projects and after that it's like I join a startup a it's very interesting it's a um cannabis technology startup you can think of it as a um cannabis version of do Dash so in there is actually like I build their analytics system from groundup the ETL system is like from groundup and uh after two years it's like I joined my current company as their um strategy and operation director in internet brands which is um reporting to our coo working from working with both the business size and the tax side I'm kind of like the translator of both sized so talking business language and also like Bridging the Gap with the tag team so that's high levels like what I do interestingly I mean I just want to kind of have a followup question on this right um you mentioned that you moved from um Wasington Post company to kind of to a startup right so what was the motivation and what was the story behind that anything specific what made you think that okay let's let's move to the startup like completely in in very early phase and what was that phase of this company and um and what was your thoughts at that time I think I think if that was your first experience getting into a startup the first time getting into start yes it it's his actually like his chaotic time so it's like at that time it's like so I was at har fre is like so by the time is like I already work in corporate for a decade so it's like one year in Disney one year in gram Holdings and then um seven plus years in Harbor Freight twos so copper is great a lots of like relevant training great people existing process and all that and one day it's actually like um my friend in the same team and then just told me is like hey um Simon I have a my best friend is actually starting a company is like with an another person and then they need someone that's actually build analytics from ground up are you interested it was chaotic time because it's like it's 2021 so it's like it's co and everyone start to work from home and then I think to myself well is well it's un chattered um territory at the time so it's like it's not corporate every everything is different but I think to myself it's like I always want to build something from the ground up and you don't get the chance actually like incorporate because it's already existing protocol construct process and all that it's kind of like once in a lifetime opportunity to build something yourself from scratch in start up the way you want it so I asked myself will I take my experience in corporate and just take a shot at it and then I say okay I will try and that's how I jump to the startup after a decade in corporate interesting so and then then um what and and I'm I'm sure you must have tried to imagine like how it would like and then when you ended up in the in the company uh how was that like like how was that feeling like for you and you really went were going through the first year probably or maybe first few months of this startup it it's very different it's actually like so there's no structure people people are nice people are friendly they come from different background so the the thing I can remember the most is like everyone so in in corporate sectors like you have different system everything automated for you daily report and oriz so they have like a portal everyone I just like go and hit the button and pull the data they can only pull 30 days worth of report and every time you hit the button it takes about two minutes to pull you 30 days of sales data I was like wow that's difference from corporate and then I was thinking like okay how do you guys work here like that's why you're here we need automation from scratch I was like great let's do it so l i I start when I said I start from scratch I start from scratch it's like do you have API what kind of SEO database are you using oh AWS that's something I know so I just like can you get me access and I was like lit was like starting from the first line of code with python linking to the apis to those database and then building your own front end it start with like Google sheet and then later evolve into powerbi and tab R but everything is like just like from scratch I like the experience it's very different but I think it's like it's good time okay question uh has my question is U was it the first time that you kind of wrote the code for the because of this startup G had you had to write this uh in the setting uh for the automation or you have been been doing the coding work even previously as well the past yeah I doing like some um python development a lot of automation is like when I was with corporate but actually like just much easier because actually like you have existing protocol existing data report so it's like I am streamlining the process I'm automate something is actually like already kind of automated versus in the startup is actually like you just write the automation from scratch automating something is a reading manual so it's like that's the difference but yeah from the code perspective it's kind of different so that's why it's like the that's in the first place it's like I ask myself is that something I can do yes okay let me take a shot at it so and and any other interesting stories from this uh as you were doing automation um what kind of data existed and what were the challenges around Ai and data or whatever the automation that you did and and did psychology help you so I've asked couple of questions of one first is what were the challenges around Ai and data when you ended up focusing on the automation bit and I think I think that's a full purpose of why uh you joined uh this organization this startup right and uh maybe you can elaborate on that Journey yeah so I guess actually like um one thing is like is so no matter is actually like big organization or startup so it's like as the as the compan is actually like grown you got have lots of different data source so pretty much is like well maybe some of the data is like is offline on an Excel sprad sheet some of them just like is offline local database some of that is in the cloud and all that so it's like it just first thing okay to just consolidate the data in uniform format so actually like you can start as actually like rolling your data so it's like one of the project I can think of is um I build a customer segmentation model from scratch when I join the startup so it's like there are two things I build two big things actually I build when I join the startup one is the customer segmentation model which help them substantially is like increase their revenue by targeting is like the customer B is like who are the customer actually buying so it's like what tier what what kind of action you should do to them what kind of promotion actually you should do to them so that's one of the big thing so a lots of like predictive modeling so it's like okay so this is actually like the buying pattern what actually like draw they in into the door so it's like those are the high frequency customer those are the high margin customer what kind of of margin actually what what kind of product actually like is driving the revenue and all that so it's actually like I guess the first challenge is like just cleaning up data processing the data the next one is actually like just working with business unit to see okay what are the kpis we should be measuring our success is that the frequency of the product the nature of the product and all that and then finally it's actually just like just final is like like roll out the test product to the client to the business client and see is actually like okay is that what they want and the final stage will be put it in task to see is actually like if is Drive Better Business results so just multiple stage different challenges but I think actually like they they have fun in their own way okay so so you and then you were the first uh person to drive this automation at the organization or you had you I mean basically you you ended up building the team or was there already some team existed uh to who was doing some some some initial work ground in the startup actually like I kind of like in onean team to build it myself but it's like so but of course actually like the knowledge did did not um just come from nowhere so it's like um in Harbor Freight 2 is actually like um I learned a lot it's actually like about customer segmentation they work with a very good vendor so it's like how to like just segment the data how to actually like classify product how to Target customers so in my seven years actually like I learned a lot how to actually like use customer segmentation to Target retail and then I take that Playbook and then actually like in startup it's actually like I just write that myself using python okay okay and then how how big was the company in terms of the number of employees at that time how big was the team at that time when you joined them when I joined them I think it's actually like about um in corporate maybe it's like 50 people and then it's actually like so because they are a delivery company and that's like they have about maybe 100 200 people is like drivers but it's like corporate is about 50 so like you can imagine it's like in finance the C Finance is like me the CFO and then it's like a Director of Finance and then an accountant and surfo is actually like several more accountants so it's like less than 10 people really laned team and I think I think now it's more than 10,000 people is it you mean their startup it's yeah yeah this company unfortunately it's actually like their startup actually closed down it's like n no they didn't okay okay okay so I mean but but I think they were growing right uh they they were growing so it's like like so um back in back into the so um it was a um series a startup and then actually like after I joined actually like we had a good run and then um it called grass store and then it become their second largest um cannabis delivery companies in California we were great at one point so like was growing and after I buil a customer segmentation model was actually like the revenue was like grown by 43% in 12 months but after that actually like I think it's like um part of it is like the general environment is like of the Cannabis industry High tax rate and all that eventually it's actually like after I left it's like they closed down last November so okay and and what about the harbor freed tools so that was also a company that was also small company when you joined they they are doing really well so like when I joined they got about 300 stores okay and then now they have over 1,500 store right now they're doing really well they know what they're doing yeah okay okay cool and and and what about the work there at at Harbor feed tool in har fre Tu is um so as I said it's like I am I was in their corporate strategy team which is a small internal Consulting team is like for the exact so it's like a lot of actually like I learn a lot there so it's like it's small team compile with like um investment bankers and also like just like top Consultants from like big four and those like high-end um Consulting brand consulting firm which I'm the only one who's not from that background I'm from um and so you are special I'm from Liberal art special in the wrong way but yes good good that I survive no I think I think I think it's it's not like that so so I mean cool cool that you are being humble So Okay cool so uh cool uh let's let's move on to for next set of questions right so U so you mentioned your journey from a junior analyst to a director right and and while we were we were chatting also you emphasize the importance of the continuous learning right so what's one skill of or piece of knowledge you have acquired along the way that surpris you the most I I will actually like have two part of to those answer I would say it's actually like so um I start as an analyst so it's like I in the corporate strategy team m i report to a boss came from Investment Banking and then I look at his skills I was like oh my God that's what an analyst really means so in the early stages actually like I really learned from him it's like how to be an analyst how to analyze problem and present like data to the exact how to actually like tell a business story so that's the most important skills actually like at the early stage and as time goes by is like when the companies actually like need a lot of automation work I spend my time actually like learning weba and python so that actually like helped me a lot in terms of Automation and also actually like later AI so just like just come into like the world of like predictive modeling and also it's like um alation and all that so like short answer is actually like tax continuous learning is actually like in terms of tech Co skills that's actually like helped me to um just continue to rise in the corporate ladder so like I got free promotion within har Freight 2s and got promoted um to directors like within eight months in the startup but also actually like on the other side like I think some skills is timeless which like communication so it's like those things actually like you need it as an analyst and also actually like until director left for ex like you still have to be able to communicate to the exact and also like communicate to the tag team so it's like I would say continuous learning keep evolving on the technical skill and continue to evolve in your communication skill which is kind of the Timeless skill those two component is actually like just kind of highlight as my most important learning in my career okay any instances that you might recall on um specifically uh the which which made you more U I would say sure of like okay these uh communication skills or the storytelling is is very very important um so I would say it's actually like earlier in my career it's actually like I really focused on um technical skills so meaning actually like crunching data how to be actually like the fastest analyst and all that but actually my delivery was lacking because actually just like I just I have to tell the exactly actually like what's going on and all that and I was like well they're not getting it because I just like I have my analyst jargon a lots of TCH jargon and all that and then I realized it is like that's not efficient so actually I spend some time it's like how to tell a simple story so it's like just like I think it's like some wise a wise person s is like like if you can not explain something in simple terms you don't know the subject good enough so I just like I take that literally and try to it's like okay will I understand it if I without the technical background I have so it's like try to think about it so it's like like just try to evolve from it I can tell it's like like I'm still evolve I'm still learning to this day because I just like now I report to the COO I work with the business side and also it's like because I I work with the tax side so it's like I within a day I speak both language I have the talk Cod so actually like that's how you do the Cod join the API on the tax side and also actually like explain to the C in terms of progress just like so if we do that what kind of business outcome can we expect this is plan a this is Plan B in a fi line SE like with technical jargons so yeah I'm still I'm still learning so uh let's switch our gears to kind of uh more on the unst structure data site right u i see that you've been been talking about computer vision a lot lately so what's the most exciting applications of computer vision that you have come across recently and how do you see uh computer vision transforming Industries in the near future so um I would say is actually like one of the things actually like I I think computer vision is actually like is fascinating today so of course actually like the most developed one of the most developed areas actually like um OCR so op the OCR is actually like just detecting is like their characters and then actually like translating like just image into actually work document and all that so Google Amazon actually like just transcript is like millions of documents like physical documents into um computer and all that I think it's like that's fascinating and also like in terms of healthare it's like like now computer vision can detect cancer even earlier is like than a human being I think that's fascinating but on the other side it's actually like because I also do my um training on computer vision somehow in my mind I feel like the technology today on computer is not there yet in some ways so for example it's actually like if you train the model the model will only works well in the mod in in the angle you train the model so actually like if you just flip it to another angle the accuracy ex will drop substantially so in order for you to maximize um the deficiency of the computer vision model you actually like have to just twist your environment set up the setting I think this is one of the main reason why the computer vision like model is not widely used in the world right now you can see lots of demos like manufacturing and all that but actually just like Google using it Google downloading it but actually like you cannot see actually a lot of widely used in other industry and all that because actually I think that still limitation and I believe is a the ne next Revolution will be actually like comes from it's like when we have more developed technology surrounding it's like Alman reality so it's like when you have like 3D scanning and all that so now you don't have the angle problem I talk about so it's like well I think it's like in short it's like I think computer vision is really developed in some aspect like OCR and maybe Health Care at the right angle but also it's like like some other aspect is waiting for the next evolution of technology to perfect it before actually like it can be widely used by our industry okay yeah no I I think I agree agree to your point and uh even I recently read about Professor V she's uh kind of called the Godmother of uh she's in a way actually lady uh godmother of AI Revolution especially when even we talk about the computer vision space right and wonderful uh stuff that she's done and so if if you recall the image net uh image net and uh the moment right what what actually transformed the whole industry and uh now I recently stumbled into her project uh which is called and exactly what you pointed out is a limitation right the computer vision doesn't understand like humans like us like okay what's on that side how does it look like on that side that's not the understanding that these computers have or the way we currently train our models the way that currently the technology is and U and and that's exactly what they are terming it as spatial intelligence it's like we humans know that okay it's object is lying there how probably it's where in the space and time uh it is it is lying there and what's the next action going to happen or likely going to happen or what is the possibilities of if if if the if the class is just lying on the corner of the table and and someone just come and kind of touch it then would it there's a likelihood that it might fall and then can we take an action to kind of catch it right exactly and so so that's not the understanding that computers have computer can just say that okay there's a class there's a table probably um there is someone who is kind of touching that class but it might not be able to kind of think that okay probably a glass can fall on their right side not on the left side because someone is applying that for so all those physics laws are not uh I mean the current computer vision Technologies I think I think not aware of this physics laws and all it can and it was an interesting example she shared right uh and she called it as so she called it as catastrophe so an example right I mean a cat going to the water and she and surfing through the water uh and still it doesn't get wet so she said what a catastrophe so she just choked about it so interesting it's a TED Talk recently and uh that I've watched a very recent Ted talk so uh I I think you're just just one example of that okay looking at from the other side it would not be able to kind of handle that U very well summarizes uh the same point yeah I I I I I completely agree I mean so because I came from a psychology background right the way I look at AI the AI is a kid with a baby with infinity learning capacity and exponential learning curve but actually like Curr I think is like the limitation is like human is not is still finding the best way to train that really smart here let's just let it's like the 2D is like looking at the bottle right so is like okay this is the angle so this is the bottle this is also the bottle this is also the bottle but when you train it 2D that's not the spatial element and also it's actually like just missing the physical law and all that how to teach abstract those those elements are missing it's not like the computational power is not enough it's just like we as human don't know to teach this little kid all those smart stuff we need the kid to know the kid is called Ai and this uh we have built a kid without a psychology we need to embed the psychology into the this AI kid in a way yes yeah I mean in fact in fact this this new project that they this this all behind the spal intelligence is actually called Behavior so so it's it's very well you know talking about all of this uh on how humans behave uh to to and how humans react to the environment yeah yeah interesting so okay so U so let's uh you know explore a fun hypothetical scenario right uh so if you could if you could build an AI powered assistant to help you with your daily task what features would it have and how would it enhance your productivity I think it's actually like literally will be my butler literally is actually like my life Butler so just like so I work with AI a lot so it's like I have to say it's actually like I admire the consistency of AI so a a a little joke is actually like I keep telling my coworkers like I say it's like the automation I create is the better self of myself so it's never get tired it never got it never complain without a big mouth of myself work 24 hours consistent never make human mistake yeah it's just so I so because I work with like so half of my day I work with code half of the day I work with hum so was like was like a lot of time it's like well I think it's like well if AI can more like human and on wise words is actually like if human can more like AI so it's like I would want an AI to eliminate my human weakness so it's like sometimes like like I forget the key and all that was like okay AI will not forget the key so if AI is with me is Simon you forgot your key hey Simon it's time for you to do what so Simon is time for you to do what this kind of like life assistance yeah yeah cool okay interesting so and one um on on some recent developments about gen I have some question quick questions and we can answer it in short right so what is your take on chat GPD and in general about gen and of course I mean there is a recent new development that has happened in ch G so would love to hear your thoughts I I think it's fascinating too little I make my life much easier so like in some case it's actually like oh I just write something really important I was like oh you need someone to look at it it's like can you look at it for me make sure it's actually like there's no mistake done I was like love it so actually like eliminating a lot of Errors add a lot of efficiency to it so it's like like also like oh I need a brainstorm partner can you help me so it's like it's really good strategic decision making assistant too so that's one side but the other side I also really cautious about like okay AI can make you looks like a genius but AI like CH GPD and gen AI can make you an idiot because I just like you you completely rely on it every single time when someone asks you a question you just go to AI without AI now you're you're an idiot so just like it can make you better it can make you worse it's just like this is a spoon you can use a spoon for coffee you can use a spoon for cocaine it's just a to you can use it for better or worse it's a really powerful tool so I think it's like like just great too but use it wisely no definitely agree to this and uh and and if if you look at right recently Lama 3 released right a smart move by meta uh in one way and given uh given whatever open a releases and then meta has has it in in few months behind it and they come up with literally open source of it um what is your opinion what's going to happen like especially after recent open release of GPD o or like you know multimodal capabilities so is is mea's uh answer to that already in in process or something else is coming I I think it's like literal it's like we can already call it like an arm race of AI so it's like Amazon Google matter is actually like all raised to it so actually like let's talk to the positive side so it will be like more resources so just like everyone is like because actually everyone is looking at each other so it's like okay are you sharing it are you doing it for the general good and all that so I think it's good for like open source now the opening is actually like for general public to participate because actually like for their business purpose is like they need to gain business like market share so it's actually like the more people who use it actually it will be better position they got and also like a strategic move as you can like look at the stock market okay now it's actually like Google announcing oh I'm having my gen AI the stock price go up and the other other who don't announce it go down because I just like every single investor is like okay I I only want to put my money on AI are you doing it you are not doing it that's a problem oh they are releasing new stuff you are not releasing it or that's a problem so I just like and you can see so we we just got um Google IO yesterday yes so you were there in the past five I watched that it's it's great I I am a register um Android developer and register is like um Apple developer too so I I pay attention to both but you notice it's actually like in past five days the stock price of Google go up 6% just because they talk about AI so I think it's actually like matters like well we have to do it and but we have to do it smart we cannot just release the AI we just say well we're doing it for everyone we're doing it for the ethical way we have to open source and all that so I think SE like is strategic step for all this company just want to stay relevant and stay competitive to their competitors yeah okay cool I mean I I I do have more questions but then um in the interest of time and I know it's late for you as well uh just kind of speeding it up uh so my my another question is U so finally reflecting on your journey so far what's one piece of advice you wish you could give to your younger self uh when you were just starting out in your in your career in Ai and data I think it's actually like so um I think over my career is like well I do something right do something wrong but after so many years actually like I think two things are one thing is actually like always right meaning actually just keep learning learning always the best investment of your life so um your best friend is actually is like your skill and your knowledge if you don't walk away from your training no matter how tough the situation is it does not walk away from you so actually like you can always count on them as long as actually like you stay close with your learning and your training and in this world like of AI I think learning and training is the only way to keep yourself relevant I mean you can ask GPT but I just like without GPT you will be like what am I doing where am I who am I so right okay cool cool and what's your professional and personal goals for the next 3 to five years just quick thought maybe I I would say is like just keep incorporating AI into my work which I am doing I think it's actually like not just me it's like everyone is kind of thinking it's like well how can I stay relevant in my career in my job market and all that and maybe it's like in maybe it's like when I have some good idea good in enough idea I would try to start my own firm that would be actually like my road map to for the three to five years to stay ran with all the AIS going on and maybe start my own company okay so a bonus question um for the audience um from the audience side and for the sake of the audience um what's the top three or two forums that you kind of spend time reading about latest development I will say it's because actually like I I usually actually like I I know it's actually like it's tempting to just read is like like what is hot outside and truly actually like is important to actually like to read about actually like what are the Hot Topics so like what are going on actually on the AI front so that's one side so it's like stay rant what KN what is going on with the world what you need to keep up I think it's actually like the second also most important is actually like just learn the AI topic actually like relate to your own profession so actually like in my industry is actually like so just learning is like P Predictive Analytics so it's like how to actually like improve the forecast with AI customer segmentation and all that but it's something actually like relate to yourself because actually like I think as human we only learn well on some something you can relate to otherwise you would just forget about it so like what's new and also it's like what's something that can really relate to you and also like you can practice it um in the best cases like daily basis okay okay any specific Forum that you might want to point out for the audience current news SE I usually I just go to is like Wall Street Journal okay because follow the money and that's like for the AI and all that and pretty much it's like just all others AI Forum Reddit and all that actually like you can gain a lot of like relevant opinion like on the AI topic all right all right cool all right uh Simon so uh so here we are at the to the end of the conversation um and and I'm sure the all the audience have enjoyed uh listening to this conversation um between Simon and myself and uh and especially the all the interesting insight especially I mean I love this line of what Simon said right um skill and knowledge are your best friends okay so thank you Simon uh for being on the podcast and uh lovely chatting with you thank you so much thank you for having me",
    "transcript_chunks": [
      "[Music] hello everyone good morning good evening to all the folks lovely folks out there uh so this is Punit uh I'm your host for this today's AI data podcast uh and uh a quick introduction about myself uh so I run labellerr uh where we provide high quality data uh for computer vision NLP and generative VI teams help them uh with rlf and so on so forth uh so that they can focus on training their AI models fine tun their generative AI models and um so so as you know in this AI data podcast series we talk about we talk to um some wonderful guests uh who are working in the space of AI automation data analytics and so and so forth and and talk to them about their interesting stories and anecdotes uh which usually you would not find on the internet easily right so it's an opportunity for them to kind of share with the whole audience and that's what the motivation that they bring why they are on the podcast right and and today we have Simon uh who who is currently the analytics uh and data director at Internet Brand it's a 12b value technology and online Media company uh interestingly in his case he's worked with forun 500 Enterprises to fastp startups uh and even more interesting his his his journey started as a in from psychology and now he's you know getting into the he's got into the AI and data automation uh space uh and helping these com internet companies uh build their competitive advantage so uh welcome Simon uh welcome to the podcast thank you for having me cool so so Simon what we will do is uh let's let's uh first uh we would want to have",
      "a brief introduction about yourself uh for the audience uh understanding and uh we'll take it from there yeah sure absolutely so um like prit said is like I am the oper analytics and operation director in um internet brands so I came from a a typical background from as a analyst or engineer so it's like I I start my career as an analyst and also like just I work in a lot of work as a data scientist and also as like engineer through um Fortune 500 company startup and mids corporate company work with executive on the business side and also as I work with their um tag team sometimes it's actually like just around the globe it's like in India and also like in US learn a lot of things so I'm I'm very fortunate cool Simon would you elaborate on like um what what are the companies that you worked with and and what these industries have been uh just in brief for the audience sure so um I start my first job after so I got a bachelor degree in Psychology and a Master Degree in Psychology and after my master I my first job was in the Walt Disney Company so it's like as an analyst and then I work with their um gram holding company used to be known as The Washington Post company and then spend majority of my time time with a um big Hardware retailer a five bilon Runway Hardware retailer called Harbor fre to in the corporate strategy team which is their internal Consulting team so mainly work um with their corporate executive so it's like on the key decisions crunching data doing different analysis to drive actionable decisions and also like on the other side like work",
      "with different functional team to do different key uh business initiatives as like work with like operations um inventory marketing category management so it's like doing lots of um data analytics project analysis project also is like Predictive Analytics customer segmentation so all range of projects and after that it's like I join a startup a it's very interesting it's a um cannabis technology startup you can think of it as a um cannabis version of do Dash so in there is actually like I build their analytics system from groundup the ETL system is like from groundup and uh after two years it's like I joined my current company as their um strategy and operation director in internet brands which is um reporting to our coo working from working with both the business size and the tax side I'm kind of like the translator of both sized so talking business language and also like Bridging the Gap with the tag team so that's high levels like what I do interestingly I mean I just want to kind of have a followup question on this right um you mentioned that you moved from um Wasington Post company to kind of to a startup right so what was the motivation and what was the story behind that anything specific what made you think that okay let's let's move to the startup like completely in in very early phase and what was that phase of this company and um and what was your thoughts at that time I think I think if that was your first experience getting into a startup the first time getting into start yes it it's his actually like his chaotic time so it's like at that time it's like so I was at har fre is like",
      "so by the time is like I already work in corporate for a decade so it's like one year in Disney one year in gram Holdings and then um seven plus years in Harbor Freight twos so copper is great a lots of like relevant training great people existing process and all that and one day it's actually like um my friend in the same team and then just told me is like hey um Simon I have a my best friend is actually starting a company is like with an another person and then they need someone that's actually build analytics from ground up are you interested it was chaotic time because it's like it's 2021 so it's like it's co and everyone start to work from home and then I think to myself well is well it's un chattered um territory at the time so it's like it's not corporate every everything is different but I think to myself it's like I always want to build something from the ground up and you don't get the chance actually like incorporate because it's already existing protocol construct process and all that it's kind of like once in a lifetime opportunity to build something yourself from scratch in start up the way you want it so I asked myself will I take my experience in corporate and just take a shot at it and then I say okay I will try and that's how I jump to the startup after a decade in corporate interesting so and then then um what and and I'm I'm sure you must have tried to imagine like how it would like and then when you ended up in the in the company uh how was that like like how was that feeling like for",
      "you and you really went were going through the first year probably or maybe first few months of this startup it it's very different it's actually like so there's no structure people people are nice people are friendly they come from different background so the the thing I can remember the most is like everyone so in in corporate sectors like you have different system everything automated for you daily report and oriz so they have like a portal everyone I just like go and hit the button and pull the data they can only pull 30 days worth of report and every time you hit the button it takes about two minutes to pull you 30 days of sales data I was like wow that's difference from corporate and then I was thinking like okay how do you guys work here like that's why you're here we need automation from scratch I was like great let's do it so l i I start when I said I start from scratch I start from scratch it's like do you have API what kind of SEO database are you using oh AWS that's something I know so I just like can you get me access and I was like lit was like starting from the first line of code with python linking to the apis to those database and then building your own front end it start with like Google sheet and then later evolve into powerbi and tab R but everything is like just like from scratch I like the experience it's very different but I think it's like it's good time okay question uh has my question is U was it the first time that you kind of wrote the code for the because of this startup G had",
      "you had to write this uh in the setting uh for the automation or you have been been doing the coding work even previously as well the past yeah I doing like some um python development a lot of automation is like when I was with corporate but actually like just much easier because actually like you have existing protocol existing data report so it's like I am streamlining the process I'm automate something is actually like already kind of automated versus in the startup is actually like you just write the automation from scratch automating something is a reading manual so it's like that's the difference but yeah from the code perspective it's kind of different so that's why it's like the that's in the first place it's like I ask myself is that something I can do yes okay let me take a shot at it so and and any other interesting stories from this uh as you were doing automation um what kind of data existed and what were the challenges around Ai and data or whatever the automation that you did and and did psychology help you so I've asked couple of questions of one first is what were the challenges around Ai and data when you ended up focusing on the automation bit and I think I think that's a full purpose of why uh you joined uh this organization this startup right and uh maybe you can elaborate on that Journey yeah so I guess actually like um one thing is like is so no matter is actually like big organization or startup so it's like as the as the compan is actually like grown you got have lots of different data source so pretty much is like well maybe some of the data",
      "is like is offline on an Excel sprad sheet some of them just like is offline local database some of that is in the cloud and all that so it's like it just first thing okay to just consolidate the data in uniform format so actually like you can start as actually like rolling your data so it's like one of the project I can think of is um I build a customer segmentation model from scratch when I join the startup so it's like there are two things I build two big things actually I build when I join the startup one is the customer segmentation model which help them substantially is like increase their revenue by targeting is like the customer B is like who are the customer actually buying so it's like what tier what what kind of action you should do to them what kind of promotion actually you should do to them so that's one of the big thing so a lots of like predictive modeling so it's like okay so this is actually like the buying pattern what actually like draw they in into the door so it's like those are the high frequency customer those are the high margin customer what kind of of margin actually what what kind of product actually like is driving the revenue and all that so it's actually like I guess the first challenge is like just cleaning up data processing the data the next one is actually like just working with business unit to see okay what are the kpis we should be measuring our success is that the frequency of the product the nature of the product and all that and then finally it's actually just like just final is like like roll out the test",
      "product to the client to the business client and see is actually like okay is that what they want and the final stage will be put it in task to see is actually like if is Drive Better Business results so just multiple stage different challenges but I think actually like they they have fun in their own way okay so so you and then you were the first uh person to drive this automation at the organization or you had you I mean basically you you ended up building the team or was there already some team existed uh to who was doing some some some initial work ground in the startup actually like I kind of like in onean team to build it myself but it's like so but of course actually like the knowledge did did not um just come from nowhere so it's like um in Harbor Freight 2 is actually like um I learned a lot it's actually like about customer segmentation they work with a very good vendor so it's like how to like just segment the data how to actually like classify product how to Target customers so in my seven years actually like I learned a lot how to actually like use customer segmentation to Target retail and then I take that Playbook and then actually like in startup it's actually like I just write that myself using python okay okay and then how how big was the company in terms of the number of employees at that time how big was the team at that time when you joined them when I joined them I think it's actually like about um in corporate maybe it's like 50 people and then it's actually like so because they are a delivery company and",
      "that's like they have about maybe 100 200 people is like drivers but it's like corporate is about 50 so like you can imagine it's like in finance the C Finance is like me the CFO and then it's like a Director of Finance and then an accountant and surfo is actually like several more accountants so it's like less than 10 people really laned team and I think I think now it's more than 10,000 people is it you mean their startup it's yeah yeah this company unfortunately it's actually like their startup actually closed down it's like n no they didn't okay okay okay so I mean but but I think they were growing right uh they they were growing so it's like like so um back in back into the so um it was a um series a startup and then actually like after I joined actually like we had a good run and then um it called grass store and then it become their second largest um cannabis delivery companies in California we were great at one point so like was growing and after I buil a customer segmentation model was actually like the revenue was like grown by 43% in 12 months but after that actually like I think it's like um part of it is like the general environment is like of the Cannabis industry High tax rate and all that eventually it's actually like after I left it's like they closed down last November so okay and and what about the harbor freed tools so that was also a company that was also small company when you joined they they are doing really well so like when I joined they got about 300 stores okay and then now they have over 1,500 store",
      "right now they're doing really well they know what they're doing yeah okay okay cool and and and what about the work there at at Harbor feed tool in har fre Tu is um so as I said it's like I am I was in their corporate strategy team which is a small internal Consulting team is like for the exact so it's like a lot of actually like I learn a lot there so it's like it's small team compile with like um investment bankers and also like just like top Consultants from like big four and those like high-end um Consulting brand consulting firm which I'm the only one who's not from that background I'm from um and so you are special I'm from Liberal art special in the wrong way but yes good good that I survive no I think I think I think it's it's not like that so so I mean cool cool that you are being humble So Okay cool so uh cool uh let's let's move on to for next set of questions right so U so you mentioned your journey from a junior analyst to a director right and and while we were we were chatting also you emphasize the importance of the continuous learning right so what's one skill of or piece of knowledge you have acquired along the way that surpris you the most I I will actually like have two part of to those answer I would say it's actually like so um I start as an analyst so it's like I in the corporate strategy team m i report to a boss came from Investment Banking and then I look at his skills I was like oh my God that's what an analyst really means so in the",
      "early stages actually like I really learned from him it's like how to be an analyst how to analyze problem and present like data to the exact how to actually like tell a business story so that's the most important skills actually like at the early stage and as time goes by is like when the companies actually like need a lot of automation work I spend my time actually like learning weba and python so that actually like helped me a lot in terms of Automation and also actually like later AI so just like just come into like the world of like predictive modeling and also it's like um alation and all that so like short answer is actually like tax continuous learning is actually like in terms of tech Co skills that's actually like helped me to um just continue to rise in the corporate ladder so like I got free promotion within har Freight 2s and got promoted um to directors like within eight months in the startup but also actually like on the other side like I think some skills is timeless which like communication so it's like those things actually like you need it as an analyst and also actually like until director left for ex like you still have to be able to communicate to the exact and also like communicate to the tag team so it's like I would say continuous learning keep evolving on the technical skill and continue to evolve in your communication skill which is kind of the Timeless skill those two component is actually like just kind of highlight as my most important learning in my career okay any instances that you might recall on um specifically uh the which which made you more U I would say",
      "sure of like okay these uh communication skills or the storytelling is is very very important um so I would say it's actually like earlier in my career it's actually like I really focused on um technical skills so meaning actually like crunching data how to be actually like the fastest analyst and all that but actually my delivery was lacking because actually just like I just I have to tell the exactly actually like what's going on and all that and I was like well they're not getting it because I just like I have my analyst jargon a lots of TCH jargon and all that and then I realized it is like that's not efficient so actually I spend some time it's like how to tell a simple story so it's like just like I think it's like some wise a wise person s is like like if you can not explain something in simple terms you don't know the subject good enough so I just like I take that literally and try to it's like okay will I understand it if I without the technical background I have so it's like try to think about it so it's like like just try to evolve from it I can tell it's like like I'm still evolve I'm still learning to this day because I just like now I report to the COO I work with the business side and also it's like because I I work with the tax side so it's like I within a day I speak both language I have the talk Cod so actually like that's how you do the Cod join the API on the tax side and also actually like explain to the C in terms of progress just like so if",
      "we do that what kind of business outcome can we expect this is plan a this is Plan B in a fi line SE like with technical jargons so yeah I'm still I'm still learning so uh let's switch our gears to kind of uh more on the unst structure data site right u i see that you've been been talking about computer vision a lot lately so what's the most exciting applications of computer vision that you have come across recently and how do you see uh computer vision transforming Industries in the near future so um I would say is actually like one of the things actually like I I think computer vision is actually like is fascinating today so of course actually like the most developed one of the most developed areas actually like um OCR so op the OCR is actually like just detecting is like their characters and then actually like translating like just image into actually work document and all that so Google Amazon actually like just transcript is like millions of documents like physical documents into um computer and all that I think it's like that's fascinating and also like in terms of healthare it's like like now computer vision can detect cancer even earlier is like than a human being I think that's fascinating but on the other side it's actually like because I also do my um training on computer vision somehow in my mind I feel like the technology today on computer is not there yet in some ways so for example it's actually like if you train the model the model will only works well in the mod in in the angle you train the model so actually like if you just flip it to another angle the accuracy",
      "ex will drop substantially so in order for you to maximize um the deficiency of the computer vision model you actually like have to just twist your environment set up the setting I think this is one of the main reason why the computer vision like model is not widely used in the world right now you can see lots of demos like manufacturing and all that but actually just like Google using it Google downloading it but actually like you cannot see actually a lot of widely used in other industry and all that because actually I think that still limitation and I believe is a the ne next Revolution will be actually like comes from it's like when we have more developed technology surrounding it's like Alman reality so it's like when you have like 3D scanning and all that so now you don't have the angle problem I talk about so it's like well I think it's like in short it's like I think computer vision is really developed in some aspect like OCR and maybe Health Care at the right angle but also it's like like some other aspect is waiting for the next evolution of technology to perfect it before actually like it can be widely used by our industry okay yeah no I I think I agree agree to your point and uh even I recently read about Professor V she's uh kind of called the Godmother of uh she's in a way actually lady uh godmother of AI Revolution especially when even we talk about the computer vision space right and wonderful uh stuff that she's done and so if if you recall the image net uh image net and uh the moment right what what actually transformed the whole industry and uh",
      "now I recently stumbled into her project uh which is called and exactly what you pointed out is a limitation right the computer vision doesn't understand like humans like us like okay what's on that side how does it look like on that side that's not the understanding that these computers have or the way we currently train our models the way that currently the technology is and U and and that's exactly what they are terming it as spatial intelligence it's like we humans know that okay it's object is lying there how probably it's where in the space and time uh it is it is lying there and what's the next action going to happen or likely going to happen or what is the possibilities of if if if the if the class is just lying on the corner of the table and and someone just come and kind of touch it then would it there's a likelihood that it might fall and then can we take an action to kind of catch it right exactly and so so that's not the understanding that computers have computer can just say that okay there's a class there's a table probably um there is someone who is kind of touching that class but it might not be able to kind of think that okay probably a glass can fall on their right side not on the left side because someone is applying that for so all those physics laws are not uh I mean the current computer vision Technologies I think I think not aware of this physics laws and all it can and it was an interesting example she shared right uh and she called it as so she called it as catastrophe so an example right I mean",
      "a cat going to the water and she and surfing through the water uh and still it doesn't get wet so she said what a catastrophe so she just choked about it so interesting it's a TED Talk recently and uh that I've watched a very recent Ted talk so uh I I think you're just just one example of that okay looking at from the other side it would not be able to kind of handle that U very well summarizes uh the same point yeah I I I I I completely agree I mean so because I came from a psychology background right the way I look at AI the AI is a kid with a baby with infinity learning capacity and exponential learning curve but actually like Curr I think is like the limitation is like human is not is still finding the best way to train that really smart here let's just let it's like the 2D is like looking at the bottle right so is like okay this is the angle so this is the bottle this is also the bottle this is also the bottle but when you train it 2D that's not the spatial element and also it's actually like just missing the physical law and all that how to teach abstract those those elements are missing it's not like the computational power is not enough it's just like we as human don't know to teach this little kid all those smart stuff we need the kid to know the kid is called Ai and this uh we have built a kid without a psychology we need to embed the psychology into the this AI kid in a way yes yeah I mean in fact in fact this this new project that they",
      "this this all behind the spal intelligence is actually called Behavior so so it's it's very well you know talking about all of this uh on how humans behave uh to to and how humans react to the environment yeah yeah interesting so okay so U so let's uh you know explore a fun hypothetical scenario right uh so if you could if you could build an AI powered assistant to help you with your daily task what features would it have and how would it enhance your productivity I think it's actually like literally will be my butler literally is actually like my life Butler so just like so I work with AI a lot so it's like I have to say it's actually like I admire the consistency of AI so a a a little joke is actually like I keep telling my coworkers like I say it's like the automation I create is the better self of myself so it's never get tired it never got it never complain without a big mouth of myself work 24 hours consistent never make human mistake yeah it's just so I so because I work with like so half of my day I work with code half of the day I work with hum so was like was like a lot of time it's like well I think it's like well if AI can more like human and on wise words is actually like if human can more like AI so it's like I would want an AI to eliminate my human weakness so it's like sometimes like like I forget the key and all that was like okay AI will not forget the key so if AI is with me is Simon you forgot your key hey Simon it's",
      "time for you to do what so Simon is time for you to do what this kind of like life assistance yeah yeah cool okay interesting so and one um on on some recent developments about gen I have some question quick questions and we can answer it in short right so what is your take on chat GPD and in general about gen and of course I mean there is a recent new development that has happened in ch G so would love to hear your thoughts I I think it's fascinating too little I make my life much easier so like in some case it's actually like oh I just write something really important I was like oh you need someone to look at it it's like can you look at it for me make sure it's actually like there's no mistake done I was like love it so actually like eliminating a lot of Errors add a lot of efficiency to it so it's like like also like oh I need a brainstorm partner can you help me so it's like it's really good strategic decision making assistant too so that's one side but the other side I also really cautious about like okay AI can make you looks like a genius but AI like CH GPD and gen AI can make you an idiot because I just like you you completely rely on it every single time when someone asks you a question you just go to AI without AI now you're you're an idiot so just like it can make you better it can make you worse it's just like this is a spoon you can use a spoon for coffee you can use a spoon for cocaine it's just a to you can use",
      "it for better or worse it's a really powerful tool so I think it's like like just great too but use it wisely no definitely agree to this and uh and and if if you look at right recently Lama 3 released right a smart move by meta uh in one way and given uh given whatever open a releases and then meta has has it in in few months behind it and they come up with literally open source of it um what is your opinion what's going to happen like especially after recent open release of GPD o or like you know multimodal capabilities so is is mea's uh answer to that already in in process or something else is coming I I think it's like literal it's like we can already call it like an arm race of AI so it's like Amazon Google matter is actually like all raised to it so actually like let's talk to the positive side so it will be like more resources so just like everyone is like because actually everyone is looking at each other so it's like okay are you sharing it are you doing it for the general good and all that so I think it's good for like open source now the opening is actually like for general public to participate because actually like for their business purpose is like they need to gain business like market share so it's actually like the more people who use it actually it will be better position they got and also like a strategic move as you can like look at the stock market okay now it's actually like Google announcing oh I'm having my gen AI the stock price go up and the other other who don't announce it",
      "go down because I just like every single investor is like okay I I only want to put my money on AI are you doing it you are not doing it that's a problem oh they are releasing new stuff you are not releasing it or that's a problem so I just like and you can see so we we just got um Google IO yesterday yes so you were there in the past five I watched that it's it's great I I am a register um Android developer and register is like um Apple developer too so I I pay attention to both but you notice it's actually like in past five days the stock price of Google go up 6% just because they talk about AI so I think it's actually like matters like well we have to do it and but we have to do it smart we cannot just release the AI we just say well we're doing it for everyone we're doing it for the ethical way we have to open source and all that so I think SE like is strategic step for all this company just want to stay relevant and stay competitive to their competitors yeah okay cool I mean I I I do have more questions but then um in the interest of time and I know it's late for you as well uh just kind of speeding it up uh so my my another question is U so finally reflecting on your journey so far what's one piece of advice you wish you could give to your younger self uh when you were just starting out in your in your career in Ai and data I think it's actually like so um I think over my career is like well",
      "I do something right do something wrong but after so many years actually like I think two things are one thing is actually like always right meaning actually just keep learning learning always the best investment of your life so um your best friend is actually is like your skill and your knowledge if you don't walk away from your training no matter how tough the situation is it does not walk away from you so actually like you can always count on them as long as actually like you stay close with your learning and your training and in this world like of AI I think learning and training is the only way to keep yourself relevant I mean you can ask GPT but I just like without GPT you will be like what am I doing where am I who am I so right okay cool cool and what's your professional and personal goals for the next 3 to five years just quick thought maybe I I would say is like just keep incorporating AI into my work which I am doing I think it's actually like not just me it's like everyone is kind of thinking it's like well how can I stay relevant in my career in my job market and all that and maybe it's like in maybe it's like when I have some good idea good in enough idea I would try to start my own firm that would be actually like my road map to for the three to five years to stay ran with all the AIS going on and maybe start my own company okay so a bonus question um for the audience um from the audience side and for the sake of the audience um what's the top three or",
      "two forums that you kind of spend time reading about latest development I will say it's because actually like I I usually actually like I I know it's actually like it's tempting to just read is like like what is hot outside and truly actually like is important to actually like to read about actually like what are the Hot Topics so like what are going on actually on the AI front so that's one side so it's like stay rant what KN what is going on with the world what you need to keep up I think it's actually like the second also most important is actually like just learn the AI topic actually like relate to your own profession so actually like in my industry is actually like so just learning is like P Predictive Analytics so it's like how to actually like improve the forecast with AI customer segmentation and all that but it's something actually like relate to yourself because actually like I think as human we only learn well on some something you can relate to otherwise you would just forget about it so like what's new and also it's like what's something that can really relate to you and also like you can practice it um in the best cases like daily basis okay okay any specific Forum that you might want to point out for the audience current news SE I usually I just go to is like Wall Street Journal okay because follow the money and that's like for the AI and all that and pretty much it's like just all others AI Forum Reddit and all that actually like you can gain a lot of like relevant opinion like on the AI topic all right all right cool all right uh",
      "Simon so uh so here we are at the to the end of the conversation um and and I'm sure the all the audience have enjoyed uh listening to this conversation um between Simon and myself and uh and especially the all the interesting insight especially I mean I love this line of what Simon said right um skill and knowledge are your best friends okay so thank you Simon uh for being on the podcast and uh lovely chatting with you thank you so much thank you for having me"
    ],
    "transcript_word_count": 6691,
    "transcript_chunk_count": 23
  },
  {
    "video_id": "C5yaKfX9xPs",
    "title": "How Labellerr Transformed Image Annotation Scalability: Insights from Eric, Senior Scientist at Foss",
    "description": "Discover how Labellerr revolutionized the image instance segmentation process for Eric, Senior Scientist at Foss Analytics.\n\nIn this testimonial, Eric shares how Labellerr’s platform enabled his team to handle complex images with multiple objects, helping them scale their annotation efforts far beyond what was possible internally. With an iterative approach to object classification and rapid turnaround times between reviews and annotations, Labellerr delivered reliable, efficient, and scalable solutions for their project.\n\nThis story highlights the power of Labellerr's image segmentation service in overcoming challenges and driving success for data annotation tasks.\n\nVisit: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\n#ImageSegmentationService #DataAnnotation #ComputerVision #AI #ArtificialIntelligence #Testimonial #AISaaS #ImageAnalysis",
    "video_url": "https://www.youtube.com/watch?v=C5yaKfX9xPs",
    "embed_url": "https://www.youtube.com/embed/C5yaKfX9xPs",
    "duration": 64,
    "view_count": 116,
    "upload_date": "20240628",
    "uploader": "Labellerr",
    "tags": [
      "image segmentation",
      "segmentation",
      "medical image segmentation",
      "semantic segmentation",
      "ai segmentation",
      "video semantic segmentation",
      "segmentation model",
      "segmentation models",
      "deep learning segmentation",
      "image classification",
      "data annotation",
      "deep learning models",
      "deep learning",
      "image processing",
      "machine learning",
      "medical segmentation",
      "data preprocessing",
      "pattern recognition",
      "image annotation",
      "computer vision",
      "visual recognition",
      "image labeling",
      "pixel classification"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "we thought that labellerr did a great job we have used labellerr for helping us with instant segmentation of images uh which had a number of objects that we needed to segment for further processing label really helped us scale this process and allowed us to get much more data than we would have been possible within our own organization we label up by having this iterative approach when defining what objects what were objects and what were not objects uh with a quick turnaround TR between reviews and annotation really quickly helped us get established a good annotation Baseline uh so we've been very happy with the quick reply time we had when we experienced problems in the output formats that were available and the quick turnaround time on feedbacks when we had reviews as well as the flexibility in scaling up and down workloads yeah we would recommend label of for all companies that need anotation services",
    "transcript_chunks": [
      "we thought that labellerr did a great job we have used labellerr for helping us with instant segmentation of images uh which had a number of objects that we needed to segment for further processing label really helped us scale this process and allowed us to get much more data than we would have been possible within our own organization we label up by having this iterative approach when defining what objects what were objects and what were not objects uh with a quick turnaround TR between reviews and annotation really quickly helped us get established a good annotation Baseline uh so we've been very happy with the quick reply time we had when we experienced problems in the output formats that were available and the quick turnaround time on feedbacks when we had reviews as well as the flexibility in scaling up and down workloads yeah we would recommend label of for all companies that need anotation services"
    ],
    "transcript_word_count": 159,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "a-mowTUI2Xg",
    "title": "AI Career Advice for Aspiring Data Scientists | Masters or Industry?",
    "description": "Confused about whether to pursue a Master’s degree or jump straight into the industry as an aspiring data scientist?\n\nIn this video, we share invaluable AI career advice tailored for interns, professionals, and enthusiasts in the AI and machine learning space.\n\nDiscover:\n\nHow hands-on industry experience can shape your career.\nWhether pursuing a Master’s or PhD aligns with your goals.\nThe intersection of research and development in AI careers.\nGet practical insights to navigate your journey in the ever-evolving world of artificial intelligence and data science.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\nStart building your AI career with confidence!",
    "video_url": "https://www.youtube.com/watch?v=a-mowTUI2Xg",
    "embed_url": "https://www.youtube.com/embed/a-mowTUI2Xg",
    "duration": 60,
    "view_count": 26,
    "upload_date": "20240625",
    "uploader": "Labellerr",
    "tags": [
      "data scientist",
      "career advice",
      "data scientist career",
      "data science career",
      "career advice for young people",
      "data science career path",
      "data scientist skills",
      "data science for beginners",
      "data scientist career path",
      "data analyst",
      "data science",
      "data analytics",
      "data science interview",
      "data skills",
      "career growth",
      "analytics careers",
      "data education",
      "data science pathways",
      "career growth hacks",
      "data science guide",
      "data roles",
      "data science roadmap",
      "analytics strategy"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "that that leads to me in fact two follow of questions especially from a aspiring audience someone who is interning somewhere in the AI space right question is always like they keep figuring should I go for Master on that you want to stick to Industry setting and you are like confident enough with your skill set and it's going to ever be ever growing because once you get into industry and you are like um learning hands-on experience with tools and platforms that are required for a data scientist or a machine learning engineer then I guess um I would personally skip that step of like again taking a step step back or like and uh I uh also had a very vague idea towards stepping into like research space as well uh fortunately my full-time roles have always been at the intersection of research and development so I did not pursue a a part to PhD but in my industry setting I'm kind of doing the same",
    "transcript_chunks": [
      "that that leads to me in fact two follow of questions especially from a aspiring audience someone who is interning somewhere in the AI space right question is always like they keep figuring should I go for Master on that you want to stick to Industry setting and you are like confident enough with your skill set and it's going to ever be ever growing because once you get into industry and you are like um learning hands-on experience with tools and platforms that are required for a data scientist or a machine learning engineer then I guess um I would personally skip that step of like again taking a step step back or like and uh I uh also had a very vague idea towards stepping into like research space as well uh fortunately my full-time roles have always been at the intersection of research and development so I did not pursue a a part to PhD but in my industry setting I'm kind of doing the same"
    ],
    "transcript_word_count": 169,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "N6zBdztpgFA",
    "title": "Top Object Annotation Services | AI-Powered Labeling Tools - Labellerr",
    "description": "Discover Labellerr’s cutting-edge object annotation services designed to accelerate your AI and ML projects. Achieve high-quality, accurate annotations with our powerful AI-driven platform.\n\nChapters\n0:00 Introduction to Labellerr\n0:14 Annotating Objects\n0:25 Annotating Face with Hair\n1:19 Annotating Hat\n1:32 Activating Common Border\n1:44 Selecting Anchors for Common Border\n2:07 Completing Common Border\n2:12 Finalizing Hat Annotation\n2:50 Reviewing Annotated Objects\n3:00 Conclusion and Thank You\n\nStreamline your data labeling process with precision and speed.\nTry Labellerr today for seamless object annotation.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=N6zBdztpgFA",
    "embed_url": "https://www.youtube.com/embed/N6zBdztpgFA",
    "duration": 190,
    "view_count": 70,
    "upload_date": "20240523",
    "uploader": "Labellerr",
    "tags": [
      "image classification",
      "data labeling",
      "dataset creation",
      "visual recognition",
      "computer vision tasks",
      "deep learning models",
      "annotation workflow",
      "visual data",
      "polygon drawing",
      "annotation platforms",
      "dataset generation",
      "data visualization",
      "data annotation",
      "image annotation",
      "data annotation services",
      "data preprocessing",
      "polygon annotation",
      "video labeling",
      "deep learning",
      "machine learning",
      "image segmentation",
      "labeling software",
      "computer vision",
      "image labeling",
      "object detection"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=N6zBdztpgFA! This is most likely caused by:\n\nNo transcripts were found for any of the requested language codes: ('en',)\n\nFor this video (N6zBdztpgFA) transcripts are available in the following languages:\n\n(MANUALLY CREATED)\nNone\n\n(GENERATED)\n - hi (\"Hindi (auto-generated)\")\n\n(TRANSLATION LANGUAGES)\nNone\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "7-TG11HJhh0",
    "title": "Revolutionize Your AI with the Best Data Annotation Services | Labellerr",
    "description": "Looking for top-notch data annotation services to power your machine learning models? Labellerr offers accurate, scalable, and efficient solutions tailored to your needs.\n\nDiscover seamless annotation workflows and unmatched quality.\nExplore services that accelerate your AI projects.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=7-TG11HJhh0",
    "embed_url": "https://www.youtube.com/embed/7-TG11HJhh0",
    "duration": 371,
    "view_count": 47,
    "upload_date": "20240521",
    "uploader": "Labellerr",
    "tags": [
      "dataset creation",
      "object detection",
      "annotation platforms",
      "data labeling",
      "image tagging",
      "data management",
      "annotation software",
      "data labeling tools",
      "feature extraction",
      "image labeling",
      "photo annotation",
      "machine learning",
      "ml datasets",
      "data annotation",
      "data annotation services",
      "data labeling service",
      "image annotation",
      "machine learning annotation",
      "image classification",
      "data insight",
      "data preprocessing",
      "visual data",
      "computer vision",
      "image segmentation",
      "video labeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "in this demo I'll show you how you can segment using S and also edit uh the polygons that it's drawn or you can draw a polygon from scratch so let's start with selecting the object first we have to select the object then we have to see that this magic icon is appear I have to click on it then samam is selected I will click on inct now I have to click at one point and see and wait for the results okay let me try it one more time good I can stand it to okay now it looks I can zoom in and zoom out to see the complete image it looks decent so I'll accept this by clicking on item now I will deselect this so now I see it's white that means it's deselected right so if I have to need to edit it I will just click on this and it will appear like this so I can do some annotation adjustment let just by a mouse cck now let me Zoom zo in to see if the pixel Perfection is there so maybe I will start this can see that I res selected this nothing is selected now if I click on this will appear and then I can start moving it and if I have to go up I can do zoom out then Focus put my cursor where I want to zoom in and then with my mouse I will do the zoom in I want this to be included that's how I will do the adjustment like this here here here here here that's how I can this annotation that's how I can do the add if I have to draw The annotation manual so what I can do I will clear this previous annotation in One Click by clearing all this answer I can click here so that I can move my image now I will select the object but this time I will not select the S this time I will do the by clicking shift and dragging my mouse I can Mo this like this and by clicking on right click on the mouse if I have to go back I can undo each and every what say one by one so let's start with this so let me zoom in so that I can add the part I can do this by one by one clicking or I can press shift and it will and just move my mouse and it will do automatic whever I need some I can move that press shift key and starting now I have to go down this image si uh then I will zo out again I put my cursor to the area where I want to do annotation and then Zoom and continue my that's how we can quickly go annotation by zooming in zooming out with your mouse cursor I can capture every part of this with my mouse and at the end I can press n to close the if I need picture then again I can do the same thing just just by clicking",
    "transcript_chunks": [
      "in this demo I'll show you how you can segment using S and also edit uh the polygons that it's drawn or you can draw a polygon from scratch so let's start with selecting the object first we have to select the object then we have to see that this magic icon is appear I have to click on it then samam is selected I will click on inct now I have to click at one point and see and wait for the results okay let me try it one more time good I can stand it to okay now it looks I can zoom in and zoom out to see the complete image it looks decent so I'll accept this by clicking on item now I will deselect this so now I see it's white that means it's deselected right so if I have to need to edit it I will just click on this and it will appear like this so I can do some annotation adjustment let just by a mouse cck now let me Zoom zo in to see if the pixel Perfection is there so maybe I will start this can see that I res selected this nothing is selected now if I click on this will appear and then I can start moving it and if I have to go up I can do zoom out then Focus put my cursor where I want to zoom in and then with my mouse I will do the zoom in I want this to be included that's how I will do the adjustment like this here here here here here that's how I can this annotation that's how I can do the add if I have to draw The annotation manual so what I can do",
      "I will clear this previous annotation in One Click by clearing all this answer I can click here so that I can move my image now I will select the object but this time I will not select the S this time I will do the by clicking shift and dragging my mouse I can Mo this like this and by clicking on right click on the mouse if I have to go back I can undo each and every what say one by one so let's start with this so let me zoom in so that I can add the part I can do this by one by one clicking or I can press shift and it will and just move my mouse and it will do automatic whever I need some I can move that press shift key and starting now I have to go down this image si uh then I will zo out again I put my cursor to the area where I want to do annotation and then Zoom and continue my that's how we can quickly go annotation by zooming in zooming out with your mouse cursor I can capture every part of this with my mouse and at the end I can press n to close the if I need picture then again I can do the same thing just just by clicking"
    ],
    "transcript_word_count": 530,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "NozKRfucqU4",
    "title": "Garbage In Garbage Out Podcast | Labellerr: Your AI Powered Data Labeling Platform",
    "description": "\"Discover Labellerr, the leading AI powered data labeling platform accelerating machine learning workflows with precision and speed. In this episode of Garbage In Garbage Out, we talk to Keshav Bimbraw, Robotics Ph.D. from WPI, about his work in AI/ML for robotics, rehabilitation, and surgery. Learn how high-quality data labeling transforms AI projects.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\"",
    "video_url": "https://www.youtube.com/watch?v=NozKRfucqU4",
    "embed_url": "https://www.youtube.com/embed/NozKRfucqU4",
    "duration": 2703,
    "view_count": 127,
    "upload_date": "20240519",
    "uploader": "Labellerr",
    "tags": [
      "image labeling",
      "data insight",
      "computer vision labeling",
      "annotation workflow",
      "image classification",
      "computer vision",
      "data labeling tools",
      "visual data",
      "machine learning",
      "image tagging",
      "deep learning",
      "image segmentation",
      "data labeling",
      "data labeling service",
      "data labelling",
      "video data labeling",
      "data labeling and annotation",
      "data annotation",
      "dataset creation",
      "labeling software",
      "annotation software",
      "image annotation",
      "labeling solutions",
      "quality assurance",
      "video labeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] hey everyone uh good morning good evening to all folks out there uh into the AI space or robotic space um Medical Imaging uh so this is Punit um your host for today at this AI data podcast um welcome all of you uh so my name is Punit and then just to quickly introduce my myself uh I take care of product at labellerr labellerr is helping AI teams uh especially working in robotics uh help label their data uh quicker to ensure that improve their accuracy of Robotics that they're building and uh and today I'm excited to have a guest with me a very special guest and my alumini uh kha bbra right and welcome kab good to be here cool so so we will spend next 40 minutes with Kev on knowing more about his journey which could be uh I'm sure would be an inspiration for many of the students uh or researchers or aspiring researchers out there and also uh uh in opportunity for executives in different companies who would be looking uh to collaborate with folks like kab who are into robotics Medical Imaging so if that is an area of interest to all of you then I think uh you should uh continue to listen to our conversation right okay so Kev uh let's start with um quick uh Twitter pitch in ction of yourself all right cool so I'm KF uh I'm a PhD candidate in robotics at wer poly Technic Institute in Massachusetts in the US uh uh my my research focuses on uh um utilizing ultrasound uh mean medical ultrasound for controlling robotic systems overall uh so one of the things that I'm very interested in and one of the things which I'm exploring is how we can use ultrasound data from the forearm and use it to estimate what's happening in the hand and what's by what's happening in the hand I mean can we estimate different hand gestures can we estimate finger angles can we estimate forces that the hands apply U onto um different surfaces while manipulating different objects and then once we get these estimations we can use these to control different types of robotic systems so that's essentially what I work on okay cool and would you mind sharing that Twitter uh pitch in in some different words that you might want to Define oh yeah sure what's coming to your mind uh well uh let's see I guess I can give a little bit of my background as well so I did my undergrad in mechron engineering uh from taper University in patala uh in Punjab in India uh and then from there I uh uh moved on uh to Georgia Tech for my masters I uh I got a scholarship to pursue a Ms of Science in music technology U interesting at gech um and uh that's where I got deeper into robotics research a little bit of ultrasound and Medical Imaging U and essentially looking at some of the signals that our bodies produce that can be used to control robotic systems which is very interesting to me and then after graduating from jch I worked in industry for a year working on uh audio Digital Signal processing and after that I uh you know right when covid hit I transitioned to WPI uh to pursue a PhD in robotics and uh during my PhD uh I'm currently in my fourth year about to finish fourth year during my PhD I've explored a lot of uh machine learning deep learning uh computer vision image processing um um um in terms of the core technology but I've also explored like applications pertaining to Medical Imaging robotic control Etc so uh what inspired your transition from mechanical engineering to robotics if you can put more highight on that and then uh any specific experiences or projects that spark your interest yeah so uh um so I I uh I I actually did my undergrad in mechatronics so that's kind of a relatively new that was a relatively new course when I started it in 2013 um okay Ander and I would interrupt probably in between right so okay so so what was the decision probably to even select mechatronics right because this is a very um this is a very common question in the mind of students when they are actually going getting into uh undergrad right yeah um so and there is a long list uh oh there is a computer science there's electronics communication there is electrical civil mechanical and a lot more right yeah and then at that time many of the students not don't have the covered in their head that okay what exactly they need to do y all they have is some excitement for a couple of Technologies out there there MH and and then there is a parents decision making out there that okay you should do this because everyone else is also taking up some stream popular at that time in for that sort of colleges so what went into at your side in your journey to chose mechatronics yeah at to yeah uh that's a good question so uh by the way by my dad uh he did uh a degree in mechanical engineering and he is a businessman uh in Punjab um uh and uh so essentially my family wanted me to take mechanical engineering but okay I I was a little bit rebellious I would say and I was like eh I'm not sure if I want to take mechanical and then the other options were uh taking CS um EC uh mechatronics or electrical engineering um and so megatronics was very interesting to me when I read about megatronics Jing like what what is it uh um I I was like oh so you're not just F focusing on one aspect you're focusing on all aspects of a of a given system right so you're focusing it's kind of like you're focusing on uh a more of a system perspective as opposed to that okay uh I'm going to hire a a a 3D uh designer mechanical engineer to design uh something and then I'm going to hire an EC person to design the electronics of it I'm going to hire some e person to design the electrical part of it and then I'm going to hire some uh U some computer scientists to um write the code to control it um but I I think giving a systems perspective where one person can understand all aspects of the system that's what mecatronics does and so that overall was very interesting to me and mecatronics has a direct link to Robotics and Robotics was something which was always fascinating to me um so so yeah despite my parents insistence on that okay you need to do mechanical engineering mechanical engineering I decided that I wanted to do mechatronics okay okay was there any anyone else who influenced the decision in your family yeah there was this one relative that I had um he was like a very like uh let's just say very open thinker my brother-in-law he uh I I talked to him about it and after like a long discussion he was like hm this could be this is really like the future because we have these core streams um and these score streams have been there for a while and yes there's some amazing amazing things happening in these these streams but eventually there would be a need for people who can bring things together so that people are not just working in isolation and so that also really influence my decision okay I mean one very important question before we come back to the the first question right um so so while taking this Choice yeah uh this question comes um because at that point in time you really don't know what's going to be covered exactly so for you at that time it it was probably some mechanical engineering some Robotics and some combination of couple of things as from the name you really don't know what's going to cover right uh what's going to be covered and uh and the key question while choosing all of this is like oh will I get placed in the best company at that point in time is usually the way I have seen uh student selecting or in general the decision making happen right so did this come to your mind as part of the selection process or not in your that did not come to my mind if if that would have been the case I would have chosen CS because like my rank was enough for me to get into CS as well um uh but I I really did not uh think about that because you know I come from a business family so there's my like my dad's business as well so I can always take over and you know work there right um uh so um what I wanted to do was I wanted to do something which was interesting to me and so that's why I you know I I I I say this like I really enjoyed my undergrad experience um in terms of the growth that happened in me um and and it's a major uh and what motivated me to come to the US okay cool cool so just a uh for for audience right mean for students uh listening to all this right uh all of the conversation um so if if you want to make your undergrad interesting on the on the academic side as well apart from the fun that we have in in the college right uh choose a stream which which really uh gives you a kick uh you know which is something that you're passionate about and then go for it right uh instead of just seeking uh just by basing that decision just based on the the number of placements or the number of active offers that in the past that college has right cool uh so kha coming back to the the the the orig the key question that we have right so so you you got into the me mechatronics at taer right so that was a kind of setting Stone uh for your journey to towards this field right in terms of the robotic right and any other things probably some projects or what what else sort of things happened across this timeline at undergrad which really uh pushed you in the direction of your your objectives of pursuing research all right so so um um so I I um I I think every undergrad in taper like the freshers as they called it the first year people they have to take a mandatory Physics course and so in in in that course um I had a professor Dr Punam unal uh and um she um so by the way I I grew up learning music uh uh Indian classical music so I was always really into um um Acoustics but I did not understand the physics behind it so in that Cas that course was the first time that I actually understood that oh you know this is the physics behind like Acoustics and Echoes um um um I remember there was like this term called Sabine which is used to measure the density of sound uh I'm probably messing some things up here but um that was like a u that was very interesting to me is what I'm saying so I reached out to uh the professor and I was like wow I'm very interested in the course I really like how you teach and uh um um and she told me that oh okay like if you like how I teach and if you like this course then you should look at my research so it was she who he kind of like planted that seed of research and when I went to her lab and uh and she used to give me that okay we have this system and I want you to understand it um like these electronic systems for characterization of some materials right um and so uh that was like a starting point of like getting into research for me where there is a lot of uh there's not a set path that A B C D no you have to explore you will fail a lot but eventually you'll have a better understanding of the problem than when you started right uh and so that kind of a mindset like I would give her the credit and then there was this other professor at toer uh Dr karanjit Kima and she was in computer science actually uh I think she was doing her PhD there but she was also a lecturer there and I went to her as well um she was very approachable very kind and I was like uh you know ma'am I want to do some research and she suggested that okay how about you write a review paper because that helps a lot of phds and a lot of and this was first year uh first year yes it was the it was the summer uh at the end of the first year so I would say maybe around U um um April or or may okay and so um so so I wrote a re paper with her and and she helped me throughout the process and she was like uh so uh like I I just completely admire her because she usually people when they help someone with their research they want uh their name on the paper but she told me that oh this is your work so you do not even need to put me as an author her and so that was like really magnanimous of her to do that and then she also helped me get some funding to actually travel to uh to France and to present uh uh that paper U and so so that that really opened my eyes and I remember when I went to France I also stopped by Switzerland and I went to ET at Zurich to this lab that I really like this robotics lab that I really like I talked to the professor there because I wanted an internship even though that didn't work out it was just good talking to these like like hire up people and um my experience interacting with people during the conference in France was really good and so when I came back I was just very pumped uh and I was like you know what this is what I want to do like this is my calling I want to do research and then from that paper um and because of like another professor uh at topper um um he he recommended me to this other uh to this to actually the head of Department of Robotics at IIT Delhi and so I I then worked there uh for six months uh and I got like two papers out of it it was a very intense environment but it was very uh it was a very productive time for me um and I worked on a lot of projects and it was again like a period of immense growth and it was there that I actually designed this project uh okay um um because I was very inspired by this lab at gech which was designing robots which would play music which is very weird thing to do you know and um and so I told uh Dr Saha at IID Delhi that okay I'm working on these core projects but can I work on the side projects you have these two robots here I want to use these two robots to play a guitar right and he agreed and uh um so I I made a demonstration I actually played the Indian national anthem with those two robots and oh yeah I programmed them and and then sent that mail to this guy at Georgia Tech U and he was very impressed and so we had a conversation and he was like okay you can come here I'm going to give you scholarship and everything so that's how I uh got to Georg Tech so that that's how my journey went through my whole underground oh so so you use robots to play national anthem yeah on a guitar on a guitar yeah you should have so did you make a video out of it or something like that yes is it is it online right I mean people can go and and watch it okay we would definitely take that link and then put that on the on the you know YouTube along with this cool that will be interesting because I I think that that's something uh quite inspiring and and really interesting Direction on how you actually U used your um in interest in music and Robotics to actually showcase your uh patriotism cool uh so so this is this is how you landed up at Georgia Tech right and with scholarship yeah and okay and then uh from from for example so these are the couple of projects and I think I think you also mentioned about U uh with with Dr Punam right you kind of did did research as well yeah and so and then uh and when is this journey I I I saw your I went through your this whole journey which you shared on LinkedIn as well and uh you mentioned that you worked with inter turn with Nokia Bell Labs Mr Bish and Google research right were they after Georgia Tech and before your PhD or like how how when in the timeline all this happened if you can put light on that yeah and and and how you got into these internships yeah so all of these happened during my PhD uh during your PhD yeah so um the first thing that happened was uh Google had I'm not sure if they still have this they had this research mentorship program uh it's very selective um um and so I applied uh to that program uh in at the end of the first year of my PhD um okay and uh uh thankfully I got in I was the first person from my school here at WPI to to get into that program and so I was actually mentored by one of the directors of research at Google and so that was a very uh eye opening experience to be you can share the name would you like to share the name not I I I think yeah maybe uh um um I would I would rather not share the name because I because I didn't like share this in my profile like I I probably yeah so um um and and so like that really helped me get inside into you know big big Tech right um which previously was something that I was not really looking that deeply into and I know like there's a lot of people especially like in the core CS world who are who probably know tons about that more than what I do but that was like my insight into that whole whole bitte world um and so uh uh at the end of the second year of my PhD uh that's when I U applied to no Labs I actually applied to no Bell Labs while I was visiting India uh my parents uh and then I got an interview uh and then uh I finally accepted the offer um and uh it was a very good experience working at no labs this was in New Jersey uh and uh um uh and I worked with Dr Ming dang uh and I essentially worked on taking signals from the body and using it to design systems that can be used to control robotic uh systems okay in a in a layman in I have to understand if you can show if you can elaborate on that yeah yeah for sure so so what I worked on at Nokia was um um um so you know the way our we control our hands is it's very interesting it's through electrical signals so our brain sends electrical signals which go through our muscles to finally our fingers or to our hands or to our limbs in general and it's very interesting that when you move your hand a certain way let's say if I'm making a fist or let's if I'm raising a finger there's a separate signature to each one of these electrical signals and so that can be classified and so we can actually classify these signals which are going through the forearm okay esate different gestures that the hands are doing oh so so essentially you can you can use a sensor like that to estimate your different hand gers and you can design a system that can estimate different hand gestur and then uh one of the other sensors that you can use is you can use accelerometers you can use inertial measurement unit sensors again in Len terms there are some sensors which can give an estimation of uh the position and orientation of an object so when you put these sensors on your arm okay you can actually train models which you can then map onto a robotic arm so the robotic arm can do exactly what your arm is doing so like this will also do exactly the same thing and if I'm opening my hand robotic arms gripper would do the same thing so what they were interested in what uh mingi was interested in was um um um working on it more from an industrial robotics uh Viewpoint and so uh that's what I worked on uh there and and I think I think you mentioned other one more internship it's Mishi research right yes yes and what was that was it also similar related to robotics it was it was yes it was also related to robotics but I um so I'm actually going to be working for Mitsubishi again this summer um so I worked with um um Toshi son there um and he uh he's an expert he has like tons of patents and extremely good research and they do a lot of uh very good signal processing research there um like they are experts in machine learning and everything like I when I go there I I'm actually a bit overwhelm because of how much of an export are you know there's like these phds from MIT they also working and who are brilliant absolutely brilliant so um um um so what I worked on there was multimodal U uh signal bodily signal analysis and let me explain that in layen terms so so usually um so we have established that you know there are different signals that our bodies uh generate can be captured and which can be used to estimate what's happening in the body or what's happening in the hands right um like for a lot of cases you would want your hands to be free so you you would not want to wear gloves or you would not want to restrain the movement of your hand if you want to control something right so what you do you instead bring all the sensors to the arm and so along the same line um at at mitubishi what I worked on and the reason I'm talking about this is because U we already have a paper out on this so I can uh u i I can talk in in depth about it both with Nokia and mitsuishi I'm only giving out information which is out in the public domain because they have very strict uh non-disclosure agreements um um so uh at Mitsubishi what I worked on was that when you have a bunch of sensors and they are sending data um um and then this data can be used to estimate different gestures um how do we make the system more robust and by that what I mean is like very often times some of the sensors stop working or they or they start giving bad data so how do we how do we actually train uh our deep learning or machine learning models so that um despite bad data the models perform well so that that is something that I investigated uh during my internship last summer yep all right so U some some question uh a little bit out of the context of your journey but more around like a sort of fun question right so if you were to build a robot companion to assist you in your daily life uh what would its name be and what task would you interest it with yeah um I think uh that's a fun question so in terms of the name uh I'd probably choose like a Sanskrit name because um I don't know something like a robo yantra or something because I'm really into you know robra yeah something like that um and and so since I know so much about robotics because I've been in the field for I think and I'm not exaggerating maybe like 10 years now so uh um um I know what is capable and I know what is just like BS because a lot of people they overstate the capabilities of robots you know so if it is something which is more uh um I an articulated anthropomorphic and sorry for these terms but essentially what I mean is like it's something which looks like a human you know then I then what I would address them with is like some very basic tasks which have no contact with a human so let's say um um you know ironing my clothes or something and but I will know like I will have to lay the lay everything down and then the robot is going to iron it um probably like I can probably program a robot to cook something very simple for me you know like maybe a p or something something right it would be great if a robot can do it in a very optimized fashion which I know which is capable with the robotic systems U and and so pretty much like activities of daily living is what I would entrust robots with which do not have any direct contact with me because I know that robotic systems they're not compliant enough and by compliant I mean like like they can hurt you robots can hurt you based on how you program them because they are viget structures and we not have very Advanced soft robotic stres there's a whole field of soft robots but we do not that's not very Advanced right now okay that's interesting so so okay that and that leads to a another followup question right so you are saying that there is a area where uh you in your understanding uh the Soft Robotics has to come up uh because they are not compliant the current way of Robotics uh and so and you are saying that if I ask you right 10 years from now would that be possible or would that be possible that these robot would be able to of collaborate with humans that would be safer yeah like that yeah I think that's a very good question uh and um I first started to understand what that meant during this internship that I did at I Delhi during my undergrad where you know I was designing this robot that would play guitar we do not want the robot to press the guitar too hard or the guitar would break right so uh and also um um so there were two robots one robot was pressing the notes the other one was tucking the strings you also don't want the one which was plucking the strings to pluck it too hard or the string would break yeah right so that's where I tried I got an intuition of that okay compliance is very important and we need to set the amount of force that the robot is applying so now to to answer your question um I think we are heading in a direction where uh um a lot of robotic systems are um going towards a direction of giving some sort of a feedback to the human which can be then used to design safer systems right so um um um and and that's also something that I have personally looked into through my research as well um how we can get a good feedback from the robot so that they can interact with objects in a better way and so that they can also like not hurt humans or any other objects um um so so that's one thing and another thing which is potentially a lot of people are working on is using techniques like reinforcement learning Etc um that really helps you you know provided you give it a lot of good demonstration U you can you can train a policy which can be then used to interact with humans in a better and a safer way incrementally you keep on improving a model or you keep on improving a a policy um um and so because of all the advancement that's happening in the Deep learning robots are becoming safer and safer day by day so I can Envision maybe 10 20 years in the future we will have very safe robots so in your opinion do you think that the with let's say you you mentioned about reinforcement learning right reinforcement learning with demonstrations would make robots safer but then in your opinion uh are these robot systems really learning at the core that what is safety here right yeah and that's yeah that's a very good question that's a very philosophical question actually uh what is learning actually there like what is actually learning because what you see like a physical robot something that you would see in a movie like I Robot or something a robot is one thing that I've learned is like robot is not like an intelligent being it's a very dump machine you have to literally tell it everything that you need it to do right so keeping U that in mind your robot is just as good as your model or just as good as your learned policy right right so um if your policy is improving and there's some great research from nvdia from some other places U that I've seen um um that they have been able to train robotic systems to be uh essentially better at doing tasks um so if you can do that and if you have some of course like some safety controls there you know U then robots will become better and by robots becoming better I mean the models underlying them will become better okay okay so I mean that's interesting I mean the ways in which robotics can help us assist humans been talking to couple of folks around Marine engineering site or even even to some of the companies who are actually doing using underwater btics to detect mammals and even and and other objects or species um to to plan the safety of those species and and their navigation Etc um including fishing and and other areas so and and what's your goal finally to get into any specific application areas is there is there an area where you are getting into with robotics yeah so I'm personally very interested in the health related aspect of Robotics okay like the kind of like the unit that I work in the my lab is in the medical robotics department at w and so something along those lines so one of the more direct directions that I uh am pursuing and I wish to pursue in the future would be surgical robotics so surg yeah yeah why particularly surgical robotics essentially humans are error prone right and surgery is a very precise um um um activity intervention or or of the human body right and so I I would really like to contribute to that and and U and when you say surgical robotics right uh now what all that could be so the surgery so basically this you're saying that in a live surgery these robots would assist uh the the the experts you know the expert person uh doing their surgery and U during during that particular life surgery right so that's what you're saying so and what all that activities would be like could be yeah so there's two things here one is that we train models that can help during surgery so it's not essentially like a physical robot that is helping it but some deep learning models that we have trained which are helping with different tasks like navigation or um um um removing certain tissues from the body uh okay cutting certain portions looking at healing all those type of Interventional things right that's one aspect where where physical robots are not involved but then there's another respect as well where physical robots are involved so one big thing there and that's something that I looked into is that a lot of the times uh we do want to have a physician do a surgery remotely right and so robot can be very helpful there right another use Cas is that humans we Physicians they're great they have years of experience um but they do get tired right and so when you're tired like it's hard to do that so if you're sitting somewhere and if you're just moving your fingers and you're just doing your surgery compared to if you're standing on your operating table and then you're trying to do it you'll get more error there compared to like if you're sitting somewhere comfortably and then you're doing it so like there are robotic surgical robotic systems out there like DaVinci system from intuitive surgical metronic has a there's a lot of companies which have these surgical robotics system especially us especially is really adopting it I know a lot of medical institutions nowadays they just for the sake of it they will just buy like a very expensive surgical robot to say that okay we can also do robotic surgery no I think I think that's a very nice uh analogy that you made to understand like why uh robot uh is going is is actually needed for so so heavy Precision task actually it's going to be much much more valuable yes so uh another question uh we are running on short short of time but then few questions left but if you can address in short that will also be helpful okay so uh so what advice would you give to students considering pursuing a PhD in robotics especially those who are transitioning from undergrad to Graduate Studies right now got it got it so one advice I would give them especially let's say if you're in early years of your undergrad definitely do some research it it will make you a better PhD candidate you do not have to have research papers in order to get into a program where you'll be a good fit but it certainly helps uh so that's one advice that I would give people and and also um uh I would say like like do your research do not just get into a PhD for the sake of it like from my conversation you would have seen that like I really like or enjoy what I do which is why I'm doing it and so it has to be that otherwise PhD is a very long journey it's a very frustrating Journey you can be miserable and you'll be like why did I do this especially in the US a lot of phds just drop out like I think it's it's a very high percentage so that's what I would say like do your research um in terms of where you want to go where you'll be a good fit and do some actual research which will be helpful thanks thanks K uh so and and uh from the other angle as as well uh so there are a couple of misconceptions about Robotics and AI um that new Commerce coming to the field has any one common misconception that you would like to share yeah that it's it's kind of like the misconception that I had about coding uh so I initially used to struggle a lot with coding because I was like oh it's something too complicated I cannot get it but essentially it's it's all math and it's it's not like some very hardcore you know calculus something which is too deeply technical uh I mean it is technical but the mathematical principles align uh underlying it it's it's relatively simple thanks so Kev what is your next three to five year goals and any message for companies who might looking to collaborate with you okay so over the next three to five years my plan is uh well first would be to graduate and then to find like a a good place for me to go to um um and and and what that would be is like uh uh somewhere where I can do some research but I can also contribute towards like developing a product I think I've been in Academia for a while now so uh I would definitely like to go somewhere where I can have more of an impact um um hopefully stay staying in somewhat like a similar area where I've been working on but I'm pretty open to you know taking up something which is relatively new where my skills can be useful okay something around industry Academia collaboration or something like that is that the kind of role that you are kind of envisioning uh I I guess I guess uh what I'm envisioning is something in Industry which has more of a academic environment and uh um and there's there's very few places which have that um like I know mitsuishi uh has something similar like Mitsubishi Electric research Labs there the culture is kind of like that uh okay so bonus question for the audience um any recommended resources on the internet some blogs or YouTube channels that you follow and admire uh especially uh those who are right now um first going to pursue or looking to pursue their PhD into robotic side um so that they can actually spend more time on those and then probably might that might answer their question yeah yeah uh I'm just trying to think about this because right now the resources that I look at are very different from what I used to look at when I was doing my undergrad and I was trying to transition like right now I mostly look at Google Scholar and I look at research of people and that is what gives me uh my Direction um and that's what I admire right now but before there's a ton of like uh um very good U um YouTube channels uh where you can pretty much learn anything like uh I I do not have anything off the top of my head because I've not been very aggressive on YouTube lately okay so um um so I cannot name that but I honestly think nowadays doing a quick Google search and that that that works for me that that makes scholar research there so many research papers even if I write then there so much that will come up how to Zone down into um particular paper if you can highlight some one or two pointers that yeah you know um in one thing that I learned during my PhD uh so I'm I'm I'm a very H I'm someone who likes through a lot of things all the time so uh which is not really uh which can get a little bit messy so one thing that I learned during the course of my PhD and this is a very cliche thing to say in the US especially is to have smart goals where I think s stands for specific m stands for measurable a I do not remember the whole thing but if there's one takeaway from that smart thing it's specific that thing has really helped me where I'm like okay this is what I want to achieve so I need to be very specific about it and that's what would help you with your search as well okay cool that's wonderful uh K we would I think we can go on and on uh with as uh you know it's it's time to kind of wrap it up and then U so and and that's where I mean I would love to thank you bottom of my heart right I mean to kind of share this motivational Journey there are a couple of things that particularly students have uh to learn through this uh conversation right and and get inspired and get motivated and I hope that uh you don't mind if uh anyone listening to this podcast might reach out to you once they go through this right and uh and uh yeah so thank you thank you KV for your time and uh All Right audience I mean so I I think you all must have enjoyed this conversation right uh we will we will uh put those links in the YouTube so I so while you are watching this video uh you can go through the comments and description and we'll try to summarize and we have tried to summarize this all these points here um and and yeah thank you thank you uh for this conversation and time K yeah for sure and it was great uh talking to you",
    "transcript_chunks": [
      "[Music] hey everyone uh good morning good evening to all folks out there uh into the AI space or robotic space um Medical Imaging uh so this is Punit um your host for today at this AI data podcast um welcome all of you uh so my name is Punit and then just to quickly introduce my myself uh I take care of product at labellerr labellerr is helping AI teams uh especially working in robotics uh help label their data uh quicker to ensure that improve their accuracy of Robotics that they're building and uh and today I'm excited to have a guest with me a very special guest and my alumini uh kha bbra right and welcome kab good to be here cool so so we will spend next 40 minutes with Kev on knowing more about his journey which could be uh I'm sure would be an inspiration for many of the students uh or researchers or aspiring researchers out there and also uh uh in opportunity for executives in different companies who would be looking uh to collaborate with folks like kab who are into robotics Medical Imaging so if that is an area of interest to all of you then I think uh you should uh continue to listen to our conversation right okay so Kev uh let's start with um quick uh Twitter pitch in ction of yourself all right cool so I'm KF uh I'm a PhD candidate in robotics at wer poly Technic Institute in Massachusetts in the US uh uh my my research focuses on uh um utilizing ultrasound uh mean medical ultrasound for controlling robotic systems overall uh so one of the things that I'm very interested in and one of the things which I'm exploring is how we can",
      "use ultrasound data from the forearm and use it to estimate what's happening in the hand and what's by what's happening in the hand I mean can we estimate different hand gestures can we estimate finger angles can we estimate forces that the hands apply U onto um different surfaces while manipulating different objects and then once we get these estimations we can use these to control different types of robotic systems so that's essentially what I work on okay cool and would you mind sharing that Twitter uh pitch in in some different words that you might want to Define oh yeah sure what's coming to your mind uh well uh let's see I guess I can give a little bit of my background as well so I did my undergrad in mechron engineering uh from taper University in patala uh in Punjab in India uh and then from there I uh uh moved on uh to Georgia Tech for my masters I uh I got a scholarship to pursue a Ms of Science in music technology U interesting at gech um and uh that's where I got deeper into robotics research a little bit of ultrasound and Medical Imaging U and essentially looking at some of the signals that our bodies produce that can be used to control robotic systems which is very interesting to me and then after graduating from jch I worked in industry for a year working on uh audio Digital Signal processing and after that I uh you know right when covid hit I transitioned to WPI uh to pursue a PhD in robotics and uh during my PhD uh I'm currently in my fourth year about to finish fourth year during my PhD I've explored a lot of uh machine learning deep learning",
      "uh computer vision image processing um um um in terms of the core technology but I've also explored like applications pertaining to Medical Imaging robotic control Etc so uh what inspired your transition from mechanical engineering to robotics if you can put more highight on that and then uh any specific experiences or projects that spark your interest yeah so uh um so I I uh I I actually did my undergrad in mechatronics so that's kind of a relatively new that was a relatively new course when I started it in 2013 um okay Ander and I would interrupt probably in between right so okay so so what was the decision probably to even select mechatronics right because this is a very um this is a very common question in the mind of students when they are actually going getting into uh undergrad right yeah um so and there is a long list uh oh there is a computer science there's electronics communication there is electrical civil mechanical and a lot more right yeah and then at that time many of the students not don't have the covered in their head that okay what exactly they need to do y all they have is some excitement for a couple of Technologies out there there MH and and then there is a parents decision making out there that okay you should do this because everyone else is also taking up some stream popular at that time in for that sort of colleges so what went into at your side in your journey to chose mechatronics yeah at to yeah uh that's a good question so uh by the way by my dad uh he did uh a degree in mechanical engineering and he is a businessman uh in Punjab um",
      "uh and uh so essentially my family wanted me to take mechanical engineering but okay I I was a little bit rebellious I would say and I was like eh I'm not sure if I want to take mechanical and then the other options were uh taking CS um EC uh mechatronics or electrical engineering um and so megatronics was very interesting to me when I read about megatronics Jing like what what is it uh um I I was like oh so you're not just F focusing on one aspect you're focusing on all aspects of a of a given system right so you're focusing it's kind of like you're focusing on uh a more of a system perspective as opposed to that okay uh I'm going to hire a a a 3D uh designer mechanical engineer to design uh something and then I'm going to hire an EC person to design the electronics of it I'm going to hire some e person to design the electrical part of it and then I'm going to hire some uh U some computer scientists to um write the code to control it um but I I think giving a systems perspective where one person can understand all aspects of the system that's what mecatronics does and so that overall was very interesting to me and mecatronics has a direct link to Robotics and Robotics was something which was always fascinating to me um so so yeah despite my parents insistence on that okay you need to do mechanical engineering mechanical engineering I decided that I wanted to do mechatronics okay okay was there any anyone else who influenced the decision in your family yeah there was this one relative that I had um he was like a very like uh let's",
      "just say very open thinker my brother-in-law he uh I I talked to him about it and after like a long discussion he was like hm this could be this is really like the future because we have these core streams um and these score streams have been there for a while and yes there's some amazing amazing things happening in these these streams but eventually there would be a need for people who can bring things together so that people are not just working in isolation and so that also really influence my decision okay I mean one very important question before we come back to the the first question right um so so while taking this Choice yeah uh this question comes um because at that point in time you really don't know what's going to be covered exactly so for you at that time it it was probably some mechanical engineering some Robotics and some combination of couple of things as from the name you really don't know what's going to cover right uh what's going to be covered and uh and the key question while choosing all of this is like oh will I get placed in the best company at that point in time is usually the way I have seen uh student selecting or in general the decision making happen right so did this come to your mind as part of the selection process or not in your that did not come to my mind if if that would have been the case I would have chosen CS because like my rank was enough for me to get into CS as well um uh but I I really did not uh think about that because you know I come from a business family so",
      "there's my like my dad's business as well so I can always take over and you know work there right um uh so um what I wanted to do was I wanted to do something which was interesting to me and so that's why I you know I I I I say this like I really enjoyed my undergrad experience um in terms of the growth that happened in me um and and it's a major uh and what motivated me to come to the US okay cool cool so just a uh for for audience right mean for students uh listening to all this right uh all of the conversation um so if if you want to make your undergrad interesting on the on the academic side as well apart from the fun that we have in in the college right uh choose a stream which which really uh gives you a kick uh you know which is something that you're passionate about and then go for it right uh instead of just seeking uh just by basing that decision just based on the the number of placements or the number of active offers that in the past that college has right cool uh so kha coming back to the the the the orig the key question that we have right so so you you got into the me mechatronics at taer right so that was a kind of setting Stone uh for your journey to towards this field right in terms of the robotic right and any other things probably some projects or what what else sort of things happened across this timeline at undergrad which really uh pushed you in the direction of your your objectives of pursuing research all right so so um um so I",
      "I um I I think every undergrad in taper like the freshers as they called it the first year people they have to take a mandatory Physics course and so in in in that course um I had a professor Dr Punam unal uh and um she um so by the way I I grew up learning music uh uh Indian classical music so I was always really into um um Acoustics but I did not understand the physics behind it so in that Cas that course was the first time that I actually understood that oh you know this is the physics behind like Acoustics and Echoes um um um I remember there was like this term called Sabine which is used to measure the density of sound uh I'm probably messing some things up here but um that was like a u that was very interesting to me is what I'm saying so I reached out to uh the professor and I was like wow I'm very interested in the course I really like how you teach and uh um um and she told me that oh okay like if you like how I teach and if you like this course then you should look at my research so it was she who he kind of like planted that seed of research and when I went to her lab and uh and she used to give me that okay we have this system and I want you to understand it um like these electronic systems for characterization of some materials right um and so uh that was like a starting point of like getting into research for me where there is a lot of uh there's not a set path that A B C D no you have",
      "to explore you will fail a lot but eventually you'll have a better understanding of the problem than when you started right uh and so that kind of a mindset like I would give her the credit and then there was this other professor at toer uh Dr karanjit Kima and she was in computer science actually uh I think she was doing her PhD there but she was also a lecturer there and I went to her as well um she was very approachable very kind and I was like uh you know ma'am I want to do some research and she suggested that okay how about you write a review paper because that helps a lot of phds and a lot of and this was first year uh first year yes it was the it was the summer uh at the end of the first year so I would say maybe around U um um April or or may okay and so um so so I wrote a re paper with her and and she helped me throughout the process and she was like uh so uh like I I just completely admire her because she usually people when they help someone with their research they want uh their name on the paper but she told me that oh this is your work so you do not even need to put me as an author her and so that was like really magnanimous of her to do that and then she also helped me get some funding to actually travel to uh to France and to present uh uh that paper U and so so that that really opened my eyes and I remember when I went to France I also stopped by Switzerland and I went to",
      "ET at Zurich to this lab that I really like this robotics lab that I really like I talked to the professor there because I wanted an internship even though that didn't work out it was just good talking to these like like hire up people and um my experience interacting with people during the conference in France was really good and so when I came back I was just very pumped uh and I was like you know what this is what I want to do like this is my calling I want to do research and then from that paper um and because of like another professor uh at topper um um he he recommended me to this other uh to this to actually the head of Department of Robotics at IIT Delhi and so I I then worked there uh for six months uh and I got like two papers out of it it was a very intense environment but it was very uh it was a very productive time for me um and I worked on a lot of projects and it was again like a period of immense growth and it was there that I actually designed this project uh okay um um because I was very inspired by this lab at gech which was designing robots which would play music which is very weird thing to do you know and um and so I told uh Dr Saha at IID Delhi that okay I'm working on these core projects but can I work on the side projects you have these two robots here I want to use these two robots to play a guitar right and he agreed and uh um so I I made a demonstration I actually played the Indian national anthem",
      "with those two robots and oh yeah I programmed them and and then sent that mail to this guy at Georgia Tech U and he was very impressed and so we had a conversation and he was like okay you can come here I'm going to give you scholarship and everything so that's how I uh got to Georg Tech so that that's how my journey went through my whole underground oh so so you use robots to play national anthem yeah on a guitar on a guitar yeah you should have so did you make a video out of it or something like that yes is it is it online right I mean people can go and and watch it okay we would definitely take that link and then put that on the on the you know YouTube along with this cool that will be interesting because I I think that that's something uh quite inspiring and and really interesting Direction on how you actually U used your um in interest in music and Robotics to actually showcase your uh patriotism cool uh so so this is this is how you landed up at Georgia Tech right and with scholarship yeah and okay and then uh from from for example so these are the couple of projects and I think I think you also mentioned about U uh with with Dr Punam right you kind of did did research as well yeah and so and then uh and when is this journey I I I saw your I went through your this whole journey which you shared on LinkedIn as well and uh you mentioned that you worked with inter turn with Nokia Bell Labs Mr Bish and Google research right were they after Georgia Tech and before your",
      "PhD or like how how when in the timeline all this happened if you can put light on that yeah and and and how you got into these internships yeah so all of these happened during my PhD uh during your PhD yeah so um the first thing that happened was uh Google had I'm not sure if they still have this they had this research mentorship program uh it's very selective um um and so I applied uh to that program uh in at the end of the first year of my PhD um okay and uh uh thankfully I got in I was the first person from my school here at WPI to to get into that program and so I was actually mentored by one of the directors of research at Google and so that was a very uh eye opening experience to be you can share the name would you like to share the name not I I I think yeah maybe uh um um I would I would rather not share the name because I because I didn't like share this in my profile like I I probably yeah so um um and and so like that really helped me get inside into you know big big Tech right um which previously was something that I was not really looking that deeply into and I know like there's a lot of people especially like in the core CS world who are who probably know tons about that more than what I do but that was like my insight into that whole whole bitte world um and so uh uh at the end of the second year of my PhD uh that's when I U applied to no Labs I actually applied to no Bell Labs",
      "while I was visiting India uh my parents uh and then I got an interview uh and then uh I finally accepted the offer um and uh it was a very good experience working at no labs this was in New Jersey uh and uh um uh and I worked with Dr Ming dang uh and I essentially worked on taking signals from the body and using it to design systems that can be used to control robotic uh systems okay in a in a layman in I have to understand if you can show if you can elaborate on that yeah yeah for sure so so what I worked on at Nokia was um um um so you know the way our we control our hands is it's very interesting it's through electrical signals so our brain sends electrical signals which go through our muscles to finally our fingers or to our hands or to our limbs in general and it's very interesting that when you move your hand a certain way let's say if I'm making a fist or let's if I'm raising a finger there's a separate signature to each one of these electrical signals and so that can be classified and so we can actually classify these signals which are going through the forearm okay esate different gestures that the hands are doing oh so so essentially you can you can use a sensor like that to estimate your different hand gers and you can design a system that can estimate different hand gestur and then uh one of the other sensors that you can use is you can use accelerometers you can use inertial measurement unit sensors again in Len terms there are some sensors which can give an estimation of uh the position and",
      "orientation of an object so when you put these sensors on your arm okay you can actually train models which you can then map onto a robotic arm so the robotic arm can do exactly what your arm is doing so like this will also do exactly the same thing and if I'm opening my hand robotic arms gripper would do the same thing so what they were interested in what uh mingi was interested in was um um um working on it more from an industrial robotics uh Viewpoint and so uh that's what I worked on uh there and and I think I think you mentioned other one more internship it's Mishi research right yes yes and what was that was it also similar related to robotics it was it was yes it was also related to robotics but I um so I'm actually going to be working for Mitsubishi again this summer um so I worked with um um Toshi son there um and he uh he's an expert he has like tons of patents and extremely good research and they do a lot of uh very good signal processing research there um like they are experts in machine learning and everything like I when I go there I I'm actually a bit overwhelm because of how much of an export are you know there's like these phds from MIT they also working and who are brilliant absolutely brilliant so um um um so what I worked on there was multimodal U uh signal bodily signal analysis and let me explain that in layen terms so so usually um so we have established that you know there are different signals that our bodies uh generate can be captured and which can be used to estimate what's happening",
      "in the body or what's happening in the hands right um like for a lot of cases you would want your hands to be free so you you would not want to wear gloves or you would not want to restrain the movement of your hand if you want to control something right so what you do you instead bring all the sensors to the arm and so along the same line um at at mitubishi what I worked on and the reason I'm talking about this is because U we already have a paper out on this so I can uh u i I can talk in in depth about it both with Nokia and mitsuishi I'm only giving out information which is out in the public domain because they have very strict uh non-disclosure agreements um um so uh at Mitsubishi what I worked on was that when you have a bunch of sensors and they are sending data um um and then this data can be used to estimate different gestures um how do we make the system more robust and by that what I mean is like very often times some of the sensors stop working or they or they start giving bad data so how do we how do we actually train uh our deep learning or machine learning models so that um despite bad data the models perform well so that that is something that I investigated uh during my internship last summer yep all right so U some some question uh a little bit out of the context of your journey but more around like a sort of fun question right so if you were to build a robot companion to assist you in your daily life uh what would its name be",
      "and what task would you interest it with yeah um I think uh that's a fun question so in terms of the name uh I'd probably choose like a Sanskrit name because um I don't know something like a robo yantra or something because I'm really into you know robra yeah something like that um and and so since I know so much about robotics because I've been in the field for I think and I'm not exaggerating maybe like 10 years now so uh um um I know what is capable and I know what is just like BS because a lot of people they overstate the capabilities of robots you know so if it is something which is more uh um I an articulated anthropomorphic and sorry for these terms but essentially what I mean is like it's something which looks like a human you know then I then what I would address them with is like some very basic tasks which have no contact with a human so let's say um um you know ironing my clothes or something and but I will know like I will have to lay the lay everything down and then the robot is going to iron it um probably like I can probably program a robot to cook something very simple for me you know like maybe a p or something something right it would be great if a robot can do it in a very optimized fashion which I know which is capable with the robotic systems U and and so pretty much like activities of daily living is what I would entrust robots with which do not have any direct contact with me because I know that robotic systems they're not compliant enough and by compliant I mean like",
      "like they can hurt you robots can hurt you based on how you program them because they are viget structures and we not have very Advanced soft robotic stres there's a whole field of soft robots but we do not that's not very Advanced right now okay that's interesting so so okay that and that leads to a another followup question right so you are saying that there is a area where uh you in your understanding uh the Soft Robotics has to come up uh because they are not compliant the current way of Robotics uh and so and you are saying that if I ask you right 10 years from now would that be possible or would that be possible that these robot would be able to of collaborate with humans that would be safer yeah like that yeah I think that's a very good question uh and um I first started to understand what that meant during this internship that I did at I Delhi during my undergrad where you know I was designing this robot that would play guitar we do not want the robot to press the guitar too hard or the guitar would break right so uh and also um um so there were two robots one robot was pressing the notes the other one was tucking the strings you also don't want the one which was plucking the strings to pluck it too hard or the string would break yeah right so that's where I tried I got an intuition of that okay compliance is very important and we need to set the amount of force that the robot is applying so now to to answer your question um I think we are heading in a direction where uh um a lot of",
      "robotic systems are um going towards a direction of giving some sort of a feedback to the human which can be then used to design safer systems right so um um um and and that's also something that I have personally looked into through my research as well um how we can get a good feedback from the robot so that they can interact with objects in a better way and so that they can also like not hurt humans or any other objects um um so so that's one thing and another thing which is potentially a lot of people are working on is using techniques like reinforcement learning Etc um that really helps you you know provided you give it a lot of good demonstration U you can you can train a policy which can be then used to interact with humans in a better and a safer way incrementally you keep on improving a model or you keep on improving a a policy um um and so because of all the advancement that's happening in the Deep learning robots are becoming safer and safer day by day so I can Envision maybe 10 20 years in the future we will have very safe robots so in your opinion do you think that the with let's say you you mentioned about reinforcement learning right reinforcement learning with demonstrations would make robots safer but then in your opinion uh are these robot systems really learning at the core that what is safety here right yeah and that's yeah that's a very good question that's a very philosophical question actually uh what is learning actually there like what is actually learning because what you see like a physical robot something that you would see in a movie like I Robot",
      "or something a robot is one thing that I've learned is like robot is not like an intelligent being it's a very dump machine you have to literally tell it everything that you need it to do right so keeping U that in mind your robot is just as good as your model or just as good as your learned policy right right so um if your policy is improving and there's some great research from nvdia from some other places U that I've seen um um that they have been able to train robotic systems to be uh essentially better at doing tasks um so if you can do that and if you have some of course like some safety controls there you know U then robots will become better and by robots becoming better I mean the models underlying them will become better okay okay so I mean that's interesting I mean the ways in which robotics can help us assist humans been talking to couple of folks around Marine engineering site or even even to some of the companies who are actually doing using underwater btics to detect mammals and even and and other objects or species um to to plan the safety of those species and and their navigation Etc um including fishing and and other areas so and and what's your goal finally to get into any specific application areas is there is there an area where you are getting into with robotics yeah so I'm personally very interested in the health related aspect of Robotics okay like the kind of like the unit that I work in the my lab is in the medical robotics department at w and so something along those lines so one of the more direct directions that I uh",
      "am pursuing and I wish to pursue in the future would be surgical robotics so surg yeah yeah why particularly surgical robotics essentially humans are error prone right and surgery is a very precise um um um activity intervention or or of the human body right and so I I would really like to contribute to that and and U and when you say surgical robotics right uh now what all that could be so the surgery so basically this you're saying that in a live surgery these robots would assist uh the the the experts you know the expert person uh doing their surgery and U during during that particular life surgery right so that's what you're saying so and what all that activities would be like could be yeah so there's two things here one is that we train models that can help during surgery so it's not essentially like a physical robot that is helping it but some deep learning models that we have trained which are helping with different tasks like navigation or um um um removing certain tissues from the body uh okay cutting certain portions looking at healing all those type of Interventional things right that's one aspect where where physical robots are not involved but then there's another respect as well where physical robots are involved so one big thing there and that's something that I looked into is that a lot of the times uh we do want to have a physician do a surgery remotely right and so robot can be very helpful there right another use Cas is that humans we Physicians they're great they have years of experience um but they do get tired right and so when you're tired like it's hard to do that so if you're",
      "sitting somewhere and if you're just moving your fingers and you're just doing your surgery compared to if you're standing on your operating table and then you're trying to do it you'll get more error there compared to like if you're sitting somewhere comfortably and then you're doing it so like there are robotic surgical robotic systems out there like DaVinci system from intuitive surgical metronic has a there's a lot of companies which have these surgical robotics system especially us especially is really adopting it I know a lot of medical institutions nowadays they just for the sake of it they will just buy like a very expensive surgical robot to say that okay we can also do robotic surgery no I think I think that's a very nice uh analogy that you made to understand like why uh robot uh is going is is actually needed for so so heavy Precision task actually it's going to be much much more valuable yes so uh another question uh we are running on short short of time but then few questions left but if you can address in short that will also be helpful okay so uh so what advice would you give to students considering pursuing a PhD in robotics especially those who are transitioning from undergrad to Graduate Studies right now got it got it so one advice I would give them especially let's say if you're in early years of your undergrad definitely do some research it it will make you a better PhD candidate you do not have to have research papers in order to get into a program where you'll be a good fit but it certainly helps uh so that's one advice that I would give people and and also um uh I would",
      "say like like do your research do not just get into a PhD for the sake of it like from my conversation you would have seen that like I really like or enjoy what I do which is why I'm doing it and so it has to be that otherwise PhD is a very long journey it's a very frustrating Journey you can be miserable and you'll be like why did I do this especially in the US a lot of phds just drop out like I think it's it's a very high percentage so that's what I would say like do your research um in terms of where you want to go where you'll be a good fit and do some actual research which will be helpful thanks thanks K uh so and and uh from the other angle as as well uh so there are a couple of misconceptions about Robotics and AI um that new Commerce coming to the field has any one common misconception that you would like to share yeah that it's it's kind of like the misconception that I had about coding uh so I initially used to struggle a lot with coding because I was like oh it's something too complicated I cannot get it but essentially it's it's all math and it's it's not like some very hardcore you know calculus something which is too deeply technical uh I mean it is technical but the mathematical principles align uh underlying it it's it's relatively simple thanks so Kev what is your next three to five year goals and any message for companies who might looking to collaborate with you okay so over the next three to five years my plan is uh well first would be to graduate and then to find",
      "like a a good place for me to go to um um and and and what that would be is like uh uh somewhere where I can do some research but I can also contribute towards like developing a product I think I've been in Academia for a while now so uh I would definitely like to go somewhere where I can have more of an impact um um hopefully stay staying in somewhat like a similar area where I've been working on but I'm pretty open to you know taking up something which is relatively new where my skills can be useful okay something around industry Academia collaboration or something like that is that the kind of role that you are kind of envisioning uh I I guess I guess uh what I'm envisioning is something in Industry which has more of a academic environment and uh um and there's there's very few places which have that um like I know mitsuishi uh has something similar like Mitsubishi Electric research Labs there the culture is kind of like that uh okay so bonus question for the audience um any recommended resources on the internet some blogs or YouTube channels that you follow and admire uh especially uh those who are right now um first going to pursue or looking to pursue their PhD into robotic side um so that they can actually spend more time on those and then probably might that might answer their question yeah yeah uh I'm just trying to think about this because right now the resources that I look at are very different from what I used to look at when I was doing my undergrad and I was trying to transition like right now I mostly look at Google Scholar and I look",
      "at research of people and that is what gives me uh my Direction um and that's what I admire right now but before there's a ton of like uh um very good U um YouTube channels uh where you can pretty much learn anything like uh I I do not have anything off the top of my head because I've not been very aggressive on YouTube lately okay so um um so I cannot name that but I honestly think nowadays doing a quick Google search and that that that works for me that that makes scholar research there so many research papers even if I write then there so much that will come up how to Zone down into um particular paper if you can highlight some one or two pointers that yeah you know um in one thing that I learned during my PhD uh so I'm I'm I'm a very H I'm someone who likes through a lot of things all the time so uh which is not really uh which can get a little bit messy so one thing that I learned during the course of my PhD and this is a very cliche thing to say in the US especially is to have smart goals where I think s stands for specific m stands for measurable a I do not remember the whole thing but if there's one takeaway from that smart thing it's specific that thing has really helped me where I'm like okay this is what I want to achieve so I need to be very specific about it and that's what would help you with your search as well okay cool that's wonderful uh K we would I think we can go on and on uh with as uh you know it's",
      "it's time to kind of wrap it up and then U so and and that's where I mean I would love to thank you bottom of my heart right I mean to kind of share this motivational Journey there are a couple of things that particularly students have uh to learn through this uh conversation right and and get inspired and get motivated and I hope that uh you don't mind if uh anyone listening to this podcast might reach out to you once they go through this right and uh and uh yeah so thank you thank you KV for your time and uh All Right audience I mean so I I think you all must have enjoyed this conversation right uh we will we will uh put those links in the YouTube so I so while you are watching this video uh you can go through the comments and description and we'll try to summarize and we have tried to summarize this all these points here um and and yeah thank you thank you uh for this conversation and time K yeah for sure and it was great uh talking to you"
    ],
    "transcript_word_count": 7094,
    "transcript_chunk_count": 24
  },
  {
    "video_id": "dxGzQm1YFNo",
    "title": "AI Data Labeling Tool - Automate Annotation & Boost Efficiency with LabelGPT | Labellerr",
    "description": "Experience the ultimate AI Data Labeling Tool with LabelGPT by Labellerr. Automate annotations, streamline workflows, and achieve 199% faster results with smart QA techniques. Perfect for industries like sports, agriculture, and more. \n\nTry it now!\nWebsite: https://www.labellerr.com \nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=dxGzQm1YFNo",
    "embed_url": "https://www.youtube.com/embed/dxGzQm1YFNo",
    "duration": 339,
    "view_count": 91,
    "upload_date": "20240503",
    "uploader": "Labellerr",
    "tags": [
      "annotation workflow",
      "data annotation software",
      "data labeling process",
      "data management",
      "data labeling jobs",
      "data annotation tools",
      "annotation platforms",
      "manual annotation",
      "data labeling tools",
      "tagging software",
      "data workflow",
      "data labeling",
      "data labeling service",
      "video data labeling",
      "data labeling and annotation",
      "labeling software",
      "data annotation",
      "data preprocessing",
      "image annotation",
      "annotation software",
      "data pipeline",
      "image labeling",
      "data entry",
      "labeling company"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] hey everyone uh so let me tell you uh let me show you uh today uh demo of uh uh product called label gbd um so what you see here is the homepage of website label.com uh as you can see I mean laer helps AI teams to prepare uh then for with their the data peline with these they high qu quality labels 199 faster it's computer vision NLP llms right so and then we provide automated annotation and smart QA techniques uh to help you reduce your time spent into the data labeling process that to we helped you set up in just few weeks uh without further Ado uh let me move on to the uh today's agenda which is uh to Showcase you uh to help you experience how label GPT looks like and how can you write yourself as well uh so as you can see here uh on the top right corner there is a button called try label GPB let's click on this and I'm switching my tab so as you can see here uh so this is the landing page where you can see uh laal GPD and it's couple of public data sets that have been listed here one is like cell segmentation parking space detection fo so now what all these data sets shown here are into the image uh images basically and uh and they you can actually filter them by various Industries like automobile agriculture Sports Marketing and so on um same is applicable data types uh for now um I'm going to show you the demo for the images data sets so let's quickly click on any one of the data let's let's say we want to do Sports analytics and then we have this image data image data of around Sports images and then taken from uh the the sports ground and then let's say we want to detect uh the player uh and let's say we want to detect the B right and so let me just quickly click so in a way what I'm doing is it's very similar to just like in chat gbt we provide prompts so here we provide the labels to be annotated as our prompts where we would want to detect player and ball uh in this data set right which contain 100 images all I need to do is say generate predictions then I can sign with my own I I can talk about it later so now as you can see here um it is getting up us to the page where whatever the data that we selected uh and look and so as you can see here um I can zoom in from here from the slider I can and right so as you can see here it has Mark the bounding boxes of there and the call right player player player ball of course it maybe missed these players maybe because they are the audience um and then you can see here it missed this there probably uh if I reduce the confidence yeah as you can see here right kind of so it was very less confident of whether this is playar or not right for various reasons because the image quality because it maybe for for the system is a goalkeeper he's a goalkeeper uh looking at this right uh this is certainly certainly uh awesome right you can see the rest of the images right and you can control in terms of confidence and then tune the confidence and from here what we are doing next is what you can actually get configured is here there's a button called discus use Cas right so what probably an Optimum next step for you is that can I addit this can I um refine these predictions by providing some human in the loop feedback and so on yes you can do this with LA and that's where what we are building um so if you really love this uh demo um don't hesitate to contact me or label or go go on to the label your website try label GPD try out yourself different uh different data sets and even you can try it on by your own U by just uh for example there will be uh there will be my data set section here right so you can actually drop in your data sets here and then you can you know play with it and then would love to hear about your experience thank you have a good day",
    "transcript_chunks": [
      "[Music] hey everyone uh so let me tell you uh let me show you uh today uh demo of uh uh product called label gbd um so what you see here is the homepage of website label.com uh as you can see I mean laer helps AI teams to prepare uh then for with their the data peline with these they high qu quality labels 199 faster it's computer vision NLP llms right so and then we provide automated annotation and smart QA techniques uh to help you reduce your time spent into the data labeling process that to we helped you set up in just few weeks uh without further Ado uh let me move on to the uh today's agenda which is uh to Showcase you uh to help you experience how label GPT looks like and how can you write yourself as well uh so as you can see here uh on the top right corner there is a button called try label GPB let's click on this and I'm switching my tab so as you can see here uh so this is the landing page where you can see uh laal GPD and it's couple of public data sets that have been listed here one is like cell segmentation parking space detection fo so now what all these data sets shown here are into the image uh images basically and uh and they you can actually filter them by various Industries like automobile agriculture Sports Marketing and so on um same is applicable data types uh for now um I'm going to show you the demo for the images data sets so let's quickly click on any one of the data let's let's say we want to do Sports analytics and then we have this image",
      "data image data of around Sports images and then taken from uh the the sports ground and then let's say we want to detect uh the player uh and let's say we want to detect the B right and so let me just quickly click so in a way what I'm doing is it's very similar to just like in chat gbt we provide prompts so here we provide the labels to be annotated as our prompts where we would want to detect player and ball uh in this data set right which contain 100 images all I need to do is say generate predictions then I can sign with my own I I can talk about it later so now as you can see here um it is getting up us to the page where whatever the data that we selected uh and look and so as you can see here um I can zoom in from here from the slider I can and right so as you can see here it has Mark the bounding boxes of there and the call right player player player ball of course it maybe missed these players maybe because they are the audience um and then you can see here it missed this there probably uh if I reduce the confidence yeah as you can see here right kind of so it was very less confident of whether this is playar or not right for various reasons because the image quality because it maybe for for the system is a goalkeeper he's a goalkeeper uh looking at this right uh this is certainly certainly uh awesome right you can see the rest of the images right and you can control in terms of confidence and then tune the confidence and from here",
      "what we are doing next is what you can actually get configured is here there's a button called discus use Cas right so what probably an Optimum next step for you is that can I addit this can I um refine these predictions by providing some human in the loop feedback and so on yes you can do this with LA and that's where what we are building um so if you really love this uh demo um don't hesitate to contact me or label or go go on to the label your website try label GPD try out yourself different uh different data sets and even you can try it on by your own U by just uh for example there will be uh there will be my data set section here right so you can actually drop in your data sets here and then you can you know play with it and then would love to hear about your experience thank you have a good day"
    ],
    "transcript_word_count": 768,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "4lRRL3h8zUU",
    "title": "Labellerr Transforms Months of Work into Weeks: Tyler, CEO of Butterfly Positronics",
    "description": "Discover how Labellerr empowers businesses to overcome challenges in preparing training datasets for vision, NLP, and LLM model training. Tyler Kapp, CEO of the US-based sports analytics software company Butterfly Positronics, shares his success story. Learn how Labellerr's advanced tools streamlined his image segmentation annotation project, reducing complexity and delivering exceptional results in record time.",
    "video_url": "https://www.youtube.com/watch?v=4lRRL3h8zUU",
    "embed_url": "https://www.youtube.com/embed/4lRRL3h8zUU",
    "duration": 37,
    "view_count": 80,
    "upload_date": "20240502",
    "uploader": "Labellerr",
    "tags": [
      "image annotation services",
      "data labeling service",
      "data labeling",
      "data labeling services",
      "image labeling",
      "image labeling service",
      "labeling images",
      "best data labeling services",
      "image annotation",
      "data labelling",
      "data entry",
      "custom labeling",
      "image classification",
      "data annotation",
      "computer vision labeling",
      "labeling software",
      "ai data labeling",
      "image markup",
      "image tagging",
      "quality assurance",
      "annotation software",
      "data labeling tools",
      "image data processing"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "I thought labellerr you did an excellent job of labeling the images very accurately there were very few that I had any corrections on and any of the corrections that I did have you guys made within the same day so you were excellent for my project in addition to that you got everything done very efficiently um I thought this was something that might take closer to a month and you guys got it done within um maybe two and a half weeks so from our perspective and our company's perspective you guys did an A+ job and we're very grateful that you um labeled all of our images and we decided to work with you",
    "transcript_chunks": [
      "I thought labellerr you did an excellent job of labeling the images very accurately there were very few that I had any corrections on and any of the corrections that I did have you guys made within the same day so you were excellent for my project in addition to that you got everything done very efficiently um I thought this was something that might take closer to a month and you guys got it done within um maybe two and a half weeks so from our perspective and our company's perspective you guys did an A+ job and we're very grateful that you um labeled all of our images and we decided to work with you"
    ],
    "transcript_word_count": 117,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "wKeBO7zEUn0",
    "title": "Streamline Data Annotation with labellerr’s Auto-Labeling Features | Labellerr",
    "description": "Simplify your data annotation process with labellerr’s advanced Auto-Labeling Features. From object creation to classification and attributes, streamline workflows for AI-ready data in minutes. Perfect for AI teams seeking efficiency and precision. Try now!\n\nChapters\n0:00 Introduction to Data Annotation with Auto Labeling\n0:27 Enabling Auto Label for Easy Annotation\n0:36 Creating Objects with a Single Click\n0:56 Selecting Object Types for Annotation\n1:12 Handling Complex Data with labellerr\n1:24 Adding Attributes to Objects\n1:48 Defining Attribute Questions\n2:11 Submitting Attributes for Mechanical Defects\n2:23 Adding Classifications for Depth\n3:20 Creating Boolean Classification Questions\n3:37 Saving Changes Across the Project\n4:00 Reviewing Annotation Labels\n4:22 Overview of Auto Labeling Features\n4:43 Enabling Auto Label in Miscellaneous Properties\n4:56 Selecting Label GPT API for Auto Labeling\n5:13 Accessing Auto Label on Labeling Screen\n5:29 Conclusion and Thank You\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=wKeBO7zEUn0",
    "embed_url": "https://www.youtube.com/embed/wKeBO7zEUn0",
    "duration": 346,
    "view_count": 119,
    "upload_date": "20240425",
    "uploader": "Labellerr",
    "tags": [
      "auto-labeling",
      "tiktok features",
      "auto labeling",
      "auto labelling",
      "labeling tool",
      "labeling",
      "automatic fabric labeling",
      "how to create labels in gmail",
      "ai labeling",
      "tiktok ai labeling",
      "how to create new labels in gmail",
      "video data labeling",
      "automatic labelling for fabrics",
      "top labeling machine",
      "image labeling",
      "label splicing",
      "best data labeling services",
      "auto label",
      "label software",
      "label creation",
      "label automation",
      "label maker",
      "labeling software"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "let's dive deep into the features of labellerr which is constantly striving to make data annotation process very easy for the AI teams so that AI teams achieve their goal of very accurate data annotation so I'll be talking about creation of objects creation of classification creation of attributes for adding depth to these objects and enabling autol label so as to ensure that your data annotation is a very easy process and it can be done in a matter of minutes so creation of objects is as easy as a click I'll just click on ADD object I'll enter the class name I'll select the object type labellerr provides different object types they can range from bounding box segmentation dot polygon Landmark key point to line to accommodate your different data annotation needs so irrespective of the complexity of the data you connect with labellerr we are ready I think I'll go for segmentation as the object type for mechanical defect I can also add attribute now these attributes can range from simple questions like colors and sizes to complex descriptors which you can Define by selecting any of these question types I'll go for the radio type for this question and I'll mark it as required I'll add the option of small and and large I'll click on submit you can see that the attribute has been created for mechanical defect I can also add classifications the classifications are of different types and they can also range from complex disc RTS to very small questions like colors and sizes they help provide depth to your annotation needs I'll make it as a Boolean question and I'll mark it as required I'll click on submit you can see that one classification has been create it down the list now in order to ensure that these changes are consistent across the project I'll click on Save changes and you can save changes in current project guidelines or you can create a new template out of the questions that have been created so I'll save changes in current project guidelines you can see that my annotation labels have been saved now can you can go back to your labeling screen and you'll see that my classification question is placed just right here and all the objects that I created are placed just at the right place I'll also give you a brief review of one of the very important features that makes annotation very easy it makes it a work of few minutes so I'll just click on misscellaneous properties and you can check on enable autol label you can select the API that is needed for autol labeling the objects that you have created so I'll select the label GPT API and back I'll click on Save changes and I'll save changes in the current project guidelines annotation label saved my changes have been saved and when I go to in the labeling screen you can access that label GPT model API by clicking on use Auto label and your annotations would be marked automatically on the file images on the image files I hope you enjoyed this EXP explanatory video thank you for being a patient listener labellerr will always strive to make the data annotation process very easy for the AI teams it will strive to make the data AI ready AI consumable thank you so much",
    "transcript_chunks": [
      "let's dive deep into the features of labellerr which is constantly striving to make data annotation process very easy for the AI teams so that AI teams achieve their goal of very accurate data annotation so I'll be talking about creation of objects creation of classification creation of attributes for adding depth to these objects and enabling autol label so as to ensure that your data annotation is a very easy process and it can be done in a matter of minutes so creation of objects is as easy as a click I'll just click on ADD object I'll enter the class name I'll select the object type labellerr provides different object types they can range from bounding box segmentation dot polygon Landmark key point to line to accommodate your different data annotation needs so irrespective of the complexity of the data you connect with labellerr we are ready I think I'll go for segmentation as the object type for mechanical defect I can also add attribute now these attributes can range from simple questions like colors and sizes to complex descriptors which you can Define by selecting any of these question types I'll go for the radio type for this question and I'll mark it as required I'll add the option of small and and large I'll click on submit you can see that the attribute has been created for mechanical defect I can also add classifications the classifications are of different types and they can also range from complex disc RTS to very small questions like colors and sizes they help provide depth to your annotation needs I'll make it as a Boolean question and I'll mark it as required I'll click on submit you can see that one classification has been create it down the list now",
      "in order to ensure that these changes are consistent across the project I'll click on Save changes and you can save changes in current project guidelines or you can create a new template out of the questions that have been created so I'll save changes in current project guidelines you can see that my annotation labels have been saved now can you can go back to your labeling screen and you'll see that my classification question is placed just right here and all the objects that I created are placed just at the right place I'll also give you a brief review of one of the very important features that makes annotation very easy it makes it a work of few minutes so I'll just click on misscellaneous properties and you can check on enable autol label you can select the API that is needed for autol labeling the objects that you have created so I'll select the label GPT API and back I'll click on Save changes and I'll save changes in the current project guidelines annotation label saved my changes have been saved and when I go to in the labeling screen you can access that label GPT model API by clicking on use Auto label and your annotations would be marked automatically on the file images on the image files I hope you enjoyed this EXP explanatory video thank you for being a patient listener labellerr will always strive to make the data annotation process very easy for the AI teams it will strive to make the data AI ready AI consumable thank you so much"
    ],
    "transcript_word_count": 569,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "h1V5Ky8gpaA",
    "title": "Enhance Accuracy with Advanced Image Display Settings | Labellerr",
    "description": "Explore how Labellerr's Image Display Settings improve annotation accuracy. Adjust brightness, contrast, saturation, and more to make every detail prominent for seamless data annotation. Empower your AI teams with enhanced visual clarity today!\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=h1V5Ky8gpaA",
    "embed_url": "https://www.youtube.com/embed/h1V5Ky8gpaA",
    "duration": 175,
    "view_count": 50,
    "upload_date": "20240425",
    "uploader": "Labellerr",
    "tags": [
      "display settings",
      "reset display settings",
      "how to change display settings",
      "image calibration",
      "xiaomi settings",
      "display calibration",
      "screen adjustment",
      "mi display settings",
      "calibrate display",
      "screen calibration",
      "realme settings",
      "contrast settings",
      "resolution settings",
      "display adjustment",
      "ui scaling",
      "visual settings",
      "calibrate monitor",
      "monitor calibration",
      "contrast calibration",
      "hdr calibration",
      "color calibration",
      "sharpness settings",
      "screen enhancement",
      "sharpness adjustment"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=h1V5Ky8gpaA! This is most likely caused by:\n\nNo transcripts were found for any of the requested language codes: ('en',)\n\nFor this video (h1V5Ky8gpaA) transcripts are available in the following languages:\n\n(MANUALLY CREATED)\nNone\n\n(GENERATED)\n - hi (\"Hindi (auto-generated)\")\n\n(TRANSLATION LANGUAGES)\nNone\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "ZanWDWgzHJ8",
    "title": "Pixel-Perfect Data Annotation with Segment Anything Model | Labellerr",
    "description": "Discover the power of the Segment Anything Model in Labellerr. Achieve pixel-perfect accuracy and efficiency in data annotation with advanced object segmentation. Simplify workflows and save time with cutting-edge AI integration. Try it now!\n\nChapters\n0:00 Introduction to Labellerr and Data Annotation\n0:02 Cutting-Edge AI Technology in Labellerr\n0:08 Introduction to Segment Anything Model\n0:18 What is Segment Anything Model?\n0:37 Demonstration of Segment Anything Model\n0:46 Annotating Cargo Object with Polygon Type\n1:06 Selecting Segment Anything as Interactor\n1:19 Pixel-Perfect Object Annotation\n1:31 Confirming Annotation\n1:37 Adding Attributes to Annotated Object\n1:51 Submitting the Annotation\n1:56 Conclusion and Summary\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=ZanWDWgzHJ8",
    "embed_url": "https://www.youtube.com/embed/ZanWDWgzHJ8",
    "duration": 127,
    "view_count": 83,
    "upload_date": "20240425",
    "uploader": "Labellerr",
    "tags": [
      "segment anything model",
      "segment anything",
      "segment anything model 2",
      "segment anything demo",
      "segment anything 2",
      "segment anything video",
      "meta segment anything",
      "segment anything meta",
      "segment anything model paper",
      "meta segment anything model 2",
      "sam segment anything model tutorial",
      "segment anything model walkthrough",
      "image segmentation",
      "tutorial video",
      "segment model",
      "deep learning",
      "machine learning",
      "github projects",
      "computer vision",
      "data annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "welcome to labellerr where data annotation meets cutting a ji technology I'll be talking about how data annotation with segment anything model that has been integrated recently in labellerr has made The annotation process quite accurate and quite efficient now what is segment anything model it identifies the pixel by pixel intensity of an object and marks the Contours around the object with Pixel Perfect accuracy and it does this by identifying the features and patterns of the pixels inside the object I'll show you how this works so let's go to the label screen inside labellerr I'll click on cargo now cargo is an object of type annotation type polygon and as you can see that there's a wand here so I'll click on the one and can select my interactor right now I'm selecting the interactor as segment anything I'll click on interact now I have my interactor now I'll click on the object that has to be annotated you can see that this object has been marked with Pixel Perfect accuracy it has been segmented very accurately and very efficiently in just a few seconds of time I'll just press my enter to confirm The annotation that has been done by segment anything model this is an attribute attached there's an attribute attached to this object so I'll click on standard to say that this object is of standard type and I'll click on submit you can see my cargo has been annotated very accurately and this is how it works I hope you enjoyed the magic of segment anything model in data annotation thank you thank you so much for listening patiently",
    "transcript_chunks": [
      "welcome to labellerr where data annotation meets cutting a ji technology I'll be talking about how data annotation with segment anything model that has been integrated recently in labellerr has made The annotation process quite accurate and quite efficient now what is segment anything model it identifies the pixel by pixel intensity of an object and marks the Contours around the object with Pixel Perfect accuracy and it does this by identifying the features and patterns of the pixels inside the object I'll show you how this works so let's go to the label screen inside labellerr I'll click on cargo now cargo is an object of type annotation type polygon and as you can see that there's a wand here so I'll click on the one and can select my interactor right now I'm selecting the interactor as segment anything I'll click on interact now I have my interactor now I'll click on the object that has to be annotated you can see that this object has been marked with Pixel Perfect accuracy it has been segmented very accurately and very efficiently in just a few seconds of time I'll just press my enter to confirm The annotation that has been done by segment anything model this is an attribute attached there's an attribute attached to this object so I'll click on standard to say that this object is of standard type and I'll click on submit you can see my cargo has been annotated very accurately and this is how it works I hope you enjoyed the magic of segment anything model in data annotation thank you thank you so much for listening patiently"
    ],
    "transcript_word_count": 276,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "H4FGoiJfkvM",
    "title": "Labellerr AI Data Podcast: Data-Driven Strategies with Mathangi Sri, Chief Data Scientist at YUBI",
    "description": "Garbage In, Garbage Out is a podcast dedicated to discussing cutting-edge advancements and data-driven strategies in AI, Machine Learning (ML), and Deep Learning (DL).\n\nIn our second episode, we are thrilled to welcome Mathangi Sri, Chief Data Scientist at YUBI, who shares her remarkable journey in the field of ML. From pioneering impactful data science solutions to shaping the future of AI, Mathangi's insights are invaluable for anyone passionate about leveraging AI for real-world impact.\n\nAbout Mathangi Sri\nA pioneering Data Science leader with over 19 years of experience.\nHolder of 100+ global patents in data science.\nExpertise in AI, NLP, ML, and data-driven strategies.\nRenowned author, speaker, and mentor.\nRecognized as one of the top AI influencers by leading industry platforms.\nAbout the Host - Puneet Jindal\nPuneet is a seasoned ML expert with over 12 years of experience working with AI-driven giants like Delhivery, Walmart, and Rategain. For the past 4 years, he has focused on building Labellerr, an automated data labeling platform that delivers high-quality training data essential for developing robust AI models.\n\nLearn More:\nVisit Labellerr: www.labellerr.com\nExplore LabelGPT: www.labellerr.com/labelgpt\nConnect with Puneet Jindal on LinkedIn: www.linkedin.com/in/puneetjindalisb\nConnect with Mathangi Sri on LinkedIn: www.linkedin.com/in/mathangisri",
    "video_url": "https://www.youtube.com/watch?v=H4FGoiJfkvM",
    "embed_url": "https://www.youtube.com/embed/H4FGoiJfkvM",
    "duration": 2481,
    "view_count": 112,
    "upload_date": "20240425",
    "uploader": "Labellerr",
    "tags": [
      "data driven strategy",
      "customer retention strategies",
      "performance marketing",
      "stock trading",
      "data science",
      "data visualization",
      "data analysis",
      "hr analytics",
      "demand forecasting",
      "data analytics",
      "decision making",
      "marketing analytics",
      "data management",
      "strategic marketing",
      "data storytelling",
      "data mining",
      "kpi tracking",
      "insightful analytics",
      "data driven growth",
      "data mining techniques",
      "retention analysis",
      "growth marketing",
      "kpi analysis",
      "marketing performance",
      "analytic storytelling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] so hi everyone good morning good evening to all those watching across the globe uh this is the AI data podcast and today uh I'm your host my name is Punit and uh I take care of labellerr uh we are a data engine for AI team Sal companies reduce their data labeling time and speed up the and enable higher data accuracies for their fine-tuning and building AI projects and uh what I'm today excited about is we have a very special guest on our episode today uh Matangi Shri uh is she's Chief data officer at UB uh she has been she has had uh data science initiative at gojack phone pay CD Bank to name it at all and more importantly to understand she has 100 plus patents on her name uh wow and she's one of the top 50 influential AI leaders in India uh so welcome Matangi uh to the podcast thank thanks pun thanks for having me it's glad meeting you uh and always a pleasure to talk to you welcome mang uh so Matangi uh let me just uh start with a question right uh interesting question which I personally wanted to ask so if you would would like to briefly introduce about yourself with a Twitter pitch how would you do that Twitter pitch is it yes um so for many people data can be a job or a profession for me data is life that's interesting okay and now can you can you kind of give a two liner brief about your journey overall all about what you are right so that will help yeah so I've been I should say I've been very grateful for my journey in the space what started out as when G came into campus interview and uh we were interviewed by this place um so it's going to be 20 years uh experience that I have so when they had come 20 years back uh and they explained what is the power of analytics they said one of the things that G where to start a windmill analytics would decide which would be those locations so that kind of caught my attention on the decision making power of data and how intuitive driven or domain driven decision making very human driven decision making can be transformed and if I can use the word democratized by data decision making right so that kind of caught my attention then that's why remember this example vividly after two decades that pulled me into to this field um forever uh from that perspective I am not a trained computer scientist I'm not a trained statistician I'm not a trained o expert I'm none of those so but whatever I have learned I have learned during the job and somehow or the other have been in the right places uh to learn these uh Technologies and hence get an opportunity to leverage them to build greater business impacts so one theme throughout my career has been how we can use the power of data to build better impacts and when I mean impacts it could be higher revenue for an organization or it could be uh a better uh experience for a customer or it is maybe putting a smile on someone's face any anything is an impact right so that's where um my journey has been so I've kind of got the ringside view of watching this space evolve from a very static field of uh an analysis or statistics to a extremely Dynamic field at scale decisioning using computer science being just a transformative Journey for AI and pleasure watching this field unfold uh and in some way be part of this field um has been has been has been very great so that is where uh that is how I would look at my uh professional Journey uh in uh in all these years so I think I think uh you know you made a point right you're not a trained statistician you're not a trained computer scientist and you're are not trained or or right but I think I think the training that you have got is something is not can't be can't be achieved just through forces I mean right I mean you have you have actually experienced that all right so I think I think you are amalgamation of all of these experiences so what we what instead of just saying Chief data officers what if if you have to device a new name uh to your uh maybe the role what would you define that as um so data for impacts that's what it is so maybe Chief impact officer that cool Ro that way so go got it so so this this uh of this experience of yours back in the day has been the the inspiring part of your career is where you pursued the career in in into Ai and data science as such would you like to call this as that right so see uh I'll tell you how the how I was part of the field so I started with G Consumer Finance uh later on it became genpac but I joined u g Consumer Finance and uh then U uh our role was to build risk score models uh using statistic te and then even then we used to do large scale transaction uh models so what happens is when you swipe a card there is an over limit so I was working for the US market fairly Advanced at that time because for over limit personalization we were doing like when you had a card and it had over limit let's say you have th whatever ,000 is your exposure and you're over the limit of $1,000 whether should I allow you till $100 more or whether should I allow you to $150 or more or whether I should allow you to $200 or more even now when I think of this problem is baffling to me because we were solving this problem 20 years back right and through so this model had two components one was a decision tree which kind of says what is the uh segment of the user once we had the segment of the user then we had different limits right so segment is in rows and uh different limits and columns then it became an O problem an optimization problem how you are kind of uh making sure that your exposure is limited but the because if you allow over limit these are the guys who will also spend more on your product and give you more finance charges so how do you maximize income without uh you know compromising on risk rat so that's the uh optimization problem that we did and this went real time like this went live real time as the person is swiping we would say what is the dollar amount that it is and then the trans system would act accordingly so that's where I feel uh I'm very thankful and I'm very fortunate to somehow have figured these problems out somehow to have landed in these uh areas so this was the field but however largely outside of where we were Etc this was also a very um static field that time so an example could be somebody wants to find out why did my sales go up and why did my sales go so somebody will write hypothesis at the maximum you will do a t test or a z test and then say is it statistically significant or not my marketing campaign did it work or not things like that right this was bulk of problems that people were uh handling except for few lucky ones like me were handling such scale problems but even I was handling a mix of these right this field slowly and steadily with explosion of more data with consumer companies coming in you know like the likes of Netflix and Google and everything and the compute becoming cheaper right uh so then the algorithms which were there suddenly started becoming to use uh these are your we talk about deep neural networks even in 1970s 1990s Etc yeah and so whether deep neural network or not some of the machine learning Concepts kind of took off at that point in time when the availability of data increased and the compute kind of cost became lower then it became more more about uh how do you liage the algorithms for these large scale problems so the world of Statistics kind of merged into computer science that way and a lot of traditional statistical use cases uh you know the single decision tree became Ensemble decision tree and and things like that became uh front ending a classic example of that I'll tell you uh when uh let's say we were doing U call center analytics or we were doing ivr training right like natural language ivr this is not natural Lang language conversational engine but notal language ivr so the ivr could say you could say things like it would say that and then say you know I want to book a flight and things like that and whatever you say it's closed it's in a way closed response but it also lets the customer speak um again this is 12 years back we were working on a project like this okay and uh this field was dominated by linguist uh by uh NLP as a field uh especially the nlu part nobody even talked about nlg which is natural language generation nlu was that intent classification sentiment analysis dominated by linguist okay now in a matter of three to five years the division of linguist completely got overtaken by computer science so people who had specialized in language modeling right language modeling is not language models that we talk about here but grammars parsing sentence structures all of that kind of became overtaken by your you know svms or after a period of time deep learning neural networks Etc so and I'm seeing this more and more pun right seeing this uh traditional statistical thing computer just smashes some of these and takes it over take computer vision for example if I were to take an image and extract it I would have to use a lot of vision techniques right about um you know how am I kind of figuring out the region of Interest I would have to use specific of specific algorithms to get there today we talk about donut models and uh the there is not even a two-stage model there is not even a computer vision model and an NLP model everything gets crunched into uh the same thing and then to top it up generative AI on top of it so the with Advent of an exponential increase of more and more data that is coming in uh the I I would say Brute Force computer science Bay just takes away the finer techniques right that's the evolution of the field okay to be jobs called statisticians I'm sure they are there still but they used to be very prominent in organization so these are the people whom you go to to figure out um whether you are going to use a structure equation modeling or whether you are kind of analyzing an experiment Etc that field is not there today so much more as how how it used to be the prominence is gone right because you don't need foundational statistics to build a machine learning model today it is it is the state as much as people like me who come from these traditional statistical backgrounds uh by way of job can cry and say whatever we want to say the reality is if you have to build a machine learning model today you don't need foundational statistics to operate you can fairly start from algebra and then even if you want to understand foundations you can algebra gradient descent back propagation and there you are you can straight land in um building machine learning models right so a shift it's a shift from fine-tuned high nuanced mathematically shaped field so mathematics is important but what's more important is your ability to handle large your ability to kind of um understand U some of the algorithms you know that is built on at a at certain level but then from there you pick up pick up pick up and move right into the computer science area so that is the fascinating thing for me about this field when people used to say oh you built a model let me see what is this model okay what are the top variables that you know you have uh you what is your why is income not directly proportional I remember that I remember have you bucketed it why is Your Bucket breaking and all of the questions thrown out of the window what buckets when you're talking about if I built a take even a random Forest right forget even neural networks take a random Forest I have built a tree with thousand uh trees in it what bucket what will break how will you even find it out can put some lime sharp examples on top of it but uh it just smashed people who were talking about explaning know to another level of bafflement right the field just opened up so yeah that's what technology can do right what people were cutely working on you know the wave comes and then just hits you on the smashes you on the face and goes that's well that's possibly describing the growth of Technology yeah I mean I remember uh since you talked about this how it has moved from Linguistics to to just everything in gen and just put in the data and then these algorithms do all that stuff uh it's I remember doing the Cs 2 to4d uh it's a Stanford course deep learning and natural language processing and how far I mean that was an more like an introduction back in I think around 2014 or something and today it's like we are not even we were at that time we were talking about breaking that thing that okay it's not just basic model you can put deep learning in there and then we are even now talking about getting a that NLP deep learning now is also being called as the word to is now anymore is not s it's it's just gone but what gave R to a lot of things yes exactly that is where the embedding Concepts come in yes yes I mean again I mean in in general if you look at it it's it's all transition right I mean the concepts and even even I wonder this question at time um even my team members uh talk to me and asked uh where should I start and at that at some sometimes I feel how would I mean they would have to go through all of those steps what we have gone through and even a lot of stuff that even I haven't gone through and I mean you I mean you have been there from even more than uh even even more a decade even before that right all those experiences that you gain so uh at one time I mean and then there are some new uh pressures coming to me and saying that okay we know Lang chain we know this and we I have built this model and I say okay but then do do I need to tell them that okay you need to actually go back to basics and fundamental and see that okay how the this this used to work and then how it changed from CNN to rnn's to lstms and then someone said oh Gans are there but then there was someone who said okay can we build a Transformer thing out of what challenges what RNN had right so I mean how how do you uh I mean what is your opinion and advice to someone uh trying to get into this right I mean even I'm trying to wonder this correct so I don't think uh we can do away with foundations fundamentals have to be there to take a text problem you got to understand tokens youve got to understand you know how a basic uh bag of wordss model will work and why bag of models works even today um our um voice engine that we have deployed in the market uh has an ml based uh you know model with back of wordss approach so I don't think you can just replace bag of words uh you it has its value as well right number of rows are limited it's a well-bounded problem why would you not try and use some of those are at least Bas line with those right bag of words to embeddings to sequence to sequence models to Transformers then uh you know attention then your uh generative then your Lang large language models and then the variations of large language models would still be a course right so I don't think because if you don't understand bag of words it's going to not even see the basic intution of text mining is this that you are converting unstructured data into structured data you are converting words to numbers right that Crux has to be there you can convert words to numbers using a tfidf matrix or you can convert word word to Wi using a embedding uh kind of approach right or you already somebody has converted and you can use that for your transfer learning the Crux of it is how you are converting words to numbers and there are multiple methods to come so I think that Crux of it will have to be there and in the world of let's say structured data same thing so you have to note some level of Statistics some level of understanding at least you should be able to fit a yal to X plus b in paper and you should understand the formula of a and b in that right that's that's basic linear algebra you you got to know that right then you do your stochastic gradient descent Etc you you kind of do your gradient and then stochastic gradient this will give you some level of and some intuition into how ml models work right then you can from there Branch out to a tree how entropy works and then get into gradient boosting where what is gradient in gradient boosting and then from there take it forward because you will be doing this will help you appreciate how data behaves when it is when you are predicting it through linear models and how data be behaves when you predict it through nonlinear models right what is that multiple layers in uh deep neural networks do right how does it break your linearity in yal to X plus b right so those and why is that needed right when we when we do certain problems you kind of understand uh you know why is that needed right for all of that the foundations are very important so if I were to I might sound very traditional now I have to say these all these hashtag words and disclaimers and everything in the world we live in because people immediately look down on you if you don't say uh I use generative a to brush my teeth right so uh that's the funniest thing that I've heard today so so it is important to get the fundamentals right because the fundamentals will help you appreciate complexity more than anything else app helps you appreciate it that makes sense so in a way the entry barrier though it looks like it has decreased for someone new trying to get into this but actually for them they still have to travel Traverse all those if they don't travel that on yes they don't travel on day one but they they got to travel I and other point is I don't know how many interview panels are not asking questions on um entropy gradient descent your refence score Precision recall uh Etc so I don't I think I'm sure most of the organizations are very thorough and they ask these questions yeah for people to uh you know come in you can say I know rags and I know pine cone and uh you know dbx yesterday got released and I tried it and I know the difference between tbx and Lama 3 and this has 13 billion that has so many billion parameters Etc but uh point is I I hope s I'm sure sanity is providing in lot of companies and their interviews are far more foundational and fundamental so there are not people who get carried by hype people who know you know to crack or take hugging face get to uh you know few notebooks uh run those few notebooks and then I know I can talk and then uh I read the last four medium post then I'm talking so I I assuming that interviews today interviewers are also kind of encouraging foundations and fundamentals because that solves a business problem more than anything else true no no that's that's something as an advice we thank you for sharing this advice for the other organizations as well and for us as well um so uh quickly jumping onto uh a new another question right so what are your personal and professional aspirations for the next three to five years I don't know whether this question is probably valid or not but then I mean I'm I'm sure at every stage in life sure even after achieving so much uh what is the what is next for you in your personal so one thing I'm very grateful for whatever it is today right I I do think in a lot of ways I don't deserve many of these things I uh that is and I've been in the right place and that's that's the first thing and uh I that's how I think and that's how I operate then um of course so in terms of personal aspiration I do want to be a better person a better version of myself uh and meaning God willing I want to use my services for larger social good uh and I started the same data is life so without data that larger good doesn't come so that's my means to an end uh but I'm getting um into a space where uh the impact that I create has to be for you know larger good uh possibly more uh social good also and if my profession can help me get there that's the place I want to operate in thank you um and so how do you Foster motivation and collaboration within your team see everybody is motivated right um as a leader all you have to do is to stop demotivating them then you let uh people be there and they'll figure out ways to solve a problem uh to your energy you can contribute uh somewhere if uh things are down the chips are down Etc but larg um and large part of uh whichever teams have been part of I feel people are there for a purpose and whether some people realize purpose and some people act on a job without knowing they behind a purpose but everybody largely are there for a purpose and if you are already you have a purpose and you love your profession and you will be motivated right and your job as a leader is to make sure that they get those problems day in and day out and you are unblocking things for them uh you are kind of operating at a level which enables them to operate on their own and you are there to kind of clear the way um that comes in their way because internally you cannot motivate any motivation is within um so just because we give an award or just because there is even a salary rise that's not motivate anyone they're not uh you get the best people in the team and then you let them do what they are great at and you kind of unblock the way for them you get the best team and and so most importantly it's first the first step is to get the best people in the team yes is the most important yeah so what are the three attributes es maybe I don't know whether it's easy to generalize but three attributes that you look for in a team member uh best team member best team member the the Curiosity to learn the ability to be not afraid of failure uh and uh the attitude to contribute of a an opinion I would would seek is there are folks who are saying gpus are the biggest bottleneck but then then there are folks who are saying uh refined data is essential for AI so according to you which one of these two is the biggest B like even if you have to kind of weigh between these to Talent creates technology okay it's not the other way around so the biggest enabler and the biggest bottleneck is Talent right so you can make gpus cheaper with great talent as well and if it's a great talent they will know how to work without those expensive gpus also so if you at a very high level the enabler and the bottleneck can be only people it cannot be anything else people create world right so at the next level business problems are extremely important whether to know whether you need those gpus or not uh so I think it uh we can do enough with the existing gpus possibly in a lot of cases uh where where it really demands uh let's say you solving a large scale vision problem voice problem Etc where it demands I know uh GPU is on the run but I'm sure like I said Talent Finds Its Own Way the next way of innovation like how it was in software it will be in Hardware it will smash the prices down of gpus pretty much right this has been a uh catch game uh right uh uh where when the compute was very high compute came down it crashed to a level then algorithms came up then there's a lot of innovation on algorithms Now The Innovation will go back on compute and it will go parallel also right now we have quanti models we have models which can run in CPUs uh so there is going to be uh work done on that also F instead of fine tuning you can do few short learning so there will be both sides happening let's say a few years from now we are not talking again talking about a GPU bottl neck world or uh something like that the expansive part of it is going to come from problems being available for AI to solve and what about and what about refined data how big is this bottleneck everybody said that people scraped internet left right center and that's how language models built just think about it who would have thought thought a language model would be built by a company which is not like Google or Facebook or any of this the first language model came from open AI from open data right so who would have thought that we would have thought everybody followed up that's a different point right but uh when people used to talk about Google or uh this was the story right because I heard this from the investor of Google uh people said why do you need another search engine alav Vista was already there why do need Google and the way Google became what it was it had all its data right and hence you would naturally think an AI algorithm would need that level of data and these guys who had the Monopoly on data would create that somebody somebody broke that itself right where they had no they had no right to data so to say but then they were able to build uh a model completely built out of the world's data so I don't think refined data or something is a problem that is in your head they can always find out a way uh and uh that's what I'm saying neither refine data nor Hardware is a problem if it is a problem it will get solved by humans matters Nothing Else Matters and and in terms of where do you see the future of a adoption and development heading right I mean prob from a considering industry and geographical Trends see there is China is catching up there is I mean us already and then India is also catching up uh and and recently Middle East announced a huge initiatives how in your understanding how is the Dynamics going to play because the the the fundamental understanding or or the saying is whosoever becomes the leaves the AI is going to be the next superar so it's an important question politically and geographically I mean on a global global certainly play a huge role uh and uh this some of the initiatives that is for nonprofit like a for bat Etc just mindblowing there's also a lot of funds fiding on these initiatives from the government and uh and we have been at Forefront of this Innovation uh I was part of the GPA Summit that happened uh where the Prime Minister had come and his vision for AI these are times to 11 right I come from the time where data science got spoken only by few right AI being a political agenda a national agenda uh in both political and National agenda in the in the country it's um a lot to see right I think India is going to definite again from the same fact of stemming um Talent built technology India's rich in Talent India's Talent can get on to AI faster and though large part of Internet is in English the large part of world in world population is non-english right so we need and that sits with India right so you have to build AI for India and then you are building AI at scale it's very simple so I think this is going to be uh a huge area for for India to play I would say India will transform AI so okay so one um one question I've written it down and I'll I'll you know speak it out so Gary Marcus said right things are about to get a lot words for generative he he he has his his blog Gary marcus. sub.com that's where he elaborated but let me just simplify what he went on to saying when will the Gen bubble bust I mean he he's a critique is a is a very popular critique about gen and uh and he says why and how it could happen in the next 12 months uh I don't know whether you got a chance to kind of read what he said but then what is your opinion I mean critiques of genbi are saying that it's a bubble burst that is going to happen in the next 12 months but then even myself uh talking to organizations and they are using organ this gen uh in in a phenomenal way as well so why why these critics are saying so that the bubble is B going to B one Nvidia booked its profits and made its money where bubble or not right so that is the first thing that has come out of it um and uh you know opena became the marketing arm for NVIDIA so these are things that uh unfolds I don't think uh it is like the crypto bubble or anything like that uh it is uh it is here to stay right there might be a bigger bubble bigger bubble might become smaller bubble and here to stay Etc but to say this will go away and it will burst and everything no it's here to stay it is here to stay because of few things it's here to stay because it's not some random magic that is happening built through science um built through Transformers which has been proven very well it's just amazing to see the technology in action uh it will find its right you usage like any other technology will find Its Right usage right so uh media blows up everything uh quite large proportion and then things settle down it you need marketing media did that marketing for for Genera and it is here to stay and it opened a lot of people's eyes and ears to AI not only generative AI uh one of the things I've been saying in a lot of forums is this that earlier we used to wonder whether AI will get a seat at the table today it is the board asking its Founders saying what have you done with generative AI right yes so it is table St and that will stay that table is not going that is not going away right uh it will find its better use cases it will get more refined maybe people who are burning GPU unnecessarily they'll get more uh fine tune but it will not be like you know we this bust is over and then let's move on it's not going to happen that it's not yeah it will never go to prenative or anything see it has really opened up a lot of possibilities what people thought was science fiction is the reality we live in right and the other interesting thing about AI uh is this right so it is ubiquitous actually it is there in our life in ways that we don't even know it is there maybe it is scary if I'll say it that way but it is real scary or not example whether what food you order you don't determine you are thinking you are determining it right when you go to the app whatever comes there you order and that's a recommendation engine that's running behind right okay so that's on the food thing what clothes you wear what is your fashion today is absolutely not determined by you it's determined by what you read and where you shop from right let's take to the deeper level what kind of a person you are what is your spiritual interest what are your views on uh you know some of the core values that you are made of you may think you are determining it or your friends are determining it your family is determining it your community is determining sorry to say it's the internet that is determining it the recommendation algorithms are determined with what you watch your movies your YouTube videos are determined by the algorithms that is going behind right so this is done this AI part it's no more burst bubble and all of that right it is done it is in our life it is letting us think in a certain way it is letting us answer in a certain way it is building your personality or taking away some part of your personality it is shaping your thinking it is shaping because your shape your thinking is shaped your value system gets shaped it's far more possibly we are hybrid humans now right we are AI in ourselves AI plus humans we are we are influenced by really right no definitely it's it's too deep and we are living our life right so our thoughts values leadership styles how you talk to people around you how you react in a certain day are also influenced by AI okay so then we used to say that it's a mind which drives our actions but now it's also the it's also the algorithms which drives the mind right that's where I'm saying we are all hybrid humans yeah yeah we are we are not anymore the same human yeah they not okay so what advice would you give to Executives or other cxos looking to implement AI strategies maybe short answers sure don't look at it like a AI strategy or something like that uh that is the first thing uh look at it like a strategy and figure out how data can make a difference in that then it becomes a data driven strategy and the strategy will come out with so uh that would be my uh thing and all Executives come with a lot of experience and exposure they're all well rooted so being rooted in the fundamentals is very important being truthful to the strategy and to the organization is important being truthful to your people is important AI becomes a means to one end if you have to use it so last question uh what are the three common mistakes AI teams make when crafting you call it data strategy AI strategy and how can they avoid it them uh one is see again the principles don't change problem has to guide your solution solution cannot pick a problem right so figure out what is the problem and fig and let the the problem be the guide let the data be the uh you know one that provides you the right way to solve the problem second is uh for a leaders I saying it is essential to note this that process codes problems programs all of that you can write and you can always rewrite and these will always change but what you are building for any organization is people so it's extremely important to stay true to the people that you are responsible for so that is the second thing uh you can get carried away you can do this you can do that Etc but don't miss out why you are there in the first place which is for the people okay so just a bonus question uh for the audience U three forums uh or two forums where Matangi loves to to spend her time if possible we know Matangi time is like gone through all of this meetings every day probably but then if possible if get a chance what two or three forums that you like to read Around AI specifically uh see definitely analytics with there is one thing that I generally go through the analytics India magazine uh articles that come in I do go through that uh some of the other things like K which talks about startup ecosystem and medium post for absolutely specific problems to solve some of the medium post are just just amazing right uh we were just uh we were just discussing about fine tuning Etc yesterday with my team okay and there's a post about J J Almer has written about R that's very critical and that's that's a medium post rate so medium post gives you some of the foundational nugget that you need to get on to a field very fast so hence that also and thank you Matangi and I'm sure all of you must have enjoyed the conversation U so look forward to all of your comments and uh yep till then stay safe and uh stay happy",
    "transcript_chunks": [
      "[Music] so hi everyone good morning good evening to all those watching across the globe uh this is the AI data podcast and today uh I'm your host my name is Punit and uh I take care of labellerr uh we are a data engine for AI team Sal companies reduce their data labeling time and speed up the and enable higher data accuracies for their fine-tuning and building AI projects and uh what I'm today excited about is we have a very special guest on our episode today uh Matangi Shri uh is she's Chief data officer at UB uh she has been she has had uh data science initiative at gojack phone pay CD Bank to name it at all and more importantly to understand she has 100 plus patents on her name uh wow and she's one of the top 50 influential AI leaders in India uh so welcome Matangi uh to the podcast thank thanks pun thanks for having me it's glad meeting you uh and always a pleasure to talk to you welcome mang uh so Matangi uh let me just uh start with a question right uh interesting question which I personally wanted to ask so if you would would like to briefly introduce about yourself with a Twitter pitch how would you do that Twitter pitch is it yes um so for many people data can be a job or a profession for me data is life that's interesting okay and now can you can you kind of give a two liner brief about your journey overall all about what you are right so that will help yeah so I've been I should say I've been very grateful for my journey in the space what started out as when G came into campus",
      "interview and uh we were interviewed by this place um so it's going to be 20 years uh experience that I have so when they had come 20 years back uh and they explained what is the power of analytics they said one of the things that G where to start a windmill analytics would decide which would be those locations so that kind of caught my attention on the decision making power of data and how intuitive driven or domain driven decision making very human driven decision making can be transformed and if I can use the word democratized by data decision making right so that kind of caught my attention then that's why remember this example vividly after two decades that pulled me into to this field um forever uh from that perspective I am not a trained computer scientist I'm not a trained statistician I'm not a trained o expert I'm none of those so but whatever I have learned I have learned during the job and somehow or the other have been in the right places uh to learn these uh Technologies and hence get an opportunity to leverage them to build greater business impacts so one theme throughout my career has been how we can use the power of data to build better impacts and when I mean impacts it could be higher revenue for an organization or it could be uh a better uh experience for a customer or it is maybe putting a smile on someone's face any anything is an impact right so that's where um my journey has been so I've kind of got the ringside view of watching this space evolve from a very static field of uh an analysis or statistics to a extremely Dynamic field at scale decisioning",
      "using computer science being just a transformative Journey for AI and pleasure watching this field unfold uh and in some way be part of this field um has been has been has been very great so that is where uh that is how I would look at my uh professional Journey uh in uh in all these years so I think I think uh you know you made a point right you're not a trained statistician you're not a trained computer scientist and you're are not trained or or right but I think I think the training that you have got is something is not can't be can't be achieved just through forces I mean right I mean you have you have actually experienced that all right so I think I think you are amalgamation of all of these experiences so what we what instead of just saying Chief data officers what if if you have to device a new name uh to your uh maybe the role what would you define that as um so data for impacts that's what it is so maybe Chief impact officer that cool Ro that way so go got it so so this this uh of this experience of yours back in the day has been the the inspiring part of your career is where you pursued the career in in into Ai and data science as such would you like to call this as that right so see uh I'll tell you how the how I was part of the field so I started with G Consumer Finance uh later on it became genpac but I joined u g Consumer Finance and uh then U uh our role was to build risk score models uh using statistic te and then even then",
      "we used to do large scale transaction uh models so what happens is when you swipe a card there is an over limit so I was working for the US market fairly Advanced at that time because for over limit personalization we were doing like when you had a card and it had over limit let's say you have th whatever ,000 is your exposure and you're over the limit of $1,000 whether should I allow you till $100 more or whether should I allow you to $150 or more or whether I should allow you to $200 or more even now when I think of this problem is baffling to me because we were solving this problem 20 years back right and through so this model had two components one was a decision tree which kind of says what is the uh segment of the user once we had the segment of the user then we had different limits right so segment is in rows and uh different limits and columns then it became an O problem an optimization problem how you are kind of uh making sure that your exposure is limited but the because if you allow over limit these are the guys who will also spend more on your product and give you more finance charges so how do you maximize income without uh you know compromising on risk rat so that's the uh optimization problem that we did and this went real time like this went live real time as the person is swiping we would say what is the dollar amount that it is and then the trans system would act accordingly so that's where I feel uh I'm very thankful and I'm very fortunate to somehow have figured these problems out somehow to",
      "have landed in these uh areas so this was the field but however largely outside of where we were Etc this was also a very um static field that time so an example could be somebody wants to find out why did my sales go up and why did my sales go so somebody will write hypothesis at the maximum you will do a t test or a z test and then say is it statistically significant or not my marketing campaign did it work or not things like that right this was bulk of problems that people were uh handling except for few lucky ones like me were handling such scale problems but even I was handling a mix of these right this field slowly and steadily with explosion of more data with consumer companies coming in you know like the likes of Netflix and Google and everything and the compute becoming cheaper right uh so then the algorithms which were there suddenly started becoming to use uh these are your we talk about deep neural networks even in 1970s 1990s Etc yeah and so whether deep neural network or not some of the machine learning Concepts kind of took off at that point in time when the availability of data increased and the compute kind of cost became lower then it became more more about uh how do you liage the algorithms for these large scale problems so the world of Statistics kind of merged into computer science that way and a lot of traditional statistical use cases uh you know the single decision tree became Ensemble decision tree and and things like that became uh front ending a classic example of that I'll tell you uh when uh let's say we were doing U call center analytics",
      "or we were doing ivr training right like natural language ivr this is not natural Lang language conversational engine but notal language ivr so the ivr could say you could say things like it would say that and then say you know I want to book a flight and things like that and whatever you say it's closed it's in a way closed response but it also lets the customer speak um again this is 12 years back we were working on a project like this okay and uh this field was dominated by linguist uh by uh NLP as a field uh especially the nlu part nobody even talked about nlg which is natural language generation nlu was that intent classification sentiment analysis dominated by linguist okay now in a matter of three to five years the division of linguist completely got overtaken by computer science so people who had specialized in language modeling right language modeling is not language models that we talk about here but grammars parsing sentence structures all of that kind of became overtaken by your you know svms or after a period of time deep learning neural networks Etc so and I'm seeing this more and more pun right seeing this uh traditional statistical thing computer just smashes some of these and takes it over take computer vision for example if I were to take an image and extract it I would have to use a lot of vision techniques right about um you know how am I kind of figuring out the region of Interest I would have to use specific of specific algorithms to get there today we talk about donut models and uh the there is not even a two-stage model there is not even a computer vision model and an",
      "NLP model everything gets crunched into uh the same thing and then to top it up generative AI on top of it so the with Advent of an exponential increase of more and more data that is coming in uh the I I would say Brute Force computer science Bay just takes away the finer techniques right that's the evolution of the field okay to be jobs called statisticians I'm sure they are there still but they used to be very prominent in organization so these are the people whom you go to to figure out um whether you are going to use a structure equation modeling or whether you are kind of analyzing an experiment Etc that field is not there today so much more as how how it used to be the prominence is gone right because you don't need foundational statistics to build a machine learning model today it is it is the state as much as people like me who come from these traditional statistical backgrounds uh by way of job can cry and say whatever we want to say the reality is if you have to build a machine learning model today you don't need foundational statistics to operate you can fairly start from algebra and then even if you want to understand foundations you can algebra gradient descent back propagation and there you are you can straight land in um building machine learning models right so a shift it's a shift from fine-tuned high nuanced mathematically shaped field so mathematics is important but what's more important is your ability to handle large your ability to kind of um understand U some of the algorithms you know that is built on at a at certain level but then from there you pick up pick up",
      "pick up and move right into the computer science area so that is the fascinating thing for me about this field when people used to say oh you built a model let me see what is this model okay what are the top variables that you know you have uh you what is your why is income not directly proportional I remember that I remember have you bucketed it why is Your Bucket breaking and all of the questions thrown out of the window what buckets when you're talking about if I built a take even a random Forest right forget even neural networks take a random Forest I have built a tree with thousand uh trees in it what bucket what will break how will you even find it out can put some lime sharp examples on top of it but uh it just smashed people who were talking about explaning know to another level of bafflement right the field just opened up so yeah that's what technology can do right what people were cutely working on you know the wave comes and then just hits you on the smashes you on the face and goes that's well that's possibly describing the growth of Technology yeah I mean I remember uh since you talked about this how it has moved from Linguistics to to just everything in gen and just put in the data and then these algorithms do all that stuff uh it's I remember doing the Cs 2 to4d uh it's a Stanford course deep learning and natural language processing and how far I mean that was an more like an introduction back in I think around 2014 or something and today it's like we are not even we were at that time we were talking about",
      "breaking that thing that okay it's not just basic model you can put deep learning in there and then we are even now talking about getting a that NLP deep learning now is also being called as the word to is now anymore is not s it's it's just gone but what gave R to a lot of things yes exactly that is where the embedding Concepts come in yes yes I mean again I mean in in general if you look at it it's it's all transition right I mean the concepts and even even I wonder this question at time um even my team members uh talk to me and asked uh where should I start and at that at some sometimes I feel how would I mean they would have to go through all of those steps what we have gone through and even a lot of stuff that even I haven't gone through and I mean you I mean you have been there from even more than uh even even more a decade even before that right all those experiences that you gain so uh at one time I mean and then there are some new uh pressures coming to me and saying that okay we know Lang chain we know this and we I have built this model and I say okay but then do do I need to tell them that okay you need to actually go back to basics and fundamental and see that okay how the this this used to work and then how it changed from CNN to rnn's to lstms and then someone said oh Gans are there but then there was someone who said okay can we build a Transformer thing out of what challenges what RNN had right",
      "so I mean how how do you uh I mean what is your opinion and advice to someone uh trying to get into this right I mean even I'm trying to wonder this correct so I don't think uh we can do away with foundations fundamentals have to be there to take a text problem you got to understand tokens youve got to understand you know how a basic uh bag of wordss model will work and why bag of models works even today um our um voice engine that we have deployed in the market uh has an ml based uh you know model with back of wordss approach so I don't think you can just replace bag of words uh you it has its value as well right number of rows are limited it's a well-bounded problem why would you not try and use some of those are at least Bas line with those right bag of words to embeddings to sequence to sequence models to Transformers then uh you know attention then your uh generative then your Lang large language models and then the variations of large language models would still be a course right so I don't think because if you don't understand bag of words it's going to not even see the basic intution of text mining is this that you are converting unstructured data into structured data you are converting words to numbers right that Crux has to be there you can convert words to numbers using a tfidf matrix or you can convert word word to Wi using a embedding uh kind of approach right or you already somebody has converted and you can use that for your transfer learning the Crux of it is how you are converting words to numbers and",
      "there are multiple methods to come so I think that Crux of it will have to be there and in the world of let's say structured data same thing so you have to note some level of Statistics some level of understanding at least you should be able to fit a yal to X plus b in paper and you should understand the formula of a and b in that right that's that's basic linear algebra you you got to know that right then you do your stochastic gradient descent Etc you you kind of do your gradient and then stochastic gradient this will give you some level of and some intuition into how ml models work right then you can from there Branch out to a tree how entropy works and then get into gradient boosting where what is gradient in gradient boosting and then from there take it forward because you will be doing this will help you appreciate how data behaves when it is when you are predicting it through linear models and how data be behaves when you predict it through nonlinear models right what is that multiple layers in uh deep neural networks do right how does it break your linearity in yal to X plus b right so those and why is that needed right when we when we do certain problems you kind of understand uh you know why is that needed right for all of that the foundations are very important so if I were to I might sound very traditional now I have to say these all these hashtag words and disclaimers and everything in the world we live in because people immediately look down on you if you don't say uh I use generative a to brush my teeth right",
      "so uh that's the funniest thing that I've heard today so so it is important to get the fundamentals right because the fundamentals will help you appreciate complexity more than anything else app helps you appreciate it that makes sense so in a way the entry barrier though it looks like it has decreased for someone new trying to get into this but actually for them they still have to travel Traverse all those if they don't travel that on yes they don't travel on day one but they they got to travel I and other point is I don't know how many interview panels are not asking questions on um entropy gradient descent your refence score Precision recall uh Etc so I don't I think I'm sure most of the organizations are very thorough and they ask these questions yeah for people to uh you know come in you can say I know rags and I know pine cone and uh you know dbx yesterday got released and I tried it and I know the difference between tbx and Lama 3 and this has 13 billion that has so many billion parameters Etc but uh point is I I hope s I'm sure sanity is providing in lot of companies and their interviews are far more foundational and fundamental so there are not people who get carried by hype people who know you know to crack or take hugging face get to uh you know few notebooks uh run those few notebooks and then I know I can talk and then uh I read the last four medium post then I'm talking so I I assuming that interviews today interviewers are also kind of encouraging foundations and fundamentals because that solves a business problem more than anything else true",
      "no no that's that's something as an advice we thank you for sharing this advice for the other organizations as well and for us as well um so uh quickly jumping onto uh a new another question right so what are your personal and professional aspirations for the next three to five years I don't know whether this question is probably valid or not but then I mean I'm I'm sure at every stage in life sure even after achieving so much uh what is the what is next for you in your personal so one thing I'm very grateful for whatever it is today right I I do think in a lot of ways I don't deserve many of these things I uh that is and I've been in the right place and that's that's the first thing and uh I that's how I think and that's how I operate then um of course so in terms of personal aspiration I do want to be a better person a better version of myself uh and meaning God willing I want to use my services for larger social good uh and I started the same data is life so without data that larger good doesn't come so that's my means to an end uh but I'm getting um into a space where uh the impact that I create has to be for you know larger good uh possibly more uh social good also and if my profession can help me get there that's the place I want to operate in thank you um and so how do you Foster motivation and collaboration within your team see everybody is motivated right um as a leader all you have to do is to stop demotivating them then you let uh people be",
      "there and they'll figure out ways to solve a problem uh to your energy you can contribute uh somewhere if uh things are down the chips are down Etc but larg um and large part of uh whichever teams have been part of I feel people are there for a purpose and whether some people realize purpose and some people act on a job without knowing they behind a purpose but everybody largely are there for a purpose and if you are already you have a purpose and you love your profession and you will be motivated right and your job as a leader is to make sure that they get those problems day in and day out and you are unblocking things for them uh you are kind of operating at a level which enables them to operate on their own and you are there to kind of clear the way um that comes in their way because internally you cannot motivate any motivation is within um so just because we give an award or just because there is even a salary rise that's not motivate anyone they're not uh you get the best people in the team and then you let them do what they are great at and you kind of unblock the way for them you get the best team and and so most importantly it's first the first step is to get the best people in the team yes is the most important yeah so what are the three attributes es maybe I don't know whether it's easy to generalize but three attributes that you look for in a team member uh best team member best team member the the Curiosity to learn the ability to be not afraid of failure uh and uh the",
      "attitude to contribute of a an opinion I would would seek is there are folks who are saying gpus are the biggest bottleneck but then then there are folks who are saying uh refined data is essential for AI so according to you which one of these two is the biggest B like even if you have to kind of weigh between these to Talent creates technology okay it's not the other way around so the biggest enabler and the biggest bottleneck is Talent right so you can make gpus cheaper with great talent as well and if it's a great talent they will know how to work without those expensive gpus also so if you at a very high level the enabler and the bottleneck can be only people it cannot be anything else people create world right so at the next level business problems are extremely important whether to know whether you need those gpus or not uh so I think it uh we can do enough with the existing gpus possibly in a lot of cases uh where where it really demands uh let's say you solving a large scale vision problem voice problem Etc where it demands I know uh GPU is on the run but I'm sure like I said Talent Finds Its Own Way the next way of innovation like how it was in software it will be in Hardware it will smash the prices down of gpus pretty much right this has been a uh catch game uh right uh uh where when the compute was very high compute came down it crashed to a level then algorithms came up then there's a lot of innovation on algorithms Now The Innovation will go back on compute and it will go parallel also right",
      "now we have quanti models we have models which can run in CPUs uh so there is going to be uh work done on that also F instead of fine tuning you can do few short learning so there will be both sides happening let's say a few years from now we are not talking again talking about a GPU bottl neck world or uh something like that the expansive part of it is going to come from problems being available for AI to solve and what about and what about refined data how big is this bottleneck everybody said that people scraped internet left right center and that's how language models built just think about it who would have thought thought a language model would be built by a company which is not like Google or Facebook or any of this the first language model came from open AI from open data right so who would have thought that we would have thought everybody followed up that's a different point right but uh when people used to talk about Google or uh this was the story right because I heard this from the investor of Google uh people said why do you need another search engine alav Vista was already there why do need Google and the way Google became what it was it had all its data right and hence you would naturally think an AI algorithm would need that level of data and these guys who had the Monopoly on data would create that somebody somebody broke that itself right where they had no they had no right to data so to say but then they were able to build uh a model completely built out of the world's data so I don't think refined data or",
      "something is a problem that is in your head they can always find out a way uh and uh that's what I'm saying neither refine data nor Hardware is a problem if it is a problem it will get solved by humans matters Nothing Else Matters and and in terms of where do you see the future of a adoption and development heading right I mean prob from a considering industry and geographical Trends see there is China is catching up there is I mean us already and then India is also catching up uh and and recently Middle East announced a huge initiatives how in your understanding how is the Dynamics going to play because the the the fundamental understanding or or the saying is whosoever becomes the leaves the AI is going to be the next superar so it's an important question politically and geographically I mean on a global global certainly play a huge role uh and uh this some of the initiatives that is for nonprofit like a for bat Etc just mindblowing there's also a lot of funds fiding on these initiatives from the government and uh and we have been at Forefront of this Innovation uh I was part of the GPA Summit that happened uh where the Prime Minister had come and his vision for AI these are times to 11 right I come from the time where data science got spoken only by few right AI being a political agenda a national agenda uh in both political and National agenda in the in the country it's um a lot to see right I think India is going to definite again from the same fact of stemming um Talent built technology India's rich in Talent India's Talent can get on to AI faster",
      "and though large part of Internet is in English the large part of world in world population is non-english right so we need and that sits with India right so you have to build AI for India and then you are building AI at scale it's very simple so I think this is going to be uh a huge area for for India to play I would say India will transform AI so okay so one um one question I've written it down and I'll I'll you know speak it out so Gary Marcus said right things are about to get a lot words for generative he he he has his his blog Gary marcus. sub.com that's where he elaborated but let me just simplify what he went on to saying when will the Gen bubble bust I mean he he's a critique is a is a very popular critique about gen and uh and he says why and how it could happen in the next 12 months uh I don't know whether you got a chance to kind of read what he said but then what is your opinion I mean critiques of genbi are saying that it's a bubble burst that is going to happen in the next 12 months but then even myself uh talking to organizations and they are using organ this gen uh in in a phenomenal way as well so why why these critics are saying so that the bubble is B going to B one Nvidia booked its profits and made its money where bubble or not right so that is the first thing that has come out of it um and uh you know opena became the marketing arm for NVIDIA so these are things that uh unfolds I don't think uh",
      "it is like the crypto bubble or anything like that uh it is uh it is here to stay right there might be a bigger bubble bigger bubble might become smaller bubble and here to stay Etc but to say this will go away and it will burst and everything no it's here to stay it is here to stay because of few things it's here to stay because it's not some random magic that is happening built through science um built through Transformers which has been proven very well it's just amazing to see the technology in action uh it will find its right you usage like any other technology will find Its Right usage right so uh media blows up everything uh quite large proportion and then things settle down it you need marketing media did that marketing for for Genera and it is here to stay and it opened a lot of people's eyes and ears to AI not only generative AI uh one of the things I've been saying in a lot of forums is this that earlier we used to wonder whether AI will get a seat at the table today it is the board asking its Founders saying what have you done with generative AI right yes so it is table St and that will stay that table is not going that is not going away right uh it will find its better use cases it will get more refined maybe people who are burning GPU unnecessarily they'll get more uh fine tune but it will not be like you know we this bust is over and then let's move on it's not going to happen that it's not yeah it will never go to prenative or anything see it has really opened up",
      "a lot of possibilities what people thought was science fiction is the reality we live in right and the other interesting thing about AI uh is this right so it is ubiquitous actually it is there in our life in ways that we don't even know it is there maybe it is scary if I'll say it that way but it is real scary or not example whether what food you order you don't determine you are thinking you are determining it right when you go to the app whatever comes there you order and that's a recommendation engine that's running behind right okay so that's on the food thing what clothes you wear what is your fashion today is absolutely not determined by you it's determined by what you read and where you shop from right let's take to the deeper level what kind of a person you are what is your spiritual interest what are your views on uh you know some of the core values that you are made of you may think you are determining it or your friends are determining it your family is determining it your community is determining sorry to say it's the internet that is determining it the recommendation algorithms are determined with what you watch your movies your YouTube videos are determined by the algorithms that is going behind right so this is done this AI part it's no more burst bubble and all of that right it is done it is in our life it is letting us think in a certain way it is letting us answer in a certain way it is building your personality or taking away some part of your personality it is shaping your thinking it is shaping because your shape your thinking is shaped",
      "your value system gets shaped it's far more possibly we are hybrid humans now right we are AI in ourselves AI plus humans we are we are influenced by really right no definitely it's it's too deep and we are living our life right so our thoughts values leadership styles how you talk to people around you how you react in a certain day are also influenced by AI okay so then we used to say that it's a mind which drives our actions but now it's also the it's also the algorithms which drives the mind right that's where I'm saying we are all hybrid humans yeah yeah we are we are not anymore the same human yeah they not okay so what advice would you give to Executives or other cxos looking to implement AI strategies maybe short answers sure don't look at it like a AI strategy or something like that uh that is the first thing uh look at it like a strategy and figure out how data can make a difference in that then it becomes a data driven strategy and the strategy will come out with so uh that would be my uh thing and all Executives come with a lot of experience and exposure they're all well rooted so being rooted in the fundamentals is very important being truthful to the strategy and to the organization is important being truthful to your people is important AI becomes a means to one end if you have to use it so last question uh what are the three common mistakes AI teams make when crafting you call it data strategy AI strategy and how can they avoid it them uh one is see again the principles don't change problem has to guide your solution solution",
      "cannot pick a problem right so figure out what is the problem and fig and let the the problem be the guide let the data be the uh you know one that provides you the right way to solve the problem second is uh for a leaders I saying it is essential to note this that process codes problems programs all of that you can write and you can always rewrite and these will always change but what you are building for any organization is people so it's extremely important to stay true to the people that you are responsible for so that is the second thing uh you can get carried away you can do this you can do that Etc but don't miss out why you are there in the first place which is for the people okay so just a bonus question uh for the audience U three forums uh or two forums where Matangi loves to to spend her time if possible we know Matangi time is like gone through all of this meetings every day probably but then if possible if get a chance what two or three forums that you like to read Around AI specifically uh see definitely analytics with there is one thing that I generally go through the analytics India magazine uh articles that come in I do go through that uh some of the other things like K which talks about startup ecosystem and medium post for absolutely specific problems to solve some of the medium post are just just amazing right uh we were just uh we were just discussing about fine tuning Etc yesterday with my team okay and there's a post about J J Almer has written about R that's very critical and that's that's a medium",
      "post rate so medium post gives you some of the foundational nugget that you need to get on to a field very fast so hence that also and thank you Matangi and I'm sure all of you must have enjoyed the conversation U so look forward to all of your comments and uh yep till then stay safe and uh stay happy"
    ],
    "transcript_word_count": 6662,
    "transcript_chunk_count": 23
  },
  {
    "video_id": "LSAfAFgYdyg",
    "title": "Healthcare AI Insights: Navigating Machine Learning in Medical Imaging | Labellerr",
    "description": "Join us in this engaging episode of the AI Data Podcast featuring Priyanka Makani, a Senior Machine Learning Engineer specializing in healthcare and medical imaging. Priyanka shares her journey into AI, insights on navigating careers in machine learning, and the importance of domain expertise in healthcare AI. She discusses key challenges, the role of data vs. algorithms, and advice for aspiring professionals. Priyanka also highlights her contributions to empowering women in STEM. Perfect for anyone curious about healthcare AI, career growth, and the evolving tech landscape.\n\nConnect with Priyanka:\nhttps://www.linkedin.com/in/priyankamakani\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\n📢 Subscribe for more episodes on AI, data science, and machine learning!",
    "video_url": "https://www.youtube.com/watch?v=LSAfAFgYdyg",
    "embed_url": "https://www.youtube.com/embed/LSAfAFgYdyg",
    "duration": 2105,
    "view_count": 257,
    "upload_date": "20240222",
    "uploader": "Labellerr",
    "tags": [
      "automated healthcare",
      "healthcare management",
      "ai in healthcare projects",
      "predictive healthcare",
      "remote healthcare",
      "ai in medicine",
      "telemedicine trends",
      "ai revolution",
      "preventive healthcare",
      "telemedicine future",
      "healthcare analytics",
      "health automation",
      "health analytics",
      "telehealth innovations",
      "data analytics",
      "wellness tech",
      "telemedicine innovations",
      "wellness automation",
      "remote wellness",
      "remote health",
      "telemedicine apps",
      "telemedicine growth",
      "telemedicine tools"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] hell everyone hello a Enthusiast welcome to AI data podcast uh I am pun gindel your host today uh so we have a special guest today with us Priyanka makani uh she's a Powerhouse in the world of machine learning specialized in health care and Medical Imaging uh before we Deep dive into the fascinating insights she is about to share let me give you a sneak peek into what's in the store so we'll be talking through priyanka's journey in the machine learning landscape exploring the challenges she has conquered and gaining invaluable advice for aspiring engineers in healthcare AI so stick around as we touch upon the empowering experiences of women in stem thanks okay so why don't you actually start with a quick introduction for our audience uh so they can know you better quick quickly in maybe in just one minute sure um hi everybody um I am prianka and I'm a senior machine learning engineer uh working for idea evolver um my path or my career path has been sticking around roles of machine learning Ai and data science uh so I have a uh experience for a little over 4 and a half years uh working in the US uh I graduate I graduated uh with a degree in computer science from Chicago and I also hold a bachelor's degree in the same um my journey uh or like my passion passion I'm being very passionate about like healthcare industry of and I have close to three years of experience just working in healthcare industry itself um and I just want to keep continuing doing the same what I do um thank you prianka so Priyanka why don't you actually talk about that pivotal moment uh where you actually entered into this journey right some very very you know put some more light on how it happened that very moment right uh I'm sure the audience would be excited to and curious to know about that oh sure so like I got interested into machine learning uh little before when I was like applying for my Master's program and I was looking through coursework from from different universities that I was interested in and surprising ly um um introduction to machine learning got my attention so which was my first ever cour workk um uh as uh deeper to learn to learn more about like machine learning itself and it was like a I would say A Brave and Bold move um as would my seniors would say at then that taking up a such a course it's it's like around 2017 and it machine learning was still thrown around as a jargon not a lot of people would know about it not a a lot of help you would get or like career counseling you would get uh because only uh PhD students would specialize in machine learning or artificial intelligence but uh it CAU me like a yua moment of my life that I really want to pursue my career into it I although loved computer science and algorithms but uh um I don't want to mix those two together because a lot of uh statistics and Mathematics also goes around when you want to specialize in machine learning and artificial intelligence um and also having a deeper understanding of different data as well so that was like uh and that that that's like history and since then I um I've been very much focused on like polishing up myself because you it's always like you're always like a student when you want to step into computer science I would say and definitely want you want to pursue it uh with um data science and artificial intelligence um so some of the factors that really like motivated me throughout my course work were like having those personal projects also deciding to go on hackathons uh and also uh making sure that I am building my profile towards what I really want um so targeting specific companies um where I would make sure that my first career step would be into um AI so which I was for enough uh right from my internship uh through my uh full-time roles I've been working in this uh industry in Ai and especially uh from past two years or three years I would say that I got very much passionate about healthcare industry so that comes like a later part when you want to have a career trajectory in AI itself like you want to do generalization versus specialization so um this is kind of like a strategic challenge I would say say that um at the onset of your career that you would want to decide that you want to specialize in a niche area like computer vision or natural language processing or you want to have like a broader set of skills and still be able to wear many hats in a day so since then um yeah it has been like a good career growth for me and um uh and like I said like healthcare uh is something that I'm very passionate about Okay cool so that that leads to me uh in fact two follow of questions especially from a uh aspiring audience uh you know one someone who is interning somewhere in the AI space right the question is always like the keep figuring should I go for Masters so I mean someone who hasn't done Masters right even I keep getting this question often um I'm I'm thinking I should I go for Masters I should I gain two three more years of more experience I don't know I'm I'm I'm sure your Juniors have might have reached out to you already right regarding this question so that's one question that usually comes out and then would love to hear your thoughts on the same um so I would like kind of um again like it's coming from my personal experience um so there are two two ways to see it firstly if you want to pursue master's degree with an intent of going for a PhD then definitely that's the only way to to navigate your PhD program is going through a master's program and then you continue from there after but if you're very set upon that you want to stick to Industry setting and you are like confident enough with your skill set and it's going to ever be ever growing because once you get into industry and you are like um learning hands-on experience the tools and platforms that are required for a data scientist or or a machine learning engineer then I guess um I would personally skip that step of like again taking a step step back or like um and going towards a two-year committed program uh because then the stakes are high as well because then the job search could be um difficult depending on like economic climate or like which country you land up in so there are two ways to see to it uh I was pretty much set on like coming to the states uh this was my personal choice and uh I uh also had a very vague idea towards stepping into like research space as well uh fortunately my full-time roles have always been at the intersection of research and development so I did not pursue a a a part to PhD but in my industry setting I'm kind of doing the same so uh I would recommend uh people who are very research Savvy would want to go to and take this part there is nothing uh there is no harm in that as well okay cool cool and the second question um is usually oh should I should I focus more from U say I want to focus on the horizontal you know skills saying I just want to be in more computer vision or in NLP or should I look it from a perspective of like okay healthcare industry or that industry or agriculture industry right so because because that's where is again the new question that keep coming in their mind so what is your thoughts on that and probably you can answer why you chose healthcare industry um what was the motivation behind that and maybe that might help the audience yeah this be computer vision be it natural language processing it comes down to those numbers how do you process your algorithms on top of it and try to make uh key insightful um and actionable um you know results out of it so that's the key um area that I would want people to focus more rather than worrying about like should I on the onset of my career I should focus just on computer vision and natural language processing uh I believe that um you should be able to adapt as well because even if you're working on a um any project for a tech giant there are different data sources that they also get in it could be multimodal so be prepared um to you know thrown a gauntlet of um a challenge to you anytime and uh that's the key to it again is like to learn and tackle that data so that's pretty much important and that's what like I guess um is demanded even throughout any recruiting process when you are like uh going through an interview process for any machine learning role it if you given such and such data what would you do with it so that's the um key aspect of uh um you know understanding that where you would want to set your foot um and again it's it's good to have some sort of like um experience or to be called as a veteran in the industry be it in healthcare or be it in automation um so that's great to have but uh you need to understand that the basics is always the data so try to you know first focus on that cool cool and one uh uh you know interesting uh aspect that I've seen uh also and maybe you know you can again clear the air around that thought I'm a machine learning engineer uh working on you know fixing the data is not my job it's a data Engineers job what are your thoughts on this like uh we do hear about um like data scientist just specifically just working on um data side of the problem and then machine learning Engineers working on model architecture and like modeling of uh these algorithms but I guess um these roles are sometimes just used interchangeably so don't get confused um meaning that if you are a data scientist then then you might end up also doing some modeling and architecture stuff and also a machine learning engineer definitely has to work around the data too right right and in in your case if I ask you right looking at your fast project how much percentage has been where you have been spending your time in the algorithms versus the data oh to be very to be uh very honest like especially when it comes to healthare Industry don't expect very clean label data sets like dogs and cats available on the internet to just download and then you have like gazila of data to train on but Health Data itself is very intricate it's it's it's um also not everybody body can understand because again we are not coming from a background in medicine or pharmaceutical uh so there are a lot of understanding that goes into it and then there have been times where I have also cleaned up annotated uh Shi the data and then used it for my machine learning model so uh there could be cases that you know you need to do uh right from annotating your data labeling the data clean in it up um um and then um you you have to use your um machine learning models on top of it top of that so I would say that yes there have been cases where I've been spending 50% of my time just on data itself and then 50% would go on modeling okay okay and in general as a team um for any AI project development uh let's say let's just just even build one or two models or iterations of the you know Healthcare AI space how much has been the percentage effort for data versus again the algorithms um collectively as a team right from data collection to to the to the deployment absolutely so I guess it has uh what I observed is like it's an iterative process again meaning that even when you start a data Pipeline and then you keep continuing your efforts on like modeling and then you have like uh a version one one of a model based on the data that you got and then there are some more uh nuances to data itself that you find out later and then you go back and then you know fix that and then it goes on and and again so what I've observed even as a team it's like um U strategically these tasks are assigned uh so that one is or like couple of team members is of course uh pretty much uh have the hang on uh in terms of like how to deal with the data itself even when there is something that is um you know there is some sort of like a data error then how would you want to tackle that or like debug that kind of a problem and then there's some few experts in the team who would mostly take care of the architecture but also have knowledge of what's going on in the data pipeline too um so I would say that yes that's how the team is usually divided um even though uh most of the team MERS know what's going on but there are few experts that are assigned to one of the task okay okay and one another question um you know how do I so when when looking out at any opportunity uh right uh in in the industry right um how do I choose a team where one team where there is more u i mean domain experts but probably I will be the only one or someone else will be the only one who will be machine learning engineer but then there is an idealistic view of oh there will be very big team of ml scientists you know data engineers and so and less of domain experts probably you know in a in a setup right depends on the organization structure so these are kind of the choices that come across uh in front of the audience Right ispiring audience so how do you think what is your suggestion on this how how do someone would go about you know exploring that opportunity which one they should choose if they have both the options I guess firstly it depends on the site of the project as well and uh how much wealth of data that you will be dealing with and of course having domain experts is always always like a bonus Point uh especially those who have like years of experience plus a PhD in the same um because it comes with its own um um you know um advantages of like learning or like diving deep into what's going on um in the data itself because there have been cases where um I've seen that physics have been involved in machine learning models or like um you know some sort of like biomedical engineering is involved so I'm not a domain expert in that but um I might be good at understanding what kind of model architecture to use but uh to understand those little nuances what what goes on around like in the first principles of um computer science or physics or like even understanding biomedical engineering is is always an add-on to the team uh again it it could be just uh focusing in terms of understanding um novel approaches how to take take the problem through um because um even when you working on an R&D problem in Industry setting you are time bound um so you would want to make sure that um um the resources are utilized optimally throughout yeah yeah yeah so business is always wanted yesterday so it has to be done timely though ml development process is an experimental process and not a very certain outcome oriented process yeah yeah the good thing about it is that you know even from your failures you learn a lot so that's that's a good part about like um uh you know being in this space that um uh to some extent yes it's blackbox people to talk about transparency and we also focus on transparency especially in healthcare AI because the stakes are high so you need to make sure that uh you know if you crack open a machine learning model what do you see and how do you explain to a medical profession professional Okay cool so another followup question though I didn't have this question uh in mind earlier so but very interesting one you working in healthcare you know the end impact is safety health of someone right uh do you have you know while while developing these models do you also feel some sort of emotions um which capture around you especially like okay you know what if this model doesn't work in that the expected with being a technical professional I'm not a medical professional yet is firstly um understanding how do you like evaluate your models so in in machine learning we call as Matrix do you want to uh uh focus on accuracy or do you want to focus on Precision so what are these Stakes so you or you want to just focus on recall so this all kind of um you know U is a game changer in terms of how do you finally uh present your model uh higher up and then you know you are confident enough to go about and go for like B data testing or like even go for another round of testing on these models if you look back on your U this journey right um what are the top three challenges that you that has occurred in your you know career um whether in terms of the technical aspect if I mean uh from a technical standpoint second navigate through working along with the other team members so both so more of a you know technical aspect as well as a professional you know management aspect so yeah if you if you can put some light on those yeah I guess uh when I was just starting with my like job search um um when I was still a graduate student so I guess the first thing that I faced was lacking industry experience so I was still a master student then and uh often times recruiters do want have some hands-on experience but um you know what really helped me was like getting the right internship or participating in hackathons or having your own personal projects building your own profile so that you know you are um kind of setting stone for yourself and uh would want to uh be ready for any interviews for full-time goals uh so that you're able to at least prove yourself that uh that you're able to bring those academic theories and you know work on real world problems again um the pivotal role of you know understanding the specialization that I want in my career um computer science is great then of course um started with machine learning that's great but then what do I really want to do out of it so understanding that I really wanted to pursue career in healthcare industry um so it might take years for someone or it might be you know pretty quick for someone so it's it's really like uh you know case toase and um sooner you know what's your calling so it's better cool and uh another question that I have I have here is um if you can talk about uh some specific scenario of a project like what was a data like you know on a high level and what is a problem statement like you know what has to be for example predicted and then any specific challenge that you fa there and how did you solve it so maybe more in summary if you can talk about genetic in a genetic way um I guess in my last company uh we were working on this uh robotic surgical tool platform which is very fancy uh it was my first time approach on working on hyperspectral Imaging data so this is unlike those RGB fancy images or not even like x-ray images or like CD images I've worked on like CT images before but but this was all together something different so it's like uh different wavelengths captured and it's like a cube as you can imagine of data so it's like with multiple channels and then you need to and you can't even see in in a in a regular viewer like a photo editor what's going on because these are different wavelengths um and the task was to identify some critical structures during the surgery and alert the surgeon on time because it's navigating through some surgical tools so you need to understand what's going on in real time and um also inform the surgeon if there's going to be any catastrophic damage that that's going to happen um so this was my first time approach of uh you know working on a data like that and uh it was very interesting to work on it of course came with its own challenges very unique um and um still it was in its R&D phase um cannot discuss much more about it so um um but I guess um like I said um coming back to a question that you posted earlier that you need some domain experts so thankfully in my team there were some domain experts to help me understand this kind of data um and um understanding physics spectrometry biomedical engineering behind it that was alog together you know going back to a classroom and then you know going through uh I literally had to go through some of the books and like research papers on it um but um I guess the key challenges to that particular problem was again like um The Domain expertise that was required and then um even creating the data pipeline from scratch and then uh of course the modeling um was there okay so in duration just to quantify it was it how does it look like like months and how many months or weeks um I guess I I guess uh the most most time was taken for like the data itself um again like it was it was done in like um I would say in chunks so there isn't like a specific number that I can you know on top of my head that I can say that okay these number of weeks we would spend on it but um I would I would roughly estimate that 50% again was gone through you know just data itself cleaning up labeling annotating and then um then I would say roughly 30% on modeling and I guess 20% was always Improvement um or like post talk analysis as you would say that to understand how the result are what can we improve um so that's that's how we got about it okay cool okay so uh so you I go went through your one of your LinkedIn post about you know you sharing thoughts around um Powerhouse women and Michelle Obama right on on stage what was it and how was experience like so yeah oh yeah that that was pretty interesting in this year um I was reached out to like share my thoughts um on like women and uh Sam um I've been kind of like part of like various organizations even during my master's program I conduct conducted one machine learning workshop and then after that I was a speaker uh in one of the events in Baya uh and also um B Ambassador for women women of AI um so it has been uh um uh like it's it's kind of like giving back to the society right and so so what was the like high level discussion points or the outcome points that were being discussed there like just a high level if you can put some light oh um it was just about like just you know thoughts on um you know women in engineering or women in stem and then uh there were other such women who who are like uh in this field in stem field especially so um I guess collectively just uh you know having our thoughts together at one page okay okay and what's your perspective of like how uh you know empowering women in stem and a as a field and it was only stem earlier now it's stem and a so so yeah I mean any any thoughts on this and what I mean are we doing enough or we you know to ensure more diversity more inclusion so what's your thought I mean or experience has been till now from that angle um yeah at least with my observation personal observation that I see more women coming up and like pursuing a career in stem which is uh pretty motivating to see uh especially how difficult it would be just to start a career in engineering um when I I imagine um you know couple of years back you know and versus what's going on now I guess it's it's more about like a of technology and access to it which has Crone um and um in this case I would say like social media has played a good good role in terms of like promoting um these certificate programs or like expedited programs online um you know promoting uh to learn and grow their businesses so I guess more or less people are using AI everywhere now um be it to you know it's a like a small business in India or like even you know just doing some social media campaign so I guess everybody's is very familiar about now chat GPT or canva or like you know other Technologies which revolve around AI so it's pretty uh motivating to see that more and more people are coming into it and like it's so healthy use of technology I would say okay good um so lastly uh one of the important things our audience would be curious in know putting in a summary someone I'm a for example let's say I'm a aspiring Healthcare data professional right uh very tons of information out there the challenge is not anymore about that information is less information is too much um something which you would advise that I should pick that first uh you know to navigate into this area of healthcare AI um I guess uh firstly understanding um the job market firstly that if you want to set foot U into this industry are you prepared for it in terms of like um um the data itself so there's so many open source um data that's available which is related to healthcare um why not try a personal project on that which kind of like was my uh personal exercise when I was trying to get into um Healthcare AI um uh definitely keeping uh eyes wide open and trying to see in real world setting how can you um you know tailor any machine learning model to needs of of patient care again coming from a personal experience this is what I observed that um if you are personally or like on a firsthand basis if you're observing something that that can be U you know a live product um that that'll be very interesting to you know think about it and um U thirdly it would be like be ready to be a student because like again comes with its um you know challenges in terms of like um The Domain expertise that's required um it's great that if you have some background in it but uh don't be discouraged if you don't because um I guess um um it really comes with the years of experience once you into it uh but um to get started I would I would definitely encourage someone to go out there and like um have a good idea about like Noel machine learning models that are out there uh Google Health releases a lot of information on it um big Tech Giants are also working on it so there are a lot of Publications out there which um gives a fair idea about how things are working currently U and then getting some hands-on experience with some personal projects is uh advisable because if you are able to understand some sort of like electronic medical records which is you know great because uh that's like uh used a lot or like even chest x-rays or like CD scans so some of these data sets are uh open source and available uh so get your hands dirty on it and uh I guess um LinkedIn again is a very collaborative community so um don't shy away from reaching out to professionals there um people do help and um it's very collaborative again to get advice from people who have been in this industry um so yeah that will be something I would advise to people who want to get into Healthcare AI thank you prianka for this for your insights so one of the insights that I for sure got is the domain expertise especially when it comes to healthcare AI is is really really important and it's uh see as being a ml engineer you have to be a you have to actually dig through all of that it's not just the code it's not just the U just the data processing but it's it's more than that right uh so that's one Insight that I've received second U someone who is looking to get started uh you know they have to do some hackathons internship to translate uh whatever the theoretical knowledge they have received and so to to enable the organization to gain confidence in you so that they can hire you and all any anything else that that you want to like third point you want to highlight here like you know as a more important Point here as part of the summary uh out of all the insights that youve shared I believe it's it's more like uh also um the the learning curve should not stop I would say like be ready to you know spend your weekends on like cor SRA B drings questions uh because um you need to understand that what's going on in the uh industry setting it can be of course Very daunting and to keep up with it the only ways to like practice and learn um so yeah like just keep up with the speed of this Tech Evolution so the continuous learning is something is is is something is one of another takeways Okay cool so uh um what what is the best way to reach out to you in case they have questions uh the audience has the questions so one for the professional uh you know collaboration second uh more from a learning perspective is the same same same channel or different channels like what could be the channel to reach out to you oh quickest would be like reaching out um on my LinkedIn which is with my name Priyanka makani and um um yes if there is some interesting opportunity then I would definitely respond to that and of course we'll be very very happy to Mentor or like even give some vocational guidance of people pursuing in this industry be it Healthcare or not Healthcare um just you know plainly a computer science uh graduate would want to ask certain questions uh around that because my background is uh in computer science basically so um we'll be very happy to have a chat um um and yes yeah that's all that's so grateful of you thank you thank you thanks thank you okay cool",
    "transcript_chunks": [
      "[Music] hell everyone hello a Enthusiast welcome to AI data podcast uh I am pun gindel your host today uh so we have a special guest today with us Priyanka makani uh she's a Powerhouse in the world of machine learning specialized in health care and Medical Imaging uh before we Deep dive into the fascinating insights she is about to share let me give you a sneak peek into what's in the store so we'll be talking through priyanka's journey in the machine learning landscape exploring the challenges she has conquered and gaining invaluable advice for aspiring engineers in healthcare AI so stick around as we touch upon the empowering experiences of women in stem thanks okay so why don't you actually start with a quick introduction for our audience uh so they can know you better quick quickly in maybe in just one minute sure um hi everybody um I am prianka and I'm a senior machine learning engineer uh working for idea evolver um my path or my career path has been sticking around roles of machine learning Ai and data science uh so I have a uh experience for a little over 4 and a half years uh working in the US uh I graduate I graduated uh with a degree in computer science from Chicago and I also hold a bachelor's degree in the same um my journey uh or like my passion passion I'm being very passionate about like healthcare industry of and I have close to three years of experience just working in healthcare industry itself um and I just want to keep continuing doing the same what I do um thank you prianka so Priyanka why don't you actually talk about that pivotal moment uh where you actually entered into this journey",
      "right some very very you know put some more light on how it happened that very moment right uh I'm sure the audience would be excited to and curious to know about that oh sure so like I got interested into machine learning uh little before when I was like applying for my Master's program and I was looking through coursework from from different universities that I was interested in and surprising ly um um introduction to machine learning got my attention so which was my first ever cour workk um uh as uh deeper to learn to learn more about like machine learning itself and it was like a I would say A Brave and Bold move um as would my seniors would say at then that taking up a such a course it's it's like around 2017 and it machine learning was still thrown around as a jargon not a lot of people would know about it not a a lot of help you would get or like career counseling you would get uh because only uh PhD students would specialize in machine learning or artificial intelligence but uh it CAU me like a yua moment of my life that I really want to pursue my career into it I although loved computer science and algorithms but uh um I don't want to mix those two together because a lot of uh statistics and Mathematics also goes around when you want to specialize in machine learning and artificial intelligence um and also having a deeper understanding of different data as well so that was like uh and that that that's like history and since then I um I've been very much focused on like polishing up myself because you it's always like you're always like a student when",
      "you want to step into computer science I would say and definitely want you want to pursue it uh with um data science and artificial intelligence um so some of the factors that really like motivated me throughout my course work were like having those personal projects also deciding to go on hackathons uh and also uh making sure that I am building my profile towards what I really want um so targeting specific companies um where I would make sure that my first career step would be into um AI so which I was for enough uh right from my internship uh through my uh full-time roles I've been working in this uh industry in Ai and especially uh from past two years or three years I would say that I got very much passionate about healthcare industry so that comes like a later part when you want to have a career trajectory in AI itself like you want to do generalization versus specialization so um this is kind of like a strategic challenge I would say say that um at the onset of your career that you would want to decide that you want to specialize in a niche area like computer vision or natural language processing or you want to have like a broader set of skills and still be able to wear many hats in a day so since then um yeah it has been like a good career growth for me and um uh and like I said like healthcare uh is something that I'm very passionate about Okay cool so that that leads to me uh in fact two follow of questions especially from a uh aspiring audience uh you know one someone who is interning somewhere in the AI space right the question",
      "is always like the keep figuring should I go for Masters so I mean someone who hasn't done Masters right even I keep getting this question often um I'm I'm thinking I should I go for Masters I should I gain two three more years of more experience I don't know I'm I'm I'm sure your Juniors have might have reached out to you already right regarding this question so that's one question that usually comes out and then would love to hear your thoughts on the same um so I would like kind of um again like it's coming from my personal experience um so there are two two ways to see it firstly if you want to pursue master's degree with an intent of going for a PhD then definitely that's the only way to to navigate your PhD program is going through a master's program and then you continue from there after but if you're very set upon that you want to stick to Industry setting and you are like confident enough with your skill set and it's going to ever be ever growing because once you get into industry and you are like um learning hands-on experience the tools and platforms that are required for a data scientist or or a machine learning engineer then I guess um I would personally skip that step of like again taking a step step back or like um and going towards a two-year committed program uh because then the stakes are high as well because then the job search could be um difficult depending on like economic climate or like which country you land up in so there are two ways to see to it uh I was pretty much set on like coming to the states uh this",
      "was my personal choice and uh I uh also had a very vague idea towards stepping into like research space as well uh fortunately my full-time roles have always been at the intersection of research and development so I did not pursue a a a part to PhD but in my industry setting I'm kind of doing the same so uh I would recommend uh people who are very research Savvy would want to go to and take this part there is nothing uh there is no harm in that as well okay cool cool and the second question um is usually oh should I should I focus more from U say I want to focus on the horizontal you know skills saying I just want to be in more computer vision or in NLP or should I look it from a perspective of like okay healthcare industry or that industry or agriculture industry right so because because that's where is again the new question that keep coming in their mind so what is your thoughts on that and probably you can answer why you chose healthcare industry um what was the motivation behind that and maybe that might help the audience yeah this be computer vision be it natural language processing it comes down to those numbers how do you process your algorithms on top of it and try to make uh key insightful um and actionable um you know results out of it so that's the key um area that I would want people to focus more rather than worrying about like should I on the onset of my career I should focus just on computer vision and natural language processing uh I believe that um you should be able to adapt as well because even if you're",
      "working on a um any project for a tech giant there are different data sources that they also get in it could be multimodal so be prepared um to you know thrown a gauntlet of um a challenge to you anytime and uh that's the key to it again is like to learn and tackle that data so that's pretty much important and that's what like I guess um is demanded even throughout any recruiting process when you are like uh going through an interview process for any machine learning role it if you given such and such data what would you do with it so that's the um key aspect of uh um you know understanding that where you would want to set your foot um and again it's it's good to have some sort of like um experience or to be called as a veteran in the industry be it in healthcare or be it in automation um so that's great to have but uh you need to understand that the basics is always the data so try to you know first focus on that cool cool and one uh uh you know interesting uh aspect that I've seen uh also and maybe you know you can again clear the air around that thought I'm a machine learning engineer uh working on you know fixing the data is not my job it's a data Engineers job what are your thoughts on this like uh we do hear about um like data scientist just specifically just working on um data side of the problem and then machine learning Engineers working on model architecture and like modeling of uh these algorithms but I guess um these roles are sometimes just used interchangeably so don't get confused um meaning that if",
      "you are a data scientist then then you might end up also doing some modeling and architecture stuff and also a machine learning engineer definitely has to work around the data too right right and in in your case if I ask you right looking at your fast project how much percentage has been where you have been spending your time in the algorithms versus the data oh to be very to be uh very honest like especially when it comes to healthare Industry don't expect very clean label data sets like dogs and cats available on the internet to just download and then you have like gazila of data to train on but Health Data itself is very intricate it's it's it's um also not everybody body can understand because again we are not coming from a background in medicine or pharmaceutical uh so there are a lot of understanding that goes into it and then there have been times where I have also cleaned up annotated uh Shi the data and then used it for my machine learning model so uh there could be cases that you know you need to do uh right from annotating your data labeling the data clean in it up um um and then um you you have to use your um machine learning models on top of it top of that so I would say that yes there have been cases where I've been spending 50% of my time just on data itself and then 50% would go on modeling okay okay and in general as a team um for any AI project development uh let's say let's just just even build one or two models or iterations of the you know Healthcare AI space how much has been the percentage effort",
      "for data versus again the algorithms um collectively as a team right from data collection to to the to the deployment absolutely so I guess it has uh what I observed is like it's an iterative process again meaning that even when you start a data Pipeline and then you keep continuing your efforts on like modeling and then you have like uh a version one one of a model based on the data that you got and then there are some more uh nuances to data itself that you find out later and then you go back and then you know fix that and then it goes on and and again so what I've observed even as a team it's like um U strategically these tasks are assigned uh so that one is or like couple of team members is of course uh pretty much uh have the hang on uh in terms of like how to deal with the data itself even when there is something that is um you know there is some sort of like a data error then how would you want to tackle that or like debug that kind of a problem and then there's some few experts in the team who would mostly take care of the architecture but also have knowledge of what's going on in the data pipeline too um so I would say that yes that's how the team is usually divided um even though uh most of the team MERS know what's going on but there are few experts that are assigned to one of the task okay okay and one another question um you know how do I so when when looking out at any opportunity uh right uh in in the industry right um how do I",
      "choose a team where one team where there is more u i mean domain experts but probably I will be the only one or someone else will be the only one who will be machine learning engineer but then there is an idealistic view of oh there will be very big team of ml scientists you know data engineers and so and less of domain experts probably you know in a in a setup right depends on the organization structure so these are kind of the choices that come across uh in front of the audience Right ispiring audience so how do you think what is your suggestion on this how how do someone would go about you know exploring that opportunity which one they should choose if they have both the options I guess firstly it depends on the site of the project as well and uh how much wealth of data that you will be dealing with and of course having domain experts is always always like a bonus Point uh especially those who have like years of experience plus a PhD in the same um because it comes with its own um um you know um advantages of like learning or like diving deep into what's going on um in the data itself because there have been cases where um I've seen that physics have been involved in machine learning models or like um you know some sort of like biomedical engineering is involved so I'm not a domain expert in that but um I might be good at understanding what kind of model architecture to use but uh to understand those little nuances what what goes on around like in the first principles of um computer science or physics or like even understanding biomedical engineering is",
      "is always an add-on to the team uh again it it could be just uh focusing in terms of understanding um novel approaches how to take take the problem through um because um even when you working on an R&D problem in Industry setting you are time bound um so you would want to make sure that um um the resources are utilized optimally throughout yeah yeah yeah so business is always wanted yesterday so it has to be done timely though ml development process is an experimental process and not a very certain outcome oriented process yeah yeah the good thing about it is that you know even from your failures you learn a lot so that's that's a good part about like um uh you know being in this space that um uh to some extent yes it's blackbox people to talk about transparency and we also focus on transparency especially in healthcare AI because the stakes are high so you need to make sure that uh you know if you crack open a machine learning model what do you see and how do you explain to a medical profession professional Okay cool so another followup question though I didn't have this question uh in mind earlier so but very interesting one you working in healthcare you know the end impact is safety health of someone right uh do you have you know while while developing these models do you also feel some sort of emotions um which capture around you especially like okay you know what if this model doesn't work in that the expected with being a technical professional I'm not a medical professional yet is firstly um understanding how do you like evaluate your models so in in machine learning we call as Matrix do",
      "you want to uh uh focus on accuracy or do you want to focus on Precision so what are these Stakes so you or you want to just focus on recall so this all kind of um you know U is a game changer in terms of how do you finally uh present your model uh higher up and then you know you are confident enough to go about and go for like B data testing or like even go for another round of testing on these models if you look back on your U this journey right um what are the top three challenges that you that has occurred in your you know career um whether in terms of the technical aspect if I mean uh from a technical standpoint second navigate through working along with the other team members so both so more of a you know technical aspect as well as a professional you know management aspect so yeah if you if you can put some light on those yeah I guess uh when I was just starting with my like job search um um when I was still a graduate student so I guess the first thing that I faced was lacking industry experience so I was still a master student then and uh often times recruiters do want have some hands-on experience but um you know what really helped me was like getting the right internship or participating in hackathons or having your own personal projects building your own profile so that you know you are um kind of setting stone for yourself and uh would want to uh be ready for any interviews for full-time goals uh so that you're able to at least prove yourself that uh that you're able to bring those",
      "academic theories and you know work on real world problems again um the pivotal role of you know understanding the specialization that I want in my career um computer science is great then of course um started with machine learning that's great but then what do I really want to do out of it so understanding that I really wanted to pursue career in healthcare industry um so it might take years for someone or it might be you know pretty quick for someone so it's it's really like uh you know case toase and um sooner you know what's your calling so it's better cool and uh another question that I have I have here is um if you can talk about uh some specific scenario of a project like what was a data like you know on a high level and what is a problem statement like you know what has to be for example predicted and then any specific challenge that you fa there and how did you solve it so maybe more in summary if you can talk about genetic in a genetic way um I guess in my last company uh we were working on this uh robotic surgical tool platform which is very fancy uh it was my first time approach on working on hyperspectral Imaging data so this is unlike those RGB fancy images or not even like x-ray images or like CD images I've worked on like CT images before but but this was all together something different so it's like uh different wavelengths captured and it's like a cube as you can imagine of data so it's like with multiple channels and then you need to and you can't even see in in a in a regular viewer like a photo",
      "editor what's going on because these are different wavelengths um and the task was to identify some critical structures during the surgery and alert the surgeon on time because it's navigating through some surgical tools so you need to understand what's going on in real time and um also inform the surgeon if there's going to be any catastrophic damage that that's going to happen um so this was my first time approach of uh you know working on a data like that and uh it was very interesting to work on it of course came with its own challenges very unique um and um still it was in its R&D phase um cannot discuss much more about it so um um but I guess um like I said um coming back to a question that you posted earlier that you need some domain experts so thankfully in my team there were some domain experts to help me understand this kind of data um and um understanding physics spectrometry biomedical engineering behind it that was alog together you know going back to a classroom and then you know going through uh I literally had to go through some of the books and like research papers on it um but um I guess the key challenges to that particular problem was again like um The Domain expertise that was required and then um even creating the data pipeline from scratch and then uh of course the modeling um was there okay so in duration just to quantify it was it how does it look like like months and how many months or weeks um I guess I I guess uh the most most time was taken for like the data itself um again like it was it was done in like",
      "um I would say in chunks so there isn't like a specific number that I can you know on top of my head that I can say that okay these number of weeks we would spend on it but um I would I would roughly estimate that 50% again was gone through you know just data itself cleaning up labeling annotating and then um then I would say roughly 30% on modeling and I guess 20% was always Improvement um or like post talk analysis as you would say that to understand how the result are what can we improve um so that's that's how we got about it okay cool okay so uh so you I go went through your one of your LinkedIn post about you know you sharing thoughts around um Powerhouse women and Michelle Obama right on on stage what was it and how was experience like so yeah oh yeah that that was pretty interesting in this year um I was reached out to like share my thoughts um on like women and uh Sam um I've been kind of like part of like various organizations even during my master's program I conduct conducted one machine learning workshop and then after that I was a speaker uh in one of the events in Baya uh and also um B Ambassador for women women of AI um so it has been uh um uh like it's it's kind of like giving back to the society right and so so what was the like high level discussion points or the outcome points that were being discussed there like just a high level if you can put some light oh um it was just about like just you know thoughts on um you know women in engineering or",
      "women in stem and then uh there were other such women who who are like uh in this field in stem field especially so um I guess collectively just uh you know having our thoughts together at one page okay okay and what's your perspective of like how uh you know empowering women in stem and a as a field and it was only stem earlier now it's stem and a so so yeah I mean any any thoughts on this and what I mean are we doing enough or we you know to ensure more diversity more inclusion so what's your thought I mean or experience has been till now from that angle um yeah at least with my observation personal observation that I see more women coming up and like pursuing a career in stem which is uh pretty motivating to see uh especially how difficult it would be just to start a career in engineering um when I I imagine um you know couple of years back you know and versus what's going on now I guess it's it's more about like a of technology and access to it which has Crone um and um in this case I would say like social media has played a good good role in terms of like promoting um these certificate programs or like expedited programs online um you know promoting uh to learn and grow their businesses so I guess more or less people are using AI everywhere now um be it to you know it's a like a small business in India or like even you know just doing some social media campaign so I guess everybody's is very familiar about now chat GPT or canva or like you know other Technologies which revolve around AI so it's",
      "pretty uh motivating to see that more and more people are coming into it and like it's so healthy use of technology I would say okay good um so lastly uh one of the important things our audience would be curious in know putting in a summary someone I'm a for example let's say I'm a aspiring Healthcare data professional right uh very tons of information out there the challenge is not anymore about that information is less information is too much um something which you would advise that I should pick that first uh you know to navigate into this area of healthcare AI um I guess uh firstly understanding um the job market firstly that if you want to set foot U into this industry are you prepared for it in terms of like um um the data itself so there's so many open source um data that's available which is related to healthcare um why not try a personal project on that which kind of like was my uh personal exercise when I was trying to get into um Healthcare AI um uh definitely keeping uh eyes wide open and trying to see in real world setting how can you um you know tailor any machine learning model to needs of of patient care again coming from a personal experience this is what I observed that um if you are personally or like on a firsthand basis if you're observing something that that can be U you know a live product um that that'll be very interesting to you know think about it and um U thirdly it would be like be ready to be a student because like again comes with its um you know challenges in terms of like um The Domain expertise that's required",
      "um it's great that if you have some background in it but uh don't be discouraged if you don't because um I guess um um it really comes with the years of experience once you into it uh but um to get started I would I would definitely encourage someone to go out there and like um have a good idea about like Noel machine learning models that are out there uh Google Health releases a lot of information on it um big Tech Giants are also working on it so there are a lot of Publications out there which um gives a fair idea about how things are working currently U and then getting some hands-on experience with some personal projects is uh advisable because if you are able to understand some sort of like electronic medical records which is you know great because uh that's like uh used a lot or like even chest x-rays or like CD scans so some of these data sets are uh open source and available uh so get your hands dirty on it and uh I guess um LinkedIn again is a very collaborative community so um don't shy away from reaching out to professionals there um people do help and um it's very collaborative again to get advice from people who have been in this industry um so yeah that will be something I would advise to people who want to get into Healthcare AI thank you prianka for this for your insights so one of the insights that I for sure got is the domain expertise especially when it comes to healthcare AI is is really really important and it's uh see as being a ml engineer you have to be a you have to actually dig through all of",
      "that it's not just the code it's not just the U just the data processing but it's it's more than that right uh so that's one Insight that I've received second U someone who is looking to get started uh you know they have to do some hackathons internship to translate uh whatever the theoretical knowledge they have received and so to to enable the organization to gain confidence in you so that they can hire you and all any anything else that that you want to like third point you want to highlight here like you know as a more important Point here as part of the summary uh out of all the insights that youve shared I believe it's it's more like uh also um the the learning curve should not stop I would say like be ready to you know spend your weekends on like cor SRA B drings questions uh because um you need to understand that what's going on in the uh industry setting it can be of course Very daunting and to keep up with it the only ways to like practice and learn um so yeah like just keep up with the speed of this Tech Evolution so the continuous learning is something is is is something is one of another takeways Okay cool so uh um what what is the best way to reach out to you in case they have questions uh the audience has the questions so one for the professional uh you know collaboration second uh more from a learning perspective is the same same same channel or different channels like what could be the channel to reach out to you oh quickest would be like reaching out um on my LinkedIn which is with my name Priyanka makani",
      "and um um yes if there is some interesting opportunity then I would definitely respond to that and of course we'll be very very happy to Mentor or like even give some vocational guidance of people pursuing in this industry be it Healthcare or not Healthcare um just you know plainly a computer science uh graduate would want to ask certain questions uh around that because my background is uh in computer science basically so um we'll be very happy to have a chat um um and yes yeah that's all that's so grateful of you thank you thank you thanks thank you okay cool"
    ],
    "transcript_word_count": 5505,
    "transcript_chunk_count": 19
  },
  {
    "video_id": "tTXWd9O5vR0",
    "title": "Effortless Annotation with Auto Labeling APIs: LabelGPT & AWS Recognition | Labellerr",
    "description": "Transform your annotation process with Auto Labeling APIs on Labellerr. Speed up data labeling with LabelGPT and AWS Recognition, enabling your team to focus on reviews, not manual tasks. Save time and boost accuracy!\n\nChapters\n0:00 Introduction to Labellerr Platform\n0:01 Overview of Auto Annotation Feature\n0:09 Use Case: Receipt Label Identification\n0:30 How Labellerr Simplifies Annotation\n1:08 Walkthrough of Auto Labeling Setup\n1:42 Using LabelGPT API for Annotation\n2:11 Demonstrating Auto Labeling with LabelGPT\n2:45 Using AWS Recognition for Annotation\n3:19 Demonstrating Auto Labeling with AWS Recognition\n3:39 Benefits of Auto Labeling APIs\n3:59 Conclusion and Happy Annotating\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=tTXWd9O5vR0",
    "embed_url": "https://www.youtube.com/embed/tTXWd9O5vR0",
    "duration": 244,
    "view_count": 539,
    "upload_date": "20240215",
    "uploader": "Labellerr",
    "tags": [
      "data labeling",
      "label applicator",
      "bottle sticker",
      "product sticker",
      "label printing",
      "sticker applicator",
      "labeling solutions",
      "sticker applicator machine",
      "label printing machines",
      "labeling process",
      "packaging automation",
      "packaging labels",
      "data labeling tools",
      "sticker manufacturing",
      "bottle sticker machine",
      "packaging stickers",
      "product sticker design",
      "stickers on bottles",
      "data annotation",
      "label applicator machine",
      "product labeling machine",
      "sticker printers"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "this is the labellerr platform and I would be talking about a very great annotation feature which helps you uh label millions of images in just a few days let's take a possible use case where you know you want uh your AI team is developing a model uh which can identify receipt labels now in order to train this model you need uh a dedicated uh annotation team which would take a lot of time to label uh data very accurately and what our labellerr platform does is it comes to your rescue and it uh actually uh helps you with model assisted labeling where uh labels uh are identified very accurately and you know uh what your annotation internal annotation team would do now is it would just you know uh act as an internal review and uh actually uh review the um data which which has been annotated by automatically by the model models of labela platform so I'll give you a quick walk through of how this feature works so uh you go to settings and you go to annotation s you have these annotations here click on miscellaneous property check the enable auto Label Box choose the option label GPT API or AWS recognition I will uh check label GPD I'll go back and I'll check this this is also laible GPD miscellaneous property this is a polygon so now I'll save changes and in current project guidelines annotation labels are saved I'll go to my labeling screen and I'll click on use Auto so I'll clear my answers first you know to tell you how this feature works so I'll click on use Auto label see there are two papers and there are two re Z bounding boxes okay so this has been identified very accurately I'll go back and I'll show you how the other API works that is AWS recognition so I'll go to annotations click on miscellaneous property on click on this AWS recognition back laneous property AWS recognition back click on Save changes in current project guidelines you see annotation labels are saved go to label screen first clear the answers so I'll clear the answers to show you how it's working click on use Auto label see there are two papers and two recept boundary boxes so this has been identified very correctly so you can see that uh you know these two apis have you know actually uh made The annotation process so simple and now your annotation team can you know uh actually uh just uh review all these annotations and act as an internal reviewer team they don't need to spend a lot of their quality time manually annotating a lot of data I hope you enjoyed this explanatory we video thank you thank you so much happy annotating with la love",
    "transcript_chunks": [
      "this is the labellerr platform and I would be talking about a very great annotation feature which helps you uh label millions of images in just a few days let's take a possible use case where you know you want uh your AI team is developing a model uh which can identify receipt labels now in order to train this model you need uh a dedicated uh annotation team which would take a lot of time to label uh data very accurately and what our labellerr platform does is it comes to your rescue and it uh actually uh helps you with model assisted labeling where uh labels uh are identified very accurately and you know uh what your annotation internal annotation team would do now is it would just you know uh act as an internal review and uh actually uh review the um data which which has been annotated by automatically by the model models of labela platform so I'll give you a quick walk through of how this feature works so uh you go to settings and you go to annotation s you have these annotations here click on miscellaneous property check the enable auto Label Box choose the option label GPT API or AWS recognition I will uh check label GPD I'll go back and I'll check this this is also laible GPD miscellaneous property this is a polygon so now I'll save changes and in current project guidelines annotation labels are saved I'll go to my labeling screen and I'll click on use Auto so I'll clear my answers first you know to tell you how this feature works so I'll click on use Auto label see there are two papers and there are two re Z bounding boxes okay so this has been identified",
      "very accurately I'll go back and I'll show you how the other API works that is AWS recognition so I'll go to annotations click on miscellaneous property on click on this AWS recognition back laneous property AWS recognition back click on Save changes in current project guidelines you see annotation labels are saved go to label screen first clear the answers so I'll clear the answers to show you how it's working click on use Auto label see there are two papers and two recept boundary boxes so this has been identified very correctly so you can see that uh you know these two apis have you know actually uh made The annotation process so simple and now your annotation team can you know uh actually uh just uh review all these annotations and act as an internal reviewer team they don't need to spend a lot of their quality time manually annotating a lot of data I hope you enjoyed this explanatory we video thank you thank you so much happy annotating with la love"
    ],
    "transcript_word_count": 476,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "ua7El0LOcmA",
    "title": "Effortless Text Annotation with Interactive Review Features | Labellerr",
    "description": "Simplify Text Annotation with Labellerr's interactive features. Easily manage and review annotations across large texts, track categories, and boost accuracy with user-friendly tools. Transform your text annotation process today!\n\nChapters\n0:00 Introduction to Labellerr Platform\n0:04 Overview of Annotated Text\n0:10 Annotating Order Date\n0:20 Annotating Order Number\n0:27 Annotating Service Name\n0:42 Interactive Review Features\n1:04 Reviewing Service Name Annotations\n1:19 Reviewing Product Name Annotations\n1:33 Easy Annotation Review Process\n1:52 Benefits of Labellerr’s Features\n2:01 Conclusion and Happy Annotating\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=ua7El0LOcmA",
    "embed_url": "https://www.youtube.com/embed/ua7El0LOcmA",
    "duration": 130,
    "view_count": 101,
    "upload_date": "20240109",
    "uploader": "Labellerr",
    "tags": [
      "text mining",
      "medical annotation",
      "image labelling",
      "annotation software",
      "dataset creation",
      "data preprocessing",
      "quality assurance",
      "image annotation",
      "medical dataset",
      "image segmentation",
      "text summarization",
      "data annotation",
      "annotation tutorial",
      "image annotation work",
      "annotation strategies",
      "annotative text",
      "text highlight",
      "text categorization",
      "annotation workflow",
      "nlp techniques",
      "annotation examples",
      "document tagging",
      "label studio tutorial",
      "text analysis",
      "annotation methods"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "so this is the labellerr platform and uh you can see a very large piece of text here which has been annotated uh you know with different colors and uh if I click on January 14 2023 you can see this is uh this has been annotated under the order date and uh category and the dark green is for the order number category you can see it popping uh in between the screen uh and if I click on need help you can see the that is has been annotated under service name category so uh it's very easy to identify the different annotations uh under different categories by clicking on them also uh there's a very cool feature that labellerr platform provides and it is the interaction uh you know between different annotations uh and uh the left panel so if you click on the review uh screen and you go here so if you click on January 14 2023 you'll see that this is the the order date and this is the first annotation in your document and uh if I click on this light green need help you'll see that need help has been annotated under service name category and service name has two annotations need help is one of them the other one is pick and uh then you if you click on Velveta original melting cheese dip you'll see that it has been annotated under the product name category and product name category has 32 different annotations so there are 32 different product names inside this piece of text now it's very easy to review your annotations uh that have been done uh and in a very large uh you know piece of text and it's very easy to uh review every one of them you can you know know the count and you can know everything about you know the annotations present inside the text so it's very easy to review because of labellerr School features so I hope you enjoyed and uh you know all the features uh as explained by me for labellerr and uh so laa has made text annotation a breeze and uh so happy annotating with your you know huge huge pieces of text thank you",
    "transcript_chunks": [
      "so this is the labellerr platform and uh you can see a very large piece of text here which has been annotated uh you know with different colors and uh if I click on January 14 2023 you can see this is uh this has been annotated under the order date and uh category and the dark green is for the order number category you can see it popping uh in between the screen uh and if I click on need help you can see the that is has been annotated under service name category so uh it's very easy to identify the different annotations uh under different categories by clicking on them also uh there's a very cool feature that labellerr platform provides and it is the interaction uh you know between different annotations uh and uh the left panel so if you click on the review uh screen and you go here so if you click on January 14 2023 you'll see that this is the the order date and this is the first annotation in your document and uh if I click on this light green need help you'll see that need help has been annotated under service name category and service name has two annotations need help is one of them the other one is pick and uh then you if you click on Velveta original melting cheese dip you'll see that it has been annotated under the product name category and product name category has 32 different annotations so there are 32 different product names inside this piece of text now it's very easy to review your annotations uh that have been done uh and in a very large uh you know piece of text and it's very easy to uh review every one of",
      "them you can you know know the count and you can know everything about you know the annotations present inside the text so it's very easy to review because of labellerr School features so I hope you enjoyed and uh you know all the features uh as explained by me for labellerr and uh so laa has made text annotation a breeze and uh so happy annotating with your you know huge huge pieces of text thank you"
    ],
    "transcript_word_count": 378,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "43FF6t3XwM4",
    "title": "Export Annotated Data Effortlessly with Multiple Formats | Labellerr",
    "description": "\"Simplify your data export process with Annotated Data Export on Labellerr. Seamlessly export annotations in formats like JSON, CSV, and YOLO V5, reducing data preparation costs and ensuring interoperability across platforms. \n\nTry it today!\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\"",
    "video_url": "https://www.youtube.com/watch?v=43FF6t3XwM4",
    "embed_url": "https://www.youtube.com/embed/43FF6t3XwM4",
    "duration": 239,
    "view_count": 91,
    "upload_date": "20240108",
    "uploader": "Labellerr",
    "tags": [
      "data export",
      "export data",
      "export ga4 data",
      "csv export",
      "shopify analytics",
      "data visualization",
      "analytics reporting",
      "data analysis",
      "shopify growth",
      "data reporting",
      "analytics optimization",
      "report automation",
      "shopify metrics",
      "csv generator",
      "shopify reports",
      "reporting software",
      "shopify insights",
      "shopify analysis",
      "shopify export",
      "data analytics",
      "csv data tools",
      "JSON export",
      "annotation software",
      "export ML data",
      "export labeled data",
      "YOLO V5 export"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "this is the labellerr platform and I would be talking about a very great feature which actually helps uh the user the clients export the files which have been uh annotated and you know which have passed through the labellerr pipeline so I can select any of the statuses and uh the most relevant statuses uh that can be selected are ready for review and ready for client review and uh after you know selecting these two statuses uh I'll apply the filters now uh you can see the export all files button here so I'll click on this now uh either I can you know export all files I can click on this part and Export all files or I can click on this and you know select all files or selected files or history so uh right now I'm am you know choosing uh some of the files and I click on uh selected files so this will help me export the selected files and I'll name my export so I'll name it as um you know 4th Jan export uh for Jan 2024 export uh I'll I'll write the description I'll select the questions so I need them all and then uh my destination is local and then there are three formats CSV Json and YOLO uh V5 all these formats uh they are quite essential because they help the client download uh the exported files in different formats and the benefit uh now Json is uh you know mostly used because it uh helps uh it provides interoperability the exported files they can be used uh across different platforms and also the data preparation cost reduces and uh so I'll choose Json and uh I'll create my export you can see that the exports are in progress and you can see generating export there's a snack bar which shows generating export and I can click on view details view details would direct me to the exports page and uh I can see that uh you know uh the status here so currently it's in progress so it has been created and I can see that uh okay uh you know this export has been created and I have the following actions download or delete I can choose to download this export and this exported data can be used by the user for uh you know different kinds of models and uh it is structured and it's already formatted it's uh in Json so the data preparation cost reduces so uh you know these are the advantages of including uh a variety of formats uh for exporting the annotated data I hope uh this feature uh I hope you enjoyed this explanatory video of this feature so uh thank you and thank you for being a patient listener thank you so much happy annotating",
    "transcript_chunks": [
      "this is the labellerr platform and I would be talking about a very great feature which actually helps uh the user the clients export the files which have been uh annotated and you know which have passed through the labellerr pipeline so I can select any of the statuses and uh the most relevant statuses uh that can be selected are ready for review and ready for client review and uh after you know selecting these two statuses uh I'll apply the filters now uh you can see the export all files button here so I'll click on this now uh either I can you know export all files I can click on this part and Export all files or I can click on this and you know select all files or selected files or history so uh right now I'm am you know choosing uh some of the files and I click on uh selected files so this will help me export the selected files and I'll name my export so I'll name it as um you know 4th Jan export uh for Jan 2024 export uh I'll I'll write the description I'll select the questions so I need them all and then uh my destination is local and then there are three formats CSV Json and YOLO uh V5 all these formats uh they are quite essential because they help the client download uh the exported files in different formats and the benefit uh now Json is uh you know mostly used because it uh helps uh it provides interoperability the exported files they can be used uh across different platforms and also the data preparation cost reduces and uh so I'll choose Json and uh I'll create my export you can see that the exports are in",
      "progress and you can see generating export there's a snack bar which shows generating export and I can click on view details view details would direct me to the exports page and uh I can see that uh you know uh the status here so currently it's in progress so it has been created and I can see that uh okay uh you know this export has been created and I have the following actions download or delete I can choose to download this export and this exported data can be used by the user for uh you know different kinds of models and uh it is structured and it's already formatted it's uh in Json so the data preparation cost reduces so uh you know these are the advantages of including uh a variety of formats uh for exporting the annotated data I hope uh this feature uh I hope you enjoyed this explanatory video of this feature so uh thank you and thank you for being a patient listener thank you so much happy annotating"
    ],
    "transcript_word_count": 477,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "mL6pgBX0gaI",
    "title": "Streamline Annotation Review with Grid and Stat Views | Labellerr",
    "description": "Experience seamless Annotation Review with Labellerr's grid, list, and stat views. Quickly scan files, ensure Pixel Perfect accuracy, compare annotations, and track progress for consistent and efficient labeling. \n\nExplore the features today!\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=mL6pgBX0gaI",
    "embed_url": "https://www.youtube.com/embed/mL6pgBX0gaI",
    "duration": 189,
    "view_count": 83,
    "upload_date": "20240108",
    "uploader": "Labellerr",
    "tags": [
      "how to write an annotation",
      "annotation methods",
      "annotation software",
      "tech career insights",
      "metadata tagging",
      "metadata management",
      "annotation workflow",
      "metadata analysis",
      "data labeling",
      "software testing",
      "labeling software",
      "annotation strategies",
      "annotation platforms",
      "software analysis",
      "data management",
      "software insights",
      "data annotation tech review",
      "data annotation review",
      "annotations",
      "data annotation",
      "data annotation tech",
      "dataannotation.tech review",
      "book annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "so this is the labellerr platform and I would be talking about a very great feature which you know gives you a quick visual scan of all the annotated files and this feature is very essential because you know I need to go through uh the files uh and uh you know see uh in in a very detailed manner see all the annotations compare the consistency of annotations across the files and you know zoom in zoom out on the annotations which is especially essential for ensuring Pixel Perfect accuracy so let's see how this feature operates so I'll select uh the feature uh the files which are ready for client review so you know I this is the this is the status I select and I'll apply the filter and uh I'll have uh my list of files which are you know ready for client review so I have all the files and then I I go to the grid View and uh the grid view this these four uh the square icon and then uh I'll see all the files you know uh visually and which will help me you know see all the annotations so uh I can see all the annotations and uh you know through this I can have a customizable viewing now every you know annotation it has uh you know as you can see every annotation and every use ke case requires different level of viewing and the zoom in feature it allows me this uh this you know uh this thing that I can you know see the annotations in a lot of detail um and I can also see the uh that these annotations are quite Pixel Perfect so I can ensure I can see that you know uh that all these annotations have Pixel Perfect accuracy things like that and then I have three views as you can see the list view the list view is very essential because uh I can see all the metadata related to the files and especially useful for the administrative tasks uh uh also and then I have the stat view which uh you know helps me know The annotation density the workload progress things like that very useful and as you can see uh the stat view you can see the frequency and everything then uh the grid view as I said it provides a quick visual scan of all the files and uh you can have a detail inspection of all the annotations you can uh you know see the consistency of you know the annotations across images you can see how uh I mean a particular annotation it has been done across images and yeah I mean this makes the review process uh quite smooth and and I hope you enjoyed uh this quick explanatory video about this feature and uh thank you for being a patient listener happy annotating",
    "transcript_chunks": [
      "so this is the labellerr platform and I would be talking about a very great feature which you know gives you a quick visual scan of all the annotated files and this feature is very essential because you know I need to go through uh the files uh and uh you know see uh in in a very detailed manner see all the annotations compare the consistency of annotations across the files and you know zoom in zoom out on the annotations which is especially essential for ensuring Pixel Perfect accuracy so let's see how this feature operates so I'll select uh the feature uh the files which are ready for client review so you know I this is the this is the status I select and I'll apply the filter and uh I'll have uh my list of files which are you know ready for client review so I have all the files and then I I go to the grid View and uh the grid view this these four uh the square icon and then uh I'll see all the files you know uh visually and which will help me you know see all the annotations so uh I can see all the annotations and uh you know through this I can have a customizable viewing now every you know annotation it has uh you know as you can see every annotation and every use ke case requires different level of viewing and the zoom in feature it allows me this uh this you know uh this thing that I can you know see the annotations in a lot of detail um and I can also see the uh that these annotations are quite Pixel Perfect so I can ensure I can see that you know uh that",
      "all these annotations have Pixel Perfect accuracy things like that and then I have three views as you can see the list view the list view is very essential because uh I can see all the metadata related to the files and especially useful for the administrative tasks uh uh also and then I have the stat view which uh you know helps me know The annotation density the workload progress things like that very useful and as you can see uh the stat view you can see the frequency and everything then uh the grid view as I said it provides a quick visual scan of all the files and uh you can have a detail inspection of all the annotations you can uh you know see the consistency of you know the annotations across images you can see how uh I mean a particular annotation it has been done across images and yeah I mean this makes the review process uh quite smooth and and I hope you enjoyed uh this quick explanatory video about this feature and uh thank you for being a patient listener happy annotating"
    ],
    "transcript_word_count": 490,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "6NAm5fBmu6g",
    "title": "Effortless Selective File Annotation for Streamlined Reviews | Labellerr",
    "description": "Simplify your Selective File Annotation process with Labellerr. Focus on specific files, identify trends, and enhance quality assurance through targeted reviews. Perfect for improving workflow and ensuring accurate annotations. \n\nTry it now!\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=6NAm5fBmu6g",
    "embed_url": "https://www.youtube.com/embed/6NAm5fBmu6g",
    "duration": 221,
    "view_count": 96,
    "upload_date": "20240108",
    "uploader": "Labellerr",
    "tags": [
      "pdf annotation",
      "data annotation",
      "autocad annotation",
      "autocad annotation scale",
      "dropbox notes",
      "autocad scale",
      "vlsi design",
      "annotation software",
      "cad dimensions",
      "pdf markup",
      "annotation methods",
      "json tools",
      "cad software",
      "autocad hacks",
      "pdf commenting",
      "image labeling",
      "vlsi tools",
      "json editing",
      "design annotations",
      "pdf tools",
      "data visualization",
      "image annotation",
      "annotation workflow",
      "json visualization",
      "autocad features",
      "data markup",
      "drawing dimensions",
      "data mapping"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "platform and I would be talking about a very nice feature that helps reviewers and client reviewers selectively you know uh assess certain files from the bulk and then uh you know uh this uh they can selectively review these files from the bulk and uh they can uh choose even choose to you know specifically review files from particular set of annotators or you know files within a particular time range to identify certain specific issues or certain trends of issues that have been happening within a specific period of time and this uh helps in focused attention of the reviews and client reviews on a specific set of files this also helps in a parallel review process because the reviewers can you know review uh files parallely and uh while uh certain reviewers are involved on a specific set of files another set of reviewers can involve themselves on another set of files also uh this helps them identify uh certain issues uh within a certain time period for example there may be you know certain annotators working on within a specific time period and this may have caused certain issues uh within particular set of annotations so uh that can be judged through this feature also um you know uh so it also helps in guiding the feedback uh mechanism and the training process uh and it creates a very streamlined process for ensuring quality annotations so I'll be giving you a quick walk through of how this feature Works uh the status uh you know so I'll select the you know any of these statuses from here so I'll select the status ready for client review and uh then I have kept my date range as first to 4th and then I'll apply my filters and I'll get the files you know uh which are ready for client review and I'll randomly select uh some of the files from here and I'll go to these uh this hamburger icon which is at the top right corner and then I'll start my client review and uh I'll click on take me to client review and this immediately takes me to a separate window uh where I can review all my annotations so I'll accept this one now this is the second one I selected and I'll accept this one also and this is the third one the third file I selected and I'll accept this one also so I'll get those those three files uh you know in a separate uh window for reviewing and uh like I said I can review in this uh window the three files uh the three sample files that I've selected from the batch of files so it's a very streamline process which allows me to you know selectively take some files out of the total batch of files and then uh push them uh into my focus area have a look at them and you know uh give them ago so I hope uh this uh this has been an enjoyable ex uh explanation experience for you and thank you so much for being a patient listener thank you",
    "transcript_chunks": [
      "platform and I would be talking about a very nice feature that helps reviewers and client reviewers selectively you know uh assess certain files from the bulk and then uh you know uh this uh they can selectively review these files from the bulk and uh they can uh choose even choose to you know specifically review files from particular set of annotators or you know files within a particular time range to identify certain specific issues or certain trends of issues that have been happening within a specific period of time and this uh helps in focused attention of the reviews and client reviews on a specific set of files this also helps in a parallel review process because the reviewers can you know review uh files parallely and uh while uh certain reviewers are involved on a specific set of files another set of reviewers can involve themselves on another set of files also uh this helps them identify uh certain issues uh within a certain time period for example there may be you know certain annotators working on within a specific time period and this may have caused certain issues uh within particular set of annotations so uh that can be judged through this feature also um you know uh so it also helps in guiding the feedback uh mechanism and the training process uh and it creates a very streamlined process for ensuring quality annotations so I'll be giving you a quick walk through of how this feature Works uh the status uh you know so I'll select the you know any of these statuses from here so I'll select the status ready for client review and uh then I have kept my date range as first to 4th and then I'll apply my filters and",
      "I'll get the files you know uh which are ready for client review and I'll randomly select uh some of the files from here and I'll go to these uh this hamburger icon which is at the top right corner and then I'll start my client review and uh I'll click on take me to client review and this immediately takes me to a separate window uh where I can review all my annotations so I'll accept this one now this is the second one I selected and I'll accept this one also and this is the third one the third file I selected and I'll accept this one also so I'll get those those three files uh you know in a separate uh window for reviewing and uh like I said I can review in this uh window the three files uh the three sample files that I've selected from the batch of files so it's a very streamline process which allows me to you know selectively take some files out of the total batch of files and then uh push them uh into my focus area have a look at them and you know uh give them ago so I hope uh this uh this has been an enjoyable ex uh explanation experience for you and thank you so much for being a patient listener thank you"
    ],
    "transcript_word_count": 529,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "kyhsPSAB9z4",
    "title": "Annotate Overlapping Objects with Transparency Control | Labellerr",
    "description": "Simplify annotating Overlapping Objects with Labellerr's transparency control feature. Adjust annotation visibility to ensure clarity, accuracy, and quality in complex images. Perfect for managing intricate annotation projects effortlessly.\n\nChapters\n0:00 Introduction to Labellerr's Transparency Feature\n0:08 Annotating Overlapping Objects\n0:16 Adjusting Polygon Transparency\n0:25 Annotating Cargo Ships\n1:23 Annotating Water Body\n1:35 Annotating Larger Ship\n2:08 Reducing Water Body Transparency\n2:28 Accessing Transparency Settings\n3:03 Viewing Transparent Annotations\n3:29 Importance of Transparency Control\n4:03 Conclusion and Happy Annotating\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=kyhsPSAB9z4",
    "embed_url": "https://www.youtube.com/embed/kyhsPSAB9z4",
    "duration": 247,
    "view_count": 76,
    "upload_date": "20240104",
    "uploader": "Labellerr",
    "tags": [
      "overlapping shapes",
      "unity selection",
      "select lines",
      "steel overlap",
      "steel design",
      "unity object selection",
      "autocad selection",
      "design principles",
      "industrial design",
      "design overlap",
      "overlapping design",
      "unity graphics design",
      "conceptual design",
      "cad design",
      "design techniques",
      "conceptual art",
      "3d design",
      "visual cohesion",
      "unity shapes",
      "autocad hacks",
      "autocad design",
      "cad techniques",
      "artistic unity",
      "industrial graphics",
      "selection shortcuts",
      "3d modeling",
      "industrial aesthetics"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "labellerr platform and uh I'm introducing a very cool feature in this and which is especially relevant you know when you're annotating overlapping objects uh so the feature I'll introduce is uh adjusting the transparency of the polygons you draw uh while annotating your objects inside an image so as you can see there is one image and uh it has uh a number of cargo ships so we have four cargo ships and one uh water body and I want to annotate all of them so what I'll do is first I'll annotate my cargo ships so uh I'll do this and I draw polygon right here right here right here right here and right here so voila so one is done and then I'll draw another one right here right here I'll draw one Anchor Point here I'll draw one Anchor Point here and I'll complete this and I'll draw another one uh right here and uh so this is the another one cargo ship one here one here one here one here one here and one here so it's done then I'll annotate this one this one this one this one this one this one and this one now I have to annotate my water body so uh the Water body uh and I have to annotate the bigger ship also so what I'll do is you know I'll I'll first annotate the bigger ship so I'll draw one Anchor Point here one Anchor Point here one Anchor Point here one Anchor Point here one Anchor Point here one Anchor Point here one here one here and it's done now I'll select water class and I'll annotate my water body so one here one here one here one here one here and now you can see that you know this uh water body it uh it's very dark and you have to uh uh reduce its transparency because uh because I cannot see my other annotations uh that is the other little cargo ships so what I'll do is you know I'll go to settings and I'll click on update annotation questions and uh you can see that water body is here and I'll click on miscellaneous properties and I'll reduce the transparency of my water body annotation and do back and save changes in current project guidelines so you can see that you know the water body it has been uh annotated and uh it has a lesser transparency right now I can I can reduce it uh more so uh this feature is very important uh because uh especially in cases you know where the annotations are overlapping and uh especially if you want to do a quality assessment and uh you want to see all the annotations that have been done inside an image and uh so transparency adjusting transparency this kind of control that is adjusting transparency becomes very important here because you want to reduce the transparency and see all the uh underlying annotations uh so that uh the underlying annotations become prominent so this feature is uh very important there and uh you can easily uh you know annotate a number of overlying underlying objects of this feature so happy annotating uh with very complex images thank you",
    "transcript_chunks": [
      "labellerr platform and uh I'm introducing a very cool feature in this and which is especially relevant you know when you're annotating overlapping objects uh so the feature I'll introduce is uh adjusting the transparency of the polygons you draw uh while annotating your objects inside an image so as you can see there is one image and uh it has uh a number of cargo ships so we have four cargo ships and one uh water body and I want to annotate all of them so what I'll do is first I'll annotate my cargo ships so uh I'll do this and I draw polygon right here right here right here right here and right here so voila so one is done and then I'll draw another one right here right here I'll draw one Anchor Point here I'll draw one Anchor Point here and I'll complete this and I'll draw another one uh right here and uh so this is the another one cargo ship one here one here one here one here one here and one here so it's done then I'll annotate this one this one this one this one this one this one and this one now I have to annotate my water body so uh the Water body uh and I have to annotate the bigger ship also so what I'll do is you know I'll I'll first annotate the bigger ship so I'll draw one Anchor Point here one Anchor Point here one Anchor Point here one Anchor Point here one Anchor Point here one Anchor Point here one here one here and it's done now I'll select water class and I'll annotate my water body so one here one here one here one here one here and now you can see that",
      "you know this uh water body it uh it's very dark and you have to uh uh reduce its transparency because uh because I cannot see my other annotations uh that is the other little cargo ships so what I'll do is you know I'll go to settings and I'll click on update annotation questions and uh you can see that water body is here and I'll click on miscellaneous properties and I'll reduce the transparency of my water body annotation and do back and save changes in current project guidelines so you can see that you know the water body it has been uh annotated and uh it has a lesser transparency right now I can I can reduce it uh more so uh this feature is very important uh because uh especially in cases you know where the annotations are overlapping and uh especially if you want to do a quality assessment and uh you want to see all the annotations that have been done inside an image and uh so transparency adjusting transparency this kind of control that is adjusting transparency becomes very important here because you want to reduce the transparency and see all the uh underlying annotations uh so that uh the underlying annotations become prominent so this feature is uh very important there and uh you can easily uh you know annotate a number of overlying underlying objects of this feature so happy annotating uh with very complex images thank you"
    ],
    "transcript_word_count": 547,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "VWP0om1k_qw",
    "title": "Annotate Objects with Precision Using Attribute-Based Labels | Labellerr",
    "description": "Enhance your annotation process with Attribute-Based Labels on Labellerr. Specify object features like location, type, size, and color to achieve precise annotations for your projects. Simplify complex annotation tasks effortlessly.\n\nChapters\n0:00 Introduction to Labellerr Platform\n0:06 Annotating Boxes in Images\n0:17 Importance of Specific Attributes\n0:48 Selecting and Annotating a Box\n1:08 Submitting Attributes for Annotation\n1:18 Creating Custom Annotations\n1:30 Accessing Annotation Settings\n1:56 Adding Color as an Attribute\n2:30 Saving Annotation Labels\n2:47 Making Annotation a Breeze\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=VWP0om1k_qw",
    "embed_url": "https://www.youtube.com/embed/VWP0om1k_qw",
    "duration": 176,
    "view_count": 47,
    "upload_date": "20240104",
    "uploader": "Labellerr",
    "tags": [
      "label text",
      "value labels",
      "gmail labels",
      "label texxt",
      "multiple variable labels",
      "how to label my emails",
      "label qgis",
      "qgis labels",
      "map labels",
      "data labeling",
      "arcgis tutorial",
      "qgis mapping",
      "data visualization",
      "arcgis labels",
      "qgis tips",
      "data representation",
      "map design",
      "arcgis mapping",
      "visual data",
      "label design",
      "qgis tutorials",
      "labels design",
      "gmail organization",
      "data mapping",
      "qgis projects",
      "labels",
      "label",
      "add labels",
      "sensitivity labels"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "so this is labellerr platform and uh here you can see an image of boxes uh which we have to annotate and uh some of the boxes they are lying on the shells and some of the boxes they are lying on the ground and we have to accurately annotate an objects in its totality so you need you know specific attributes you need to specify specific attributes of an object for instance location or for instance type of the box or the color of the box now these attributes are very important because you know every object it's not uh uh placed in a similar location or uh you know it's not uh it's not in a uh similar place inside this whole picture so you need to accurately describe every object with respect to location with respect to size and everything so what do I do is I select uh the box and then I you know annotate it here and uh a popup comes up and which asks me you know what is the type of the pop box it's a cardboard box so for me it's a cardboard box and then what's the location of the Box on the ground so I uh specify both these attributes and I submit it now the box has been correctly annotated in its totality with all its features that is the location and uh the type of the box now how do I create these annotations so in order to create these annotations what I do is I go back and uh I will go to this settings now the settings has uh The annotation settings uh so The annotation settings uh I go and uh I in The annotation settings I click on annotations I click on annotations and then uh so the Box already has three attributes specified so one is the location and one is the type of the box and one is the size of the box now I may need another attribute that is uh you know maybe the color of the box so the color of the Box the color of the box and uh so I specify the color of the box as one of the attribute and uh so this is the question which would be asked and uh then I specify uh whether it's a green box or you know whether it's a blue box or a brown box so these are some of the attributes uh which I specify while creating this annotation and uh then I submit it and I save changes in the current project questions and uh now these attributes annotation labels are saved and these attributes can be easily used uh when I am inside my labeling screen to specify my object in totality so um happy annotating I mean this is the so this makes annotation a breeze so enjoy your annotation process and make it a breeze with lab platform",
    "transcript_chunks": [
      "so this is labellerr platform and uh here you can see an image of boxes uh which we have to annotate and uh some of the boxes they are lying on the shells and some of the boxes they are lying on the ground and we have to accurately annotate an objects in its totality so you need you know specific attributes you need to specify specific attributes of an object for instance location or for instance type of the box or the color of the box now these attributes are very important because you know every object it's not uh uh placed in a similar location or uh you know it's not uh it's not in a uh similar place inside this whole picture so you need to accurately describe every object with respect to location with respect to size and everything so what do I do is I select uh the box and then I you know annotate it here and uh a popup comes up and which asks me you know what is the type of the pop box it's a cardboard box so for me it's a cardboard box and then what's the location of the Box on the ground so I uh specify both these attributes and I submit it now the box has been correctly annotated in its totality with all its features that is the location and uh the type of the box now how do I create these annotations so in order to create these annotations what I do is I go back and uh I will go to this settings now the settings has uh The annotation settings uh so The annotation settings uh I go and uh I in The annotation settings I click on annotations I click on annotations",
      "and then uh so the Box already has three attributes specified so one is the location and one is the type of the box and one is the size of the box now I may need another attribute that is uh you know maybe the color of the box so the color of the Box the color of the box and uh so I specify the color of the box as one of the attribute and uh so this is the question which would be asked and uh then I specify uh whether it's a green box or you know whether it's a blue box or a brown box so these are some of the attributes uh which I specify while creating this annotation and uh then I submit it and I save changes in the current project questions and uh now these attributes annotation labels are saved and these attributes can be easily used uh when I am inside my labeling screen to specify my object in totality so um happy annotating I mean this is the so this makes annotation a breeze so enjoy your annotation process and make it a breeze with lab platform"
    ],
    "transcript_word_count": 497,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "ErYWoML8DfM",
    "title": "Effortless Text Categorization with Color-Coded Annotations | Labellerr",
    "description": "Simplify text annotation with Color-Coded Annotations on Labellerr. Use customizable colors to categorize large datasets for improved clarity and organization. Perfect for managing complex annotation tasks with ease!\n\nChapters\n0:00 Introduction to Color-Coded Annotations\n0:09 Identifying Text with Colors\n0:26 Benefits of Color Coding for Large Datasets\n0:43 How to Use Color Coding in Labellerr\n0:53 Navigating Settings for Annotations\n1:05 Customizing Colors with Hex Codes and RGB\n1:39 Simplifying Data Categorization\n1:46 Conclusion and Happy Annotating\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=ErYWoML8DfM",
    "embed_url": "https://www.youtube.com/embed/ErYWoML8DfM",
    "duration": 109,
    "view_count": 47,
    "upload_date": "20240104",
    "uploader": "Labellerr",
    "tags": [
      "annotation tips",
      "annotating",
      "education",
      "annotating books",
      "translation",
      "study organization",
      "color palette",
      "visual learning",
      "study tips",
      "notebook organization",
      "annotation strategies",
      "study motivation",
      "annotation ideas",
      "study techniques",
      "learning strategies",
      "color coding notes",
      "active learning",
      "study inspiration",
      "mind map",
      "note management",
      "book mapping",
      "study planner",
      "annotation methods"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "laer platform and you can see u a large piece of text with different annotations uh which have been identified by different colors as you can see uh there's a date uh which has been annotated by you know pink color and uh need help uh that is the service name has been identified by light green color and uh Velveta original melting cheese dip that's a product name has been identified by yellow color now this helps you categorize text Parts very easily so color coding is very uh very important when you have a a very large piece of data and you want to you know demaret a different uh categories of data with different colors so how do we do this so I'll uh give you a quick walk through of how to you know use color coding to categorize a large uh piece of text into different parts so you go to settings and you go to annotations and uh you can see that service name has been uh identified with a light green color when you click on this little light green colored box uh you'll see the hex code here now you can easily copy a different hex code uh and uh change the uh color coding for the service name or uh you can change the RGB values here and change the color uh coding for service name or you can you know click on this part and change it so very uh easy and uh you know a very large piece of text or data or some different kind of uh data can be categorized into different parts uh using color coding so here uh this feature comes into play and it makes uh identification of different parts a breeze so happy annotating with this feature of labellerr thank you",
    "transcript_chunks": [
      "laer platform and you can see u a large piece of text with different annotations uh which have been identified by different colors as you can see uh there's a date uh which has been annotated by you know pink color and uh need help uh that is the service name has been identified by light green color and uh Velveta original melting cheese dip that's a product name has been identified by yellow color now this helps you categorize text Parts very easily so color coding is very uh very important when you have a a very large piece of data and you want to you know demaret a different uh categories of data with different colors so how do we do this so I'll uh give you a quick walk through of how to you know use color coding to categorize a large uh piece of text into different parts so you go to settings and you go to annotations and uh you can see that service name has been uh identified with a light green color when you click on this little light green colored box uh you'll see the hex code here now you can easily copy a different hex code uh and uh change the uh color coding for the service name or uh you can change the RGB values here and change the color uh coding for service name or you can you know click on this part and change it so very uh easy and uh you know a very large piece of text or data or some different kind of uh data can be categorized into different parts uh using color coding so here uh this feature comes into play and it makes uh identification of different parts a breeze so happy",
      "annotating with this feature of labellerr thank you"
    ],
    "transcript_word_count": 308,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "JPhDXrh5v94",
    "title": "Monitor Annotation Quality with Real-Time Error Rate Metrics | Labellerr",
    "description": "\"Ensure precise annotations using Error Rate Metrics on Labellerr. Track and analyze mistakes in real time to maintain quality benchmarks and enhance overall annotation efficiency. Keep your projects on track with actionable insights!\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\"",
    "video_url": "https://www.youtube.com/watch?v=JPhDXrh5v94",
    "embed_url": "https://www.youtube.com/embed/JPhDXrh5v94",
    "duration": 117,
    "view_count": 28,
    "upload_date": "20240103",
    "uploader": "Labellerr",
    "tags": [
      "evaluation metrics",
      "mean squared error",
      "test metrics",
      "testing metrics",
      "product metrics",
      "process metrics",
      "software test metrics",
      "accuracy metrics",
      "software testing metrics",
      "confusion matrix metrics",
      "regression analysis",
      "quality assurance",
      "service evaluation",
      "software testing",
      "confusion matrix",
      "qa tools",
      "software kpis",
      "data validation",
      "automation testing",
      "model evaluation",
      "kpi analysis",
      "data analysis",
      "kpi tracking",
      "qa testing",
      "data testing",
      "software evaluation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "labellerr platform and I would be talking about a very important metric that acts like a flag Post in guiding the quality of The annotation process now uh you cannot uh you know uh monitor the quality of annot without uh uh actually checking some numbers uh which uh and uh these numbers should be on your uh tab so uh I'm talking about a number that's error rate and you can see that the error rate has been uh uh it is running at 1.06% right now and uh you can actually uh click on uh you can click on this expand icon and you can see the number of annotations where uh you know mistakes were there so uh you can see that 22 with mistake remarks from client so uh there are you know annotations have remarks which have been categorized as suggestions and mistakes and the actual uh an I mean the actual the annotations where there are big mistakes they are 22 so uh this helps you drill down to that number that you know these are the uh these are the numbers of annotations where uh some mistakes were done during the overall annotation process and the overall uh error rate which has been calculated uh through this number is 1.06% and this is the real error rate uh so this number guides all the parties that you know uh okay this is the error rate and uh uh The Benchmark is this and it it should not Rise Above This and all the parties should you know actually uh strive to achieve error rates uh below this number so it's it's a guiding number and uh it helps uh it actually keeps every party on its tiptoes when doing the overall annotation process so I hope you enjoyed this explanatory video about this very important metric happy annotating",
    "transcript_chunks": [
      "labellerr platform and I would be talking about a very important metric that acts like a flag Post in guiding the quality of The annotation process now uh you cannot uh you know uh monitor the quality of annot without uh uh actually checking some numbers uh which uh and uh these numbers should be on your uh tab so uh I'm talking about a number that's error rate and you can see that the error rate has been uh uh it is running at 1.06% right now and uh you can actually uh click on uh you can click on this expand icon and you can see the number of annotations where uh you know mistakes were there so uh you can see that 22 with mistake remarks from client so uh there are you know annotations have remarks which have been categorized as suggestions and mistakes and the actual uh an I mean the actual the annotations where there are big mistakes they are 22 so uh this helps you drill down to that number that you know these are the uh these are the numbers of annotations where uh some mistakes were done during the overall annotation process and the overall uh error rate which has been calculated uh through this number is 1.06% and this is the real error rate uh so this number guides all the parties that you know uh okay this is the error rate and uh uh The Benchmark is this and it it should not Rise Above This and all the parties should you know actually uh strive to achieve error rates uh below this number so it's it's a guiding number and uh it helps uh it actually keeps every party on its tiptoes when doing the overall annotation process",
      "so I hope you enjoyed this explanatory video about this very important metric happy annotating"
    ],
    "transcript_word_count": 315,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "tqM7V_hEHck",
    "title": "Enhance Annotation Quality with Remark-Based File Search | Labellerr",
    "description": "\"Optimize your annotation process using Remark-Based File Search on Labellerr. Easily locate files with mistakes or suggestions to refine annotations and achieve better accuracy. Revolutionize your workflow today!\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\"",
    "video_url": "https://www.youtube.com/watch?v=tqM7V_hEHck",
    "embed_url": "https://www.youtube.com/embed/tqM7V_hEHck",
    "duration": 198,
    "view_count": 31,
    "upload_date": "20240103",
    "uploader": "Labellerr",
    "tags": [
      "find annotation mistakes",
      "client rejected annotations",
      "annotation suggestions",
      "ML annotation feedback",
      "improve annotation accuracy",
      "annotation revision workflow",
      "annotation error detection",
      "annotation optimization",
      "search",
      "file search",
      "search media",
      "google search",
      "search by image",
      "fsearch",
      "pdf search",
      "search menu",
      "search feature",
      "power bi search",
      "keyword search",
      "search shortcuts",
      "perplexity ai",
      "power bi",
      "keyword finder",
      "ai search",
      "media search",
      "annotation quality control"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "this is the labellerr platform and I would be discussing a very great feature that is searching uh searching files by you know certain remark categories now uh now there are two types now because of the subjectivity of the whole annotation process you may realize that uh some mistakes would be done by uh the annotators uh some mistakes would be done from the annotator side and uh there can be two kinds of uh mistakes so some can be you know blatant mistakes or you know in some annotation areas uh there there may be C uh certain scope for improvement and you want to see uh you know all these kinds of files and especially uh these files uh files under the client rejected categories because uh this provides data for the future annotation process and it makes the future annotation process uh you know much uh accurate so what I'll do is I can I can select all the files from here and then I'll click on uh my remarks and then I'll uh click on search by tag and then I'll click on I'll check the mistakes uh checkbox and I'll apply my filters and you can see that there are three files which have the status of client rejected and all these files uh have some mistake in The annotation so this guides uh focused uh this guides my attention as a client reviewer to the files where some mistakes were done during The annotation process as you can see I can click uh so I can click on the remarks uh icon and I'll see that uh okay this is a mistake and the mistake is this is not green so I can easily identify okay uh you know this is the area where some mistake uh has been done and uh this helps me uh guide revisions in this area in the future annotation process and this will help me achieve higher accuracy see in future in this whole annotation process so similarly I can also uh see the suggestions and uh suggestions so right now if I click on suggestions and uh I click on apply filters so you'll see that there's one file uh which have which has been client rejected and uh it has a suggestion uh a suggestion has been provided on this annotation and it's specific useful because suggestions uh they tell uh about what is the scope of improvement uh and in The annotation process and this is especially important because all the parties would get to know what are the Baseline uh you know expectations of the clients when it comes to guiding the whole annotation process in future so I think this feature is a great we to communicate uh all the you know uh mistakes or scopes of uh scopes for improvement in the whole annotation process and I hope you enjoyed uh this uh explanatory video about uh this feature so happy annotating and I hope uh labellerr platform is making your annotation process uh a breeze thank you",
    "transcript_chunks": [
      "this is the labellerr platform and I would be discussing a very great feature that is searching uh searching files by you know certain remark categories now uh now there are two types now because of the subjectivity of the whole annotation process you may realize that uh some mistakes would be done by uh the annotators uh some mistakes would be done from the annotator side and uh there can be two kinds of uh mistakes so some can be you know blatant mistakes or you know in some annotation areas uh there there may be C uh certain scope for improvement and you want to see uh you know all these kinds of files and especially uh these files uh files under the client rejected categories because uh this provides data for the future annotation process and it makes the future annotation process uh you know much uh accurate so what I'll do is I can I can select all the files from here and then I'll click on uh my remarks and then I'll uh click on search by tag and then I'll click on I'll check the mistakes uh checkbox and I'll apply my filters and you can see that there are three files which have the status of client rejected and all these files uh have some mistake in The annotation so this guides uh focused uh this guides my attention as a client reviewer to the files where some mistakes were done during The annotation process as you can see I can click uh so I can click on the remarks uh icon and I'll see that uh okay this is a mistake and the mistake is this is not green so I can easily identify okay uh you know this is the area",
      "where some mistake uh has been done and uh this helps me uh guide revisions in this area in the future annotation process and this will help me achieve higher accuracy see in future in this whole annotation process so similarly I can also uh see the suggestions and uh suggestions so right now if I click on suggestions and uh I click on apply filters so you'll see that there's one file uh which have which has been client rejected and uh it has a suggestion uh a suggestion has been provided on this annotation and it's specific useful because suggestions uh they tell uh about what is the scope of improvement uh and in The annotation process and this is especially important because all the parties would get to know what are the Baseline uh you know expectations of the clients when it comes to guiding the whole annotation process in future so I think this feature is a great we to communicate uh all the you know uh mistakes or scopes of uh scopes for improvement in the whole annotation process and I hope you enjoyed uh this uh explanatory video about uh this feature so happy annotating and I hope uh labellerr platform is making your annotation process uh a breeze thank you"
    ],
    "transcript_word_count": 517,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "22dxf-u8HiE",
    "title": "Achieve Annotation Consensus with Remarks on Annotations | Labellerr",
    "description": "Boost annotation accuracy with Remarks on Annotations in Labellerr. Collaborate seamlessly by providing comments, marking mistakes, and suggesting improvements on complex annotations. Ensure high-quality and accurate labeling outcomes with this essential feature.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=22dxf-u8HiE",
    "embed_url": "https://www.youtube.com/embed/22dxf-u8HiE",
    "duration": 242,
    "view_count": 87,
    "upload_date": "20240103",
    "uploader": "Labellerr",
    "tags": [
      "book annotations",
      "youtube annotations",
      "pdf annotation",
      "annotation tips",
      "annotation tutorial",
      "how to write an annotation",
      "annotation methods",
      "zotero tutorial",
      "text analysis",
      "annotation ideas",
      "academic writing",
      "annotation software",
      "online learning",
      "pdf markup",
      "learning strategies",
      "literature review",
      "zotero setup",
      "annotation skills",
      "annotation collaboration",
      "annotation comments",
      "mark annotation mistakes",
      "annotation suggestions",
      "complex annotation",
      "annotation quality control"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "so this is the labellerr platform and I would be discussing a very great feature that is remarks on annotations remarks on annotation specifically comes into play because annotations uh annotations uh it's a very subjective area and annotators may be very subjective around a particular annotation they may need to provide their comments on certain annotations also the other parties that is the client reviewers and reviewers may need to provide additional comments as answers to these comments uh they may also uh need to provide the suggestions on annotations or they may also want to uh provide uh point out mistakes around these annotations this collaborative process of pointing issues in these annotations helps in res uh helps in achievement of 95% accuracy in future annotations so let's go to the client review screen where the client can you know easily review all the comments uh Around The annotation and I'll take a specific use case where uh of you know a specific use case where there are lot of attributes uh around an annotation and lot of attributes implies lot of subjectivity in an annotation so uh so in this case you know remarks on annotation feature helps all parties a lot uh in order to achieve consensus on all the parameters for example in this specific use case you can see it has been annotated under specific categories such as stability weight limit balance box proximity box orientation which are very relevant when you are discussing the stability of this whole ship uh in in this image so you can see that stability has been marked as at the risk of tipping so I think uh as an as a client review I think this ship is quite stable and because the cargo uh is evenly distributed throughout the ship and uh so I'll put a remark as this ship is quite stable as cargo is evenly distributed throughout this sh check and I'll mark this as a mistake because this has been marked stability under stability it has been marked the answer was it is at the risk of tipping so this is a mistake so I'll add my remark and uh so you have your remark here and it has been categorized as a mistake so also I can see uh other you know other parameters that is one is balance and it has been suggested that uh uh the cargo it is evenly distributed and so uh it and I think this is right so you can see you know that uh and I can see all the commments uh provided by other parties for example there's one commment that there would be more uh there should be more attributes while discussing balance for a cargo ship and you know these uh all these remarks uh helps in uh open collaboration with other parties and it helps uh all the parties annotate better and uh when when you'll be annotating another batch of images uh so it will help all the parties uh achieve more consensus in annotations and it it will help all parties achieve a higher accuracy in future annotations so happy annotating with uh you know with specific use cases where there are lot of attributes lot of parameters and a lot of consensus is needed around all these annotations so happy annotating have a great annotation Journey with laer thank you",
    "transcript_chunks": [
      "so this is the labellerr platform and I would be discussing a very great feature that is remarks on annotations remarks on annotation specifically comes into play because annotations uh annotations uh it's a very subjective area and annotators may be very subjective around a particular annotation they may need to provide their comments on certain annotations also the other parties that is the client reviewers and reviewers may need to provide additional comments as answers to these comments uh they may also uh need to provide the suggestions on annotations or they may also want to uh provide uh point out mistakes around these annotations this collaborative process of pointing issues in these annotations helps in res uh helps in achievement of 95% accuracy in future annotations so let's go to the client review screen where the client can you know easily review all the comments uh Around The annotation and I'll take a specific use case where uh of you know a specific use case where there are lot of attributes uh around an annotation and lot of attributes implies lot of subjectivity in an annotation so uh so in this case you know remarks on annotation feature helps all parties a lot uh in order to achieve consensus on all the parameters for example in this specific use case you can see it has been annotated under specific categories such as stability weight limit balance box proximity box orientation which are very relevant when you are discussing the stability of this whole ship uh in in this image so you can see that stability has been marked as at the risk of tipping so I think uh as an as a client review I think this ship is quite stable and because the cargo uh is evenly",
      "distributed throughout the ship and uh so I'll put a remark as this ship is quite stable as cargo is evenly distributed throughout this sh check and I'll mark this as a mistake because this has been marked stability under stability it has been marked the answer was it is at the risk of tipping so this is a mistake so I'll add my remark and uh so you have your remark here and it has been categorized as a mistake so also I can see uh other you know other parameters that is one is balance and it has been suggested that uh uh the cargo it is evenly distributed and so uh it and I think this is right so you can see you know that uh and I can see all the commments uh provided by other parties for example there's one commment that there would be more uh there should be more attributes while discussing balance for a cargo ship and you know these uh all these remarks uh helps in uh open collaboration with other parties and it helps uh all the parties annotate better and uh when when you'll be annotating another batch of images uh so it will help all the parties uh achieve more consensus in annotations and it it will help all parties achieve a higher accuracy in future annotations so happy annotating with uh you know with specific use cases where there are lot of attributes lot of parameters and a lot of consensus is needed around all these annotations so happy annotating have a great annotation Journey with laer thank you"
    ],
    "transcript_word_count": 572,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "6_GgufO9uSE",
    "title": "User Activity Tracking and Velocity Insights for Annotation Performance | Labellerr",
    "description": "Enhance your team's efficiency with Labellerr's User Activity Tracking and User Level Velocity Tracking. Monitor file rejection rates, time spent on tasks, and individual user performance to ensure high-quality annotations. Empower your annotation process with actionable insights.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=6_GgufO9uSE",
    "embed_url": "https://www.youtube.com/embed/6_GgufO9uSE",
    "duration": 99,
    "view_count": 41,
    "upload_date": "20231229",
    "uploader": "Labellerr",
    "tags": [
      "user activity",
      "activity tracking",
      "user activity tracking",
      "tracking",
      "user activity log",
      "time tracking",
      "time management",
      "laravel analytics",
      "audit trail tracking",
      "performance tracking",
      "laravel logs",
      "real-time tracking",
      "velocity tracking",
      "annotation dashboard",
      "data labeling metrics",
      "annotation team management",
      "monitor annotator performance",
      "track annotation progress",
      "ML data labeling insights",
      "annotation analytics"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "this is the labellerr platform and I'm looking at the dashboard of the labellerr platform and I would be talking about two very important parts of the dashboard one is user activity tracking and the other is user level velocity tracking now we uh we received a requirement from the customer that you know uh who wanted to know about the number of files rejected by each team member so what user activity tracking does is it helps uh the customer know the number of files rejected by each team member for example you can see the number of files rejected by this team member is 12 and uh you can also know the average duration uh you know time spent uh by this user uh on this task so the other part is user level velocity tracking now uh it's also important to know the average time spent by any user in completing the files the completing the task beat annotator review or client review so if you select one particular us user you'll know the number of files done by this particular user and you'll also know the average time spent by this user on uh each of these files uh this is a very important metric because it helps you know judge uh the performance of the team members in The annotation process and it helps you know U control and guide the overall quality of The annotation process so I hope this explanatory video uh gave uh you know a very clearer view of these two important features thank you thank you for being a patient listener",
    "transcript_chunks": [
      "this is the labellerr platform and I'm looking at the dashboard of the labellerr platform and I would be talking about two very important parts of the dashboard one is user activity tracking and the other is user level velocity tracking now we uh we received a requirement from the customer that you know uh who wanted to know about the number of files rejected by each team member so what user activity tracking does is it helps uh the customer know the number of files rejected by each team member for example you can see the number of files rejected by this team member is 12 and uh you can also know the average duration uh you know time spent uh by this user uh on this task so the other part is user level velocity tracking now uh it's also important to know the average time spent by any user in completing the files the completing the task beat annotator review or client review so if you select one particular us user you'll know the number of files done by this particular user and you'll also know the average time spent by this user on uh each of these files uh this is a very important metric because it helps you know judge uh the performance of the team members in The annotation process and it helps you know U control and guide the overall quality of The annotation process so I hope this explanatory video uh gave uh you know a very clearer view of these two important features thank you thank you for being a patient listener"
    ],
    "transcript_word_count": 271,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "6eDOEhcS9wM",
    "title": "Optimize Annotation Processes with Labeling Trends and Label Distribution | Labellerr",
    "description": "Discover how Labellerr's Labeling Trends and Label Distribution features empower annotation teams. Track file statuses, optimize resource scheduling, and choose the best annotation methods for accurate and efficient labeling. Streamline your annotation workflow with actionable data insights.\n\nChapters\n0:00 Introduction to Labellerr Platform\n0:04 Overview of the Dashboard\n0:08 Understanding the Annotation Workflow\n1:03 Exploring Labeling Trends\n2:22 Importance of Label Distribution\n3:23 Conclusion and Strategic Insights\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=6eDOEhcS9wM",
    "embed_url": "https://www.youtube.com/embed/6eDOEhcS9wM",
    "duration": 219,
    "view_count": 37,
    "upload_date": "20231229",
    "uploader": "Labellerr",
    "tags": [
      "starting a record label",
      "how to run a record label",
      "record label startup",
      "artist promotion",
      "startup record label",
      "digital distribution",
      "music startup",
      "record label basics",
      "independent artist support",
      "indie music",
      "record business",
      "recording studio",
      "music marketing",
      "promote music",
      "producer tips",
      "recording tips",
      "Annotation Platform",
      "Labeller Tutorial",
      "AI Data Annotation",
      "Annotation Process",
      "Optimize Data Labeling",
      "Data Labeling Tools",
      "Data Annotation Strategies"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "so this is the labellerr platform and I'm looking at the dashboard of labellerr platform and as you can see uh it's a whole data repository to guide The annotation process in future so a file when it comes inside labellerr it passes between different parties so the annotator is assigned the file the annotator annotates the file submits it and it passes to the internal reviewer the internal reviewer can either accept the file reject the file or skip the file if he rejects it it passes to the annotator again and if he uh you know uh skips it it goes into the category of review skipped now the reviewer May accept the file and it goes further to the client reviewer the client reviewer can either accept reject or skip the file and if he accepts it then it goes to the export and if he skips the file it goes into the client review skipped category if it if he rejects it then this file goes back to the internal reviewer and so you can see the counts of all the statuses at the top now labeling Trends is a very important area because this guides your resource scheduling uh you know uh with respect to time with respect to people or uh with respect to any party so you know for example you know if uh it it tells you you know what are the number of data files that uh have a particular status and uh uh across certain time period so um across certain time period for example you can see the time periods are daily weekly monthly and these statuses are review skip client review skipped rejected uh Etc now if I you know if I see that if a if there are large number of files that are getting client reviews skipped on weekends so I'll try to allocate you know I'll try to schedule uh the reviewing by client reviewers on you know other days something like that so this uh helps you strategy you know uh think strategically about your resources especially about time or especially uh you know uh especially about time because you know time is very important it's a very important resource uh during The annotation process you want the AI teams to get uh their accurately labeled files at the right time so this is how labeling Trends helps you now uh we can see the label distribution now label distribution is very important because it helps you see the count of different labels across different files and uh this is uh this is very important uh parameter because you can see that you know uh you know if if there are a lot of accepted files for example coffee pod has lot of acceptances when uh polygon has been used for annotating so you know that polygon has uh you know accurately annotated objects similar to coffee pot so you will uh you know uh it will guide The annotation process uh in future because you know that you know if there's something like uh if there is an object which is you know something like coffee pot then polygon is the right method for annotation so it helps you a lot you know in thinking how you can you know do The annotation process accurately across different types of labels I hope uh you know uh this explanatory video helped you you know know the importance of this uh data repository and I hope hope this uh will help you strategize your resources in the right direction while doing The annotation process thank you thanks a lot",
    "transcript_chunks": [
      "so this is the labellerr platform and I'm looking at the dashboard of labellerr platform and as you can see uh it's a whole data repository to guide The annotation process in future so a file when it comes inside labellerr it passes between different parties so the annotator is assigned the file the annotator annotates the file submits it and it passes to the internal reviewer the internal reviewer can either accept the file reject the file or skip the file if he rejects it it passes to the annotator again and if he uh you know uh skips it it goes into the category of review skipped now the reviewer May accept the file and it goes further to the client reviewer the client reviewer can either accept reject or skip the file and if he accepts it then it goes to the export and if he skips the file it goes into the client review skipped category if it if he rejects it then this file goes back to the internal reviewer and so you can see the counts of all the statuses at the top now labeling Trends is a very important area because this guides your resource scheduling uh you know uh with respect to time with respect to people or uh with respect to any party so you know for example you know if uh it it tells you you know what are the number of data files that uh have a particular status and uh uh across certain time period so um across certain time period for example you can see the time periods are daily weekly monthly and these statuses are review skip client review skipped rejected uh Etc now if I you know if I see that if a if there",
      "are large number of files that are getting client reviews skipped on weekends so I'll try to allocate you know I'll try to schedule uh the reviewing by client reviewers on you know other days something like that so this uh helps you strategy you know uh think strategically about your resources especially about time or especially uh you know uh especially about time because you know time is very important it's a very important resource uh during The annotation process you want the AI teams to get uh their accurately labeled files at the right time so this is how labeling Trends helps you now uh we can see the label distribution now label distribution is very important because it helps you see the count of different labels across different files and uh this is uh this is very important uh parameter because you can see that you know uh you know if if there are a lot of accepted files for example coffee pod has lot of acceptances when uh polygon has been used for annotating so you know that polygon has uh you know accurately annotated objects similar to coffee pot so you will uh you know uh it will guide The annotation process uh in future because you know that you know if there's something like uh if there is an object which is you know something like coffee pot then polygon is the right method for annotation so it helps you a lot you know in thinking how you can you know do The annotation process accurately across different types of labels I hope uh you know uh this explanatory video helped you you know know the importance of this uh data repository and I hope hope this uh will help you strategize your resources",
      "in the right direction while doing The annotation process thank you thanks a lot"
    ],
    "transcript_word_count": 614,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "p0XdjxBKqF8",
    "title": "How Labellerr Helped Spare-It Succeed: A Testimonial by Laurent Meunier, Co-founder & CTO",
    "description": "Discover how Labellerr empowers customers to overcome challenges in preparing training datasets for vision, NLP, and LLM model training. In this testimonial, Laurent Meunier, Co-founder, COO, and CTO of US-based waste intelligence platform Spare-It, shares his firsthand experience using Labellerr's tools. Learn how Labellerr helped Spare-It tackle complex image segmentation and annotation tasks, delivering high-quality solutions for their AI projects.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=p0XdjxBKqF8",
    "embed_url": "https://www.youtube.com/embed/p0XdjxBKqF8",
    "duration": 54,
    "view_count": 84,
    "upload_date": "20231226",
    "uploader": "Labellerr",
    "tags": [
      "data labeling",
      "programmatic labeling",
      "data labelling",
      "image labeling",
      "audio labeling",
      "video labeling",
      "white labelling",
      "video data labeling",
      "machine learning",
      "engineering",
      "label images",
      "label",
      "online",
      "data annotation",
      "labeling software",
      "image annotation",
      "annotation software",
      "label automation",
      "labeling solutions",
      "data management",
      "ai data labeling",
      "deep learning",
      "data preprocessing",
      "computer vision",
      "ai tagging",
      "data labeling platform",
      "text tagging",
      "annotation workflow"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hello my name is Laur Monier I am CTO for Spirit we are an online platform for Waste intelligence we are analyzing the waste from our customers with AI we have been working with labellerr to annotate the images that we're capturing from our customers beIN we need very detailed segmentation and we need to be able to classify um the objects into 100 different waste material classes it's been a great experience working with label air um they've um been able to learn very fast the specifics of our domain and generate good quality annotations they are very good at taking feedbacks and have been continuously improving their platform overall I would recommend labellerr for any company who needs a great partner for AI labeling",
    "transcript_chunks": [
      "hello my name is Laur Monier I am CTO for Spirit we are an online platform for Waste intelligence we are analyzing the waste from our customers with AI we have been working with labellerr to annotate the images that we're capturing from our customers beIN we need very detailed segmentation and we need to be able to classify um the objects into 100 different waste material classes it's been a great experience working with label air um they've um been able to learn very fast the specifics of our domain and generate good quality annotations they are very good at taking feedbacks and have been continuously improving their platform overall I would recommend labellerr for any company who needs a great partner for AI labeling"
    ],
    "transcript_word_count": 126,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "0Ilo_MsI6AQ",
    "title": "LabelGPT: The Ultimate Auto Annotation Tool",
    "description": "LabelGPT is a new product built by Labellerr.\nIt can pre-label large volumes of visual data in a few minutes. Integrated into Labellerr's data annotation platform, now vision AI team can now save days annotating images manually.\nThe beta version is ready to use for free for limited images.\n\nChapters\n0:00 Introduction to LabelGPT\n0:15 upload file\n0:30 Enter prompt to detect object automatically\n0:50 Find physical damage on solar panel \n\n\nTry Now: https://www.labellerr.com/labelgpt\nTo read about latest in AI visit: https://www.labellerr.com/blog\nTo access the power image/video/text annotation performance visit https://www.labellerr.com/ to book a demo.\n\n#computervision #objectdetection #segmentation #imageclassification #deeplearning #automation #datapipeline",
    "video_url": "https://www.youtube.com/watch?v=0Ilo_MsI6AQ",
    "embed_url": "https://www.youtube.com/embed/0Ilo_MsI6AQ",
    "duration": 105,
    "view_count": 1953,
    "upload_date": "20231012",
    "uploader": "Labellerr",
    "tags": [
      "computer vision",
      "object detection",
      "segmentation",
      "data annotation",
      "deep learning",
      "machine learning",
      "image classification",
      "deeplearning",
      "image processing",
      "automation",
      "realworld ai",
      "image recognition",
      "AI data annotation",
      "prompt-based labeling",
      "automated data labeling",
      "visual data annotation",
      "image pre-labeling",
      "annotation platform",
      "generative AI annotation",
      "data labeling automation",
      "computer vision labeling",
      "bulk image annotation",
      "machine learning annotation",
      "annotation workflow"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=0Ilo_MsI6AQ! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "uoKE3GgF1vo",
    "title": "Discover the Power of Large Language Models (LLMs) | Labellerr",
    "description": "Explore how Large Language Models (LLMs) are revolutionizing AI with deep learning and massive datasets. From answering questions to generating creative content, LLMs transform industries like customer service, research, and education. Learn more about how this cutting-edge innovation empowers your AI journey.\n\nChapters\n0:00 Introduction to Large Language Models\n0:05 The Role of Language in Technology\n0:17 What is a Large Language Model (LLM)?\n0:30 Core Capabilities of LLMs\n0:51 Engaging with LLMs: Conversations and Creativity\n1:07 Applications of LLMs Across Industries\n1:34 The Future of Human-Machine Interaction\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=uoKE3GgF1vo",
    "embed_url": "https://www.youtube.com/embed/uoKE3GgF1vo",
    "duration": 104,
    "view_count": 39,
    "upload_date": "20230810",
    "uploader": "Labellerr",
    "tags": [
      "large language models",
      "what is large language model",
      "ai chatbot",
      "nlp models",
      "building llms",
      "deep learning",
      "nlp techniques",
      "ai explained",
      "machine learning",
      "chatgpt",
      "deep learning models",
      "ai automation",
      "language model tutorial",
      "nlp innovation",
      "artificial intelligence",
      "text analysis",
      "deep learning tutorial",
      "ai trends",
      "chatgpt tutorial",
      "text generation",
      "ai content",
      "deep tech",
      "machine learning hacks",
      "natural language processing",
      "ai revolution",
      "text analysis ai"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "understanding large language model in a world driven by language and information a new kind of innovation has emerged revolutionizing how we interact with technology introducing in-depth knowledge about the large language model a large language model or llm is a type of artificial intelligence I algorithm that uses deep learning techniques and massively large data sets to understand summarize generate and predict new content at its core an llm is an immense reservoir of knowledge capable of comprehending and processing vast amounts of information from a wide range of sources it learns from books articles websites and countless other texts constantly expanding its understanding of the world but it doesn't stop there a large language model can also engage in conversations answer questions provide explanations and even generate creative content it can adapt to different writing styles tones and contexts making it a versatile tool for countless applications from customer service and content creation to research and education the large language model has found its way into various Industries empowering individuals and organizations to accomplish more with less effort with its vast language capabilities an llm opens up new possibilities helping us find answers spark creativity and amplify our own abilities it's like having a language expert at your fingertips ready to assist and Inspire the large language model shaping the future of human machine interaction thanks for watching",
    "transcript_chunks": [
      "understanding large language model in a world driven by language and information a new kind of innovation has emerged revolutionizing how we interact with technology introducing in-depth knowledge about the large language model a large language model or llm is a type of artificial intelligence I algorithm that uses deep learning techniques and massively large data sets to understand summarize generate and predict new content at its core an llm is an immense reservoir of knowledge capable of comprehending and processing vast amounts of information from a wide range of sources it learns from books articles websites and countless other texts constantly expanding its understanding of the world but it doesn't stop there a large language model can also engage in conversations answer questions provide explanations and even generate creative content it can adapt to different writing styles tones and contexts making it a versatile tool for countless applications from customer service and content creation to research and education the large language model has found its way into various Industries empowering individuals and organizations to accomplish more with less effort with its vast language capabilities an llm opens up new possibilities helping us find answers spark creativity and amplify our own abilities it's like having a language expert at your fingertips ready to assist and Inspire the large language model shaping the future of human machine interaction thanks for watching"
    ],
    "transcript_word_count": 230,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "MTkyxzGGjNM",
    "title": "YOLO V8 Object Detection Model - Advanced Features | Labellerr",
    "description": "Discover the power of YOLO V8 Object Detection, the latest state-of-the-art model for accurate and customizable object detection. With YOLO V8, detect objects in images and videos effortlessly and enhance your AI capabilities. Experience speed, precision, and innovation with YOLO V8 Object Detection.\n\nChapters\n0:00 Introduction to YOLO V8\n0:06 Overview of YOLO V8 Features\n0:27 Advanced Image Stabilization\n0:36 Object Detection Capabilities\n0:50 Customization Options\n0:55 High-Quality Visual Output\n1:11 Intelligent AI Features\n1:23 Seamless Connectivity Options\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=MTkyxzGGjNM",
    "embed_url": "https://www.youtube.com/embed/MTkyxzGGjNM",
    "duration": 96,
    "view_count": 126,
    "upload_date": "20230810",
    "uploader": "Labellerr",
    "tags": [
      "object detection",
      "yolov8 object detection",
      "yolov8 custom object detection",
      "object detection python",
      "object detection pytorch",
      "yolo object detection",
      "yolov8",
      "opencv object detection",
      "yolo object detection python",
      "yolo object detection tutorial",
      "yolov5 custom object detection",
      "yolov8 tutorial",
      "opencv detection",
      "yolo implementation",
      "deep learning",
      "yolo tutorial",
      "custom yolo",
      "machine learning",
      "opencv projects",
      "data preprocessing",
      "pytorch object detection"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "introducing the next evolution in Cutting Edge technology the YOLO V8 YOLO V8 is the latest state-of-the-art object detection model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility YOLO V8 is designed to be fast accurate and easy to use making it an excellent choice for a wide range of applications with its state-of-the-art features the YOLO V8 takes your photography and videography to new heights its Advanced image stabilization ensures every shot is smooth and Shake free even in the most intense situations with yolo V8 you can detect objects in images and videos with ease the model is trained on a wide range of objects including people animals vehicles and more it's also highly customizable allowing you to fine tune the model to your specific needs the YOLO V8 is a Cutting Edge lens captures stunning detail and vibrant colors making every frame a work of art whether you're shooting in bright daylight or low light environments the YOLO V8 delivers exceptional results every time but that's not all the YOLO V8 is packed with intelligent features to enhance your creativity its smart AI assisted mode analyzes your surroundings and automatically adjusts settings for the perfect shot share your adventures instantly with the YOLO V8s as seamless connectivity options connect wirelessly to your smartphone tablet or laptop and share your stunning visuals with friends and followers at the touch of a button thanks for watching",
    "transcript_chunks": [
      "introducing the next evolution in Cutting Edge technology the YOLO V8 YOLO V8 is the latest state-of-the-art object detection model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility YOLO V8 is designed to be fast accurate and easy to use making it an excellent choice for a wide range of applications with its state-of-the-art features the YOLO V8 takes your photography and videography to new heights its Advanced image stabilization ensures every shot is smooth and Shake free even in the most intense situations with yolo V8 you can detect objects in images and videos with ease the model is trained on a wide range of objects including people animals vehicles and more it's also highly customizable allowing you to fine tune the model to your specific needs the YOLO V8 is a Cutting Edge lens captures stunning detail and vibrant colors making every frame a work of art whether you're shooting in bright daylight or low light environments the YOLO V8 delivers exceptional results every time but that's not all the YOLO V8 is packed with intelligent features to enhance your creativity its smart AI assisted mode analyzes your surroundings and automatically adjusts settings for the perfect shot share your adventures instantly with the YOLO V8s as seamless connectivity options connect wirelessly to your smartphone tablet or laptop and share your stunning visuals with friends and followers at the touch of a button thanks for watching"
    ],
    "transcript_word_count": 250,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "U2yp2PYNGtg",
    "title": "Explore DINO Self-Supervised Learning Model: Revolutionizing AI Training on Unlabeled Data",
    "description": "Discover how the DINO self-supervised learning model leverages unlabeled data for state-of-the-art performance in image classification, object detection, and natural language processing. Learn about its innovative transformer architecture and contrastive learning technique.\n\nAdditional Info:\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=U2yp2PYNGtg",
    "embed_url": "https://www.youtube.com/embed/U2yp2PYNGtg",
    "duration": 86,
    "view_count": 42,
    "upload_date": "20230710",
    "uploader": "Labellerr",
    "tags": [
      "deep learning",
      "machine learning",
      "self supervised learning",
      "self-supervised",
      "unsupervised learning",
      "self-supervised representation learning",
      "foundational model machine learning",
      "supervised learning",
      "yann lecun self supervised learning",
      "unsupervised feature learning",
      "unsupervised machine learning",
      "data representation",
      "ai research",
      "feature extraction",
      "data science",
      "ai trends",
      "data preprocessing"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Dino a new self-supervised learning model self-supervised learning is a type of machine learning where the model learns from unlabeled data this is in contrast to supervised learning where the model is trained on label data self-supervised learning is becoming increasingly popular because it can be used to train models on much larger data sets than supervised learning this is because unlabeled data is much easier to obtain than labeled data Dino is a new self-supervised learning model that has been shown to achieve state-of-the-art results on a variety of tasks including image classification object detection and natural language processing Dino stands for deep image Network for supervised learning it is a self-supervised learning model that was developed by researchers at Google AI Dino is based on the Transformer architecture which is a neural network architecture that is very effective for natural language processing tasks Dino is trained on a massive data set of unlabeled images the images are randomly cropped and resized and then the model is asked to predict the original image Dino uses a new Training Method called contrastive learning in contrastive learning the model is trained to distinguish between positive pairs of images similar images and negative pairs of images images that are different Dino is still under development but it has the potential to be used to solve a wide range of problems in computer vision natural language processing and other areas thanks for watching",
    "transcript_chunks": [
      "Dino a new self-supervised learning model self-supervised learning is a type of machine learning where the model learns from unlabeled data this is in contrast to supervised learning where the model is trained on label data self-supervised learning is becoming increasingly popular because it can be used to train models on much larger data sets than supervised learning this is because unlabeled data is much easier to obtain than labeled data Dino is a new self-supervised learning model that has been shown to achieve state-of-the-art results on a variety of tasks including image classification object detection and natural language processing Dino stands for deep image Network for supervised learning it is a self-supervised learning model that was developed by researchers at Google AI Dino is based on the Transformer architecture which is a neural network architecture that is very effective for natural language processing tasks Dino is trained on a massive data set of unlabeled images the images are randomly cropped and resized and then the model is asked to predict the original image Dino uses a new Training Method called contrastive learning in contrastive learning the model is trained to distinguish between positive pairs of images similar images and negative pairs of images images that are different Dino is still under development but it has the potential to be used to solve a wide range of problems in computer vision natural language processing and other areas thanks for watching"
    ],
    "transcript_word_count": 240,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "ItL0iwOOgg0",
    "title": "AI Model Training Stages: From Data Collection to Testing",
    "description": "Explore the six key stages of AI model training, including data collection, pre-processing, model design, training, validation, and testing. Discover how these steps ensure accuracy and reliability in AI predictions.\n\nChapters\n0:00 Introduction to AI Model Training Stages\n0:05 Overview of AI Model Training\n0:12 Stage 1: Data Collection\n0:22 Stage 2: Data Pre-processing\n0:32 Stage 3: Model Architecture Design\n0:42 Stage 4: Training\n0:54 Stage 5: Validation and Optimization\n1:09 Stage 6: Testing\n1:19 Conclusion\n\nAdditional Info:\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=ItL0iwOOgg0",
    "embed_url": "https://www.youtube.com/embed/ItL0iwOOgg0",
    "duration": 83,
    "view_count": 40,
    "upload_date": "20230710",
    "uploader": "Labellerr",
    "tags": [
      "the 7 stages of ai",
      "ai training",
      "stages of artificial intelligence",
      "7 stages of artificial intelligence",
      "training llms",
      "ai development stages",
      "ai stages",
      "chatgpt training process",
      "ai stages explained",
      "deep learning",
      "artificial intelligence",
      "learn ai",
      "machine learning",
      "model optimization",
      "deep learning models",
      "custom model training",
      "chatgpt development",
      "supervised learning",
      "data preprocessing",
      "chatgpt prompts",
      "ai tools",
      "data science",
      "algorithm optimization"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "stages of AI model training training an AI model involves several stages that help optimize its performance and enable it to make accurate predictions stage 1 data collection massive amounts of diverse and relevant data are gathered to train the model stage 2 Data pre-processing the collected data is cleaned organized and transformed into a format suitable for training stage 3 Model architecture design experts create a blueprint for the AI model deciding its structure and layers stage 4 training the model is exposed to the prepared data and algorithms learn to recognize patterns and make predictions Stage 5 validation and optimization after training the model goes through validation and optimization its performance is evaluated and adjustments are made to improve accuracy stage 6 testing real world scenarios are simulated to assess its reliability and performance thanks for watching",
    "transcript_chunks": [
      "stages of AI model training training an AI model involves several stages that help optimize its performance and enable it to make accurate predictions stage 1 data collection massive amounts of diverse and relevant data are gathered to train the model stage 2 Data pre-processing the collected data is cleaned organized and transformed into a format suitable for training stage 3 Model architecture design experts create a blueprint for the AI model deciding its structure and layers stage 4 training the model is exposed to the prepared data and algorithms learn to recognize patterns and make predictions Stage 5 validation and optimization after training the model goes through validation and optimization its performance is evaluated and adjustments are made to improve accuracy stage 6 testing real world scenarios are simulated to assess its reliability and performance thanks for watching"
    ],
    "transcript_word_count": 139,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "jdvTKnd-Lq4",
    "title": "Top 7 Automotive Datasets for Computer Vision Projects",
    "description": "Watch this captivating video to learn about the \"Top 7 Automotive Datasets for Computer Vision.\" Learn about the most comprehensive collection of top-notch datasets that will help you improve your computer vision abilities in the automobile industry. Whether you're interested in traffic analysis, object detection, or driverless vehicles, this video provides the most useful datasets to support your work. Don't pass up this crucial resource if you are an enthusiast of computer vision and automobile technology alike!\n\nBlog link: Top 7 Automotive Datasets for Computer Vision Projects\nLabelGPT: https://www.labellerr.com/labelgpt\nOur Website: https://www.labellerr.com/\n\nContact us at:\nPhone: +917565883102\nE-mail: support@tensormatics.com",
    "video_url": "https://www.youtube.com/watch?v=jdvTKnd-Lq4",
    "embed_url": "https://www.youtube.com/embed/jdvTKnd-Lq4",
    "duration": 140,
    "view_count": 29,
    "upload_date": "20230710",
    "uploader": "Labellerr",
    "tags": [
      "deep learning",
      "machine learning",
      "data preprocessing",
      "image segmentation",
      "unsupervised learning",
      "automotive datasets",
      "computer vision datasets",
      "data science",
      "computer vision",
      "image labeling",
      "vehicle detection dataset",
      "traffic analysis dataset",
      "deep learning models",
      "image classification",
      "data augmentation",
      "dataset creation",
      "object detection",
      "object detection automotive",
      "visual data",
      "traffic detection",
      "pattern recognition",
      "smart city datasets",
      "visual recognition"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "top 7 Automotive data sets for computer vision projects here are some of the top seven Automotive data sets for computer vision projects 1. kitti Vision Benchmark Suite this is a well-liked data set for computer vision applications in the automobile sector it contains various data sets for object detection tracking and scene comprehension that were obtained from a camera mounted on an automobile and other sensors 2. Apollo Escape [Music] apolloscape includes a variety of data sets for stereo depth estimation 3D object detection and semantic segmentation that were collected from a variety of sensors including litter radar and cameras over 10 000 photos and 10 000 litter scans are included in the data set together with real-world data on object detection segmentation and depth estimate 3. cityscapes a for semantic scene segmentation and scene comprehension in urban settings there is a large data set called cityscapes more than 5000 high resolution images of urban scenes are included each annotated in great detail with regard to object classes and semantic segmentation 4. new scenes for computer vision and autonomous driving research new scenes offers a sizable data set over 1.4 million images and litter Point clouds from various sensors including litter radar and cameras were used to create the over 1000 scenes that comprise the data set 5. Udacity self-driving car data set this data set contains approximately 200 000 images in litter Point clouds that were obtained from a camera and litter sensor mounted on an automobile Daimler Urban driving data set object tracking and detection in urban settings are made possible by the Daimler Urban driving data set it contains more than 100 000 photos taken with a camera mounted on an automobile and actual data on item classes and bounding boxes 7. Bosch small traffic lights data set the Bosch small traffic lights data set is a traffic light detection and recognition data set it includes over 10 000 images captured from a car mounted camera with ground truth information on traffic light classes and States thanks for watching",
    "transcript_chunks": [
      "top 7 Automotive data sets for computer vision projects here are some of the top seven Automotive data sets for computer vision projects 1. kitti Vision Benchmark Suite this is a well-liked data set for computer vision applications in the automobile sector it contains various data sets for object detection tracking and scene comprehension that were obtained from a camera mounted on an automobile and other sensors 2. Apollo Escape [Music] apolloscape includes a variety of data sets for stereo depth estimation 3D object detection and semantic segmentation that were collected from a variety of sensors including litter radar and cameras over 10 000 photos and 10 000 litter scans are included in the data set together with real-world data on object detection segmentation and depth estimate 3. cityscapes a for semantic scene segmentation and scene comprehension in urban settings there is a large data set called cityscapes more than 5000 high resolution images of urban scenes are included each annotated in great detail with regard to object classes and semantic segmentation 4. new scenes for computer vision and autonomous driving research new scenes offers a sizable data set over 1.4 million images and litter Point clouds from various sensors including litter radar and cameras were used to create the over 1000 scenes that comprise the data set 5. Udacity self-driving car data set this data set contains approximately 200 000 images in litter Point clouds that were obtained from a camera and litter sensor mounted on an automobile Daimler Urban driving data set object tracking and detection in urban settings are made possible by the Daimler Urban driving data set it contains more than 100 000 photos taken with a camera mounted on an automobile and actual data on item classes and bounding boxes 7. Bosch small traffic lights",
      "data set the Bosch small traffic lights data set is a traffic light detection and recognition data set it includes over 10 000 images captured from a car mounted camera with ground truth information on traffic light classes and States thanks for watching"
    ],
    "transcript_word_count": 343,
    "transcript_chunk_count": 2
  },
  {
    "video_id": "ZHHVrqrKV6s",
    "title": "Unleash the Power of AI with Labellerr | Best Data Annotation Services",
    "description": "\"Discover how Labellerr empowers industries with cutting-edge data annotation services tailored for AI and machine learning. From accurate labeling to seamless scalability, we help businesses unlock their true potential.\n\nExplore our solutions today!\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\"",
    "video_url": "https://www.youtube.com/watch?v=ZHHVrqrKV6s",
    "embed_url": "https://www.youtube.com/embed/ZHHVrqrKV6s",
    "duration": 104,
    "view_count": 28,
    "upload_date": "20230616",
    "uploader": "Labellerr",
    "tags": [
      "data analyst interview",
      "interview questions and answers",
      "data analytics",
      "sql interview questions and answers",
      "microservices tutorial",
      "data analysis",
      "sql basics",
      "microservices architecture",
      "cidr notation",
      "big data analytics",
      "data science interview",
      "interview prep",
      "sql query examples",
      "data modeling",
      "data visualization techniques",
      "database queries",
      "data workflow",
      "architecture patterns",
      "cidr notation explained",
      "microservices",
      "data analyst interview questions and answers"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "understanding large language model in a world driven by language and information a new kind of innovation has emerged revolutionizing how we interact with technology introducing in-depth knowledge about the large language model a large language model or llm is a type of artificial intelligence I algorithm that uses deep learning techniques and massively large data sets to understand summarize generate and predict new content at its core an llm is an immense reservoir of knowledge capable of comprehending and processing vast amounts of information from a wide range of sources it learns from books articles websites and countless other texts constantly expanding its understanding of the world but it doesn't stop there a large language model can also engage in conversations answer questions provide explanations and even generate creative content it can adapt to different writing styles tones and contexts making it a versatile tool for countless applications from customer service and content creation to research and education the large language model has found its way into various Industries empowering individuals and organizations to accomplish more with less effort with its vast language capabilities an llm opens up new possibilities helping us find answers spark creativity and amplify our own abilities it's like having a language expert at your fingertips ready to assist and Inspire the large language model shaping the future of human machine interaction thanks for watching",
    "transcript_chunks": [
      "understanding large language model in a world driven by language and information a new kind of innovation has emerged revolutionizing how we interact with technology introducing in-depth knowledge about the large language model a large language model or llm is a type of artificial intelligence I algorithm that uses deep learning techniques and massively large data sets to understand summarize generate and predict new content at its core an llm is an immense reservoir of knowledge capable of comprehending and processing vast amounts of information from a wide range of sources it learns from books articles websites and countless other texts constantly expanding its understanding of the world but it doesn't stop there a large language model can also engage in conversations answer questions provide explanations and even generate creative content it can adapt to different writing styles tones and contexts making it a versatile tool for countless applications from customer service and content creation to research and education the large language model has found its way into various Industries empowering individuals and organizations to accomplish more with less effort with its vast language capabilities an llm opens up new possibilities helping us find answers spark creativity and amplify our own abilities it's like having a language expert at your fingertips ready to assist and Inspire the large language model shaping the future of human machine interaction thanks for watching"
    ],
    "transcript_word_count": 230,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "sZHZQRmBjrc",
    "title": "Streamline AI Model Training with Labellerr | Best Data Annotation Services",
    "description": "Master the stages of AI model training with Labellerr's precise data annotation services. From data collection to real-world testing, we help AI and ML models achieve top-notch performance with clean, high-quality datasets.\n\nExplore our solutions and elevate your AI models!\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=sZHZQRmBjrc",
    "embed_url": "https://www.youtube.com/embed/sZHZQRmBjrc",
    "duration": 83,
    "view_count": 30,
    "upload_date": "20230616",
    "uploader": "Labellerr",
    "tags": [
      "lora model training",
      "large language models",
      "google training",
      "how to train gpt like model",
      "machine learning models",
      "how to train custom ai models",
      "machine learning models explained",
      "ml models explained",
      "machine learning models basics",
      "machine learning models from scratch",
      "basics of machine learning models",
      "finetune llm model",
      "lora training",
      "model insights",
      "ml training",
      "finetune gpt",
      "gpt finetuning",
      "llm optimization",
      "machine learning",
      "ai optimization",
      "lora model"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "stages of AI model training training an AI model involves several stages that help optimize its performance and enable it to make accurate predictions stage 1 data collection massive amounts of diverse and relevant data are gathered to train the model stage 2 Data pre-processing the collected data is cleaned organized and transformed into a format suitable for training stage 3 Model architecture design experts create a blueprint for the AI model deciding its structure and layers stage 4 training the model is exposed to the prepared data and algorithms learn to recognize patterns and make predictions Stage 5 validation and optimization after training the model goes through validation and optimization its performance is evaluated and adjustments are made to improve accuracy stage 6 testing real world scenarios are simulated to assess its reliability and performance thanks for watching",
    "transcript_chunks": [
      "stages of AI model training training an AI model involves several stages that help optimize its performance and enable it to make accurate predictions stage 1 data collection massive amounts of diverse and relevant data are gathered to train the model stage 2 Data pre-processing the collected data is cleaned organized and transformed into a format suitable for training stage 3 Model architecture design experts create a blueprint for the AI model deciding its structure and layers stage 4 training the model is exposed to the prepared data and algorithms learn to recognize patterns and make predictions Stage 5 validation and optimization after training the model goes through validation and optimization its performance is evaluated and adjustments are made to improve accuracy stage 6 testing real world scenarios are simulated to assess its reliability and performance thanks for watching"
    ],
    "transcript_word_count": 139,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "UDXrbeuWa4U",
    "title": "Explore Dino Self-Supervised Learning Model with Labellerr | Data Annotation Simplified",
    "description": "Dive into Dino, the revolutionary self-supervised learning model excelling in image classification, object detection, and NLP. Discover how Labellerr streamlines data annotation to power your AI projects effortlessly.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=UDXrbeuWa4U",
    "embed_url": "https://www.youtube.com/embed/UDXrbeuWa4U",
    "duration": 86,
    "view_count": 34,
    "upload_date": "20230615",
    "uploader": "Labellerr",
    "tags": [
      "deep learning",
      "machine learning",
      "self-supervised learning",
      "self supervised learning",
      "unsupervised learning",
      "self-supervised",
      "supervised learning",
      "unsupervised machine learning",
      "self supervised learning computer vision",
      "unsupervised feature learning",
      "emerging properties in self-supervised vision transformers",
      "self learning",
      "semi-supervised",
      "neural network distillation",
      "feature extraction",
      "data augmentation",
      "data science",
      "computer vision"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "Dino a new self-supervised learning model self-supervised learning is a type of machine learning where the model learns from unlabeled data this is in contrast to supervised learning where the model is trained on label data self-supervised learning is becoming increasingly popular because it can be used to train models on much larger data sets than supervised learning this is because unlabeled data is much easier to obtain than label data Dino is a new self-supervised learning model that has been shown to achieve state-of-the-art results on a variety of tasks including image classification object detection and natural language processing Dino stands for deep image Network for supervised learning it is a self-supervised learning model that was developed by researchers at Google AI Dino is based on the Transformer architecture which is a neural network architecture that is very effective for natural language processing tasks Dino is trained on a massive data set of unlabeled images the images are randomly cropped and resized and then the model is asked to predict the original image Dino uses a new Training Method called contrastive learning in contrastive learning the model is trained to distinguish between positive pairs of images similar images and negative pairs of images images that are different Dino is still under development but it has the potential to be used to solve a wide range of problems in computer vision natural language processing and other areas thanks for watching",
    "transcript_chunks": [
      "Dino a new self-supervised learning model self-supervised learning is a type of machine learning where the model learns from unlabeled data this is in contrast to supervised learning where the model is trained on label data self-supervised learning is becoming increasingly popular because it can be used to train models on much larger data sets than supervised learning this is because unlabeled data is much easier to obtain than label data Dino is a new self-supervised learning model that has been shown to achieve state-of-the-art results on a variety of tasks including image classification object detection and natural language processing Dino stands for deep image Network for supervised learning it is a self-supervised learning model that was developed by researchers at Google AI Dino is based on the Transformer architecture which is a neural network architecture that is very effective for natural language processing tasks Dino is trained on a massive data set of unlabeled images the images are randomly cropped and resized and then the model is asked to predict the original image Dino uses a new Training Method called contrastive learning in contrastive learning the model is trained to distinguish between positive pairs of images similar images and negative pairs of images images that are different Dino is still under development but it has the potential to be used to solve a wide range of problems in computer vision natural language processing and other areas thanks for watching"
    ],
    "transcript_word_count": 240,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "wE2lxss_m_U",
    "title": "LabelGPT: Turns raw images into labeled images in minutes",
    "description": "Labellerr launches a new generative AI-based data labeling tool. Our labeling engine uses foundation model-based zero-shot labeling automation to enable ML teams to generate large volumes of labeled data. \nGenerative AI-powered prompt-based labeling makes it super fast.\nIn just 3 Simple Steps To Generate Large Volume, High-Quality Labels\n1. Import your data\n2. Give prompt\n3. Review the labels\n\nBenefits:\nNo Manual Labeling\n1000s of labels in minutes\nEasy review\n\nvisit our website to know more- https://www.labellerr.com/labelgpt",
    "video_url": "https://www.youtube.com/watch?v=wE2lxss_m_U",
    "embed_url": "https://www.youtube.com/embed/wE2lxss_m_U",
    "duration": 81,
    "view_count": 377,
    "upload_date": "20230509",
    "uploader": "Labellerr",
    "tags": [
      "machine learning",
      "ai tools",
      "dataset creation",
      "deep learning",
      "data preprocessing",
      "data labeling automation",
      "AI data annotation",
      "prompt engineering",
      "data labeling tools",
      "data processing frameworks",
      "ai automation",
      "data annotation tools",
      "automated labeling",
      "data cleaning techniques",
      "data pipeline automation",
      "dataset generation",
      "generative ai",
      "data annotation",
      "data pipeline",
      "data engineering",
      "data augmentation",
      "ai training",
      "data labellerr"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=wE2lxss_m_U! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "EkpbGa4sfxQ",
    "title": "Labellerr: Unleash the power of automation in training data generation pipeline (DIY Sandbox)",
    "description": "AI-ML teams developing production-ready computer vision AI used to have a major problem: the immense requirement of training data in order to train their model. \n\nUntil now, the process was notorious for its manual and tedious nature. But, no more! \nWith Labellerr, automation has been brought to the process, allowing you to easily draw bounding boxes, and classify, and segment images with just one click.\n\nWant to check it out? Visit https://demo.labellerr.com/sandbox without any signup. \nJust go to the URL, and you'll be automating your annotation in no time! \n\nIf you want to implement the power of automation in your production pipeline, reach out to us or write directly to support@tensormatics.com. \nYou can also use our contact form submission at https://labellerr.com/contact-us. \n\nReady, set, automate!",
    "video_url": "https://www.youtube.com/watch?v=EkpbGa4sfxQ",
    "embed_url": "https://www.youtube.com/embed/EkpbGa4sfxQ",
    "duration": 101,
    "view_count": 48,
    "upload_date": "20230221",
    "uploader": "Labellerr",
    "tags": [
      "computer vision",
      "ai models",
      "computer vision annotation",
      "ai optimization",
      "data validation",
      "deep learning models",
      "AI data labeling",
      "vision datasets",
      "algorithm optimization",
      "image labeling",
      "bounding box annotation",
      "image segmentation",
      "image classification",
      "image processing",
      "data annotation",
      "deep learning",
      "machine learning",
      "quality assurance",
      "data preprocessing",
      "image annotation",
      "data governance",
      "ai training",
      "data analysis",
      "data science",
      "python",
      "neural networks"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=EkpbGa4sfxQ! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "Z5YdaJxHhbo",
    "title": "Labellerr Feature demo 4: Copy Previous file to speed up classification annotation",
    "description": "10,000 hrs!!!\nAn average annotation team spent on a classification project of 1,50,000 images large dataset.\n\nThat's the time an average annotation has to spend on labeling on a classification with an average of 60-70 labels per image.\nIn most of the project up to 70% of labels are the same, it's just 30% of labels that changes with every image/video.\nIf it's a video project, the time would go up by 150%.\nTo save labelling time, Labellerr introduced copy the label feature which reduces the time spent on each image by up to 70%.\nNow, annotators can leverage their past annotations and copy all the labels with just ONE CLICK!\nThat's not it.\nFor large and diverse datasets, our system sorts the annotated image based on the similarity with the highest chance of similar labels.\nThis additionally saves annotators time to look for the right image to copy, because it will always be at the top.\n\nSee the demo below.\nDo you also have a classification project, contact our team at support@tensormatics.com to set it up for you.\n\n#classification #annotation #trainingdata #automation #computervision #feature #announcement #productivity",
    "video_url": "https://www.youtube.com/watch?v=Z5YdaJxHhbo",
    "embed_url": "https://www.youtube.com/embed/Z5YdaJxHhbo",
    "duration": 40,
    "view_count": 76,
    "upload_date": "20221111",
    "uploader": "Labellerr",
    "tags": [
      "machine learning",
      "annotation preview",
      "data preprocessing",
      "annotation software",
      "annotation methods",
      "data labeling software",
      "computer vision",
      "image annotation",
      "annotation workflow",
      "visual data",
      "image segmentation",
      "image classification",
      "image labeling",
      "image metadata",
      "object detection",
      "feature extraction",
      "deep learning",
      "dataset creation",
      "data labeling tools",
      "data pipeline",
      "feature mapping",
      "data annotation",
      "visual recognition",
      "ai",
      "big data",
      "image processing"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=Z5YdaJxHhbo! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "4g6PqmzLuuc",
    "title": "Labellerr Feature Demo 3: Autolabel",
    "description": "Labellerr auto-label feature helps annotators annotate files(image/document) in just one click.\nSaves their time and makes them more productive.\nLabellerr has been known as a training data platform with a heavy focus on human-in-the-loop optimization and automating computer vision data pipelines.\n\nKnow more about Labellerr: https://www.labellerr.com",
    "video_url": "https://www.youtube.com/watch?v=4g6PqmzLuuc",
    "embed_url": "https://www.youtube.com/embed/4g6PqmzLuuc",
    "duration": 30,
    "view_count": 75,
    "upload_date": "20221110",
    "uploader": "Labellerr",
    "tags": [
      "labeling software",
      "image annotation",
      "machine learning",
      "annotation software",
      "feedback loops",
      "data preprocessing",
      "visual data",
      "data markup",
      "data labeling automation",
      "deep learning",
      "computer vision",
      "image labeling",
      "image classification",
      "data labeling",
      "image segmentation",
      "image markup",
      "labeling solutions",
      "segmentation software",
      "image detection",
      "data visualization",
      "image processing",
      "data labeling tools",
      "deep learning models",
      "image sorting",
      "ml datasets",
      "ai",
      "data",
      "ml"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=4g6PqmzLuuc! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "XAgIEFXDAcw",
    "title": "Labellerr feture demo 2: Quick Review",
    "description": "Reviewing a classification project has been always a daunting task for teams.\n\nTill now most of the team deploy multiple reviewers to review 100,000+ data projects.\n\nReviewers have no option other than manually look into files to see if tags are correctly labeled.\n\nThey just randomly select files and open them one by one and see the labels.\n\nIf each image has to be tagged on 100+ labels, it would take hours for all the reviewers on daily basis, and still, errors can get unnoticed.\n\nTo solve that, we have launched a new feature for reviewers.\n\nNow reviewers can see in a quick view to check the pattern and anomaly of tagged labeled by annotators and each parameter and that too in seconds.\n\nIt will benefit them by-\n1. Reducing the daily tasks that took hrs to a few minutes.\n2. Much better accuracy can be ensured\n3. Remove the dependency on allocating multiple reviewers on a daily basis\n4. Speed up the whole data generation process.\n\nSee a quick demo below-\n\nHappy annotating!!!\n\n#projectmanagement #qualityassurance  #highquality #data #trainingdata  #costreduction #computervision #dataannotation",
    "video_url": "https://www.youtube.com/watch?v=XAgIEFXDAcw",
    "embed_url": "https://www.youtube.com/embed/XAgIEFXDAcw",
    "duration": 37,
    "view_count": 29,
    "upload_date": "20221104",
    "uploader": "Labellerr",
    "tags": [
      "machine learning",
      "tech review",
      "deep learning",
      "data science",
      "computer vision",
      "ai training",
      "data visualization",
      "big data analytics",
      "data labeling",
      "data preprocessing",
      "computer vision projects",
      "data analysis",
      "machine learning projects",
      "data labeling tools",
      "image labeling",
      "data management",
      "deep learning projects",
      "annotation software",
      "ml projects",
      "image annotation",
      "ai tools",
      "dataset creation",
      "image processing",
      "data annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=XAgIEFXDAcw! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "KwX69aT1RJ0",
    "title": "Labellerr Feature demo 1: Search filters for files",
    "description": "Big annotation project management is a hassle we all know that and @Labellerr  is built to solve that.\n\nFor an annotation project of 100,000 images/videos finding the right image after an annotation is not easy. Labellerr's admin panel provides several filter criteria to search the file admin needs when they need it.\n\nHere admin can search files based on their\n1. Name\n2. ID\n3. Status\n4. Last activity\n5. User's name and much more.\n\nAdmin can see the activity history in quick view with JUST ONE CLICK.\n\nSee the quick demo below-\n\nManaging annotation projects has never been easier.\n\n#trainingdata #dataannotation #projectmanagement #qualityassurance #computervision",
    "video_url": "https://www.youtube.com/watch?v=KwX69aT1RJ0",
    "embed_url": "https://www.youtube.com/embed/KwX69aT1RJ0",
    "duration": 44,
    "view_count": 33,
    "upload_date": "20221102",
    "uploader": "Labellerr",
    "tags": [
      "file management",
      "productivity tools",
      "file sorting",
      "project management",
      "digital efficiency",
      "file organization",
      "time management",
      "task management",
      "data annotation admin",
      "time blocking",
      "data management",
      "annotation workflow",
      "workflow management",
      "annotation search filters",
      "productivity apps",
      "data workflow",
      "data pipeline",
      "digital organization",
      "project tracking",
      "data organization",
      "annotation strategies",
      "data efficiency",
      "file sorter",
      "annotation methods",
      "file organization tips"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=KwX69aT1RJ0! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "3gc4MhHx_6E",
    "title": "Smartest Training Data Platform: Labellerr",
    "description": "Introducing a whole new approach to creating and managing training data for ML model training.\nComputer vision is an advanced technology which has disrupted the world of AI and automation. It has applications in every industry and its use cases are endless. The biggest challenge for enterprises is to manage their training data pipeline because that's the most crucial and critical part of machine vision AI development.\nLabellerr solves this problem for ML experts by securely connecting a large volume of raw images/video and converting them into high-quality training data sets. Our system automates the overall process and AI leaders can easily monitor the whole process with utmost ease.",
    "video_url": "https://www.youtube.com/watch?v=3gc4MhHx_6E",
    "embed_url": "https://www.youtube.com/embed/3gc4MhHx_6E",
    "duration": 47,
    "view_count": 24,
    "upload_date": "20220801",
    "uploader": "Labellerr",
    "tags": [
      "data annotation",
      "machine learning",
      "annotation software",
      "deep learning",
      "data preprocessing",
      "dataset creation",
      "image annotation",
      "computer vision",
      "annotation platforms",
      "computer vision projects",
      "dataset generation",
      "image segmentation",
      "labeling software",
      "annotation workflow",
      "data labeling tools",
      "image classification",
      "data labeling",
      "visual recognition",
      "image labeling",
      "image labeling tools",
      "image metadata",
      "data management",
      "data labeling software",
      "image annotation tool",
      "ai"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] foreign [Music] foreign",
    "transcript_chunks": [
      "[Music] foreign [Music] foreign"
    ],
    "transcript_word_count": 4,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "lt0AakQ3AI4",
    "title": "Labellerr Podcast: Insights from Soma Dhavala on Responsible AI | Best Data Labeling Tools",
    "description": "Soma Dhavala, Principal Researcher at Wadhwani AI, shares his inspiring journey into data science and his work on building responsible AI for social good. In this insightful discussion, discover how Labellerr, one of the best data labeling tools, supports businesses in scaling their AI-ML operations by streamlining data preparation, annotation, model training, and deployment. \n\nEmpower your machine learning initiatives with cutting-edge solutions.\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\n#BestDataLabelingTools #AI #DataAnnotation #MachineLearning #MLOps",
    "video_url": "https://www.youtube.com/watch?v=lt0AakQ3AI4",
    "embed_url": "https://www.youtube.com/embed/lt0AakQ3AI4",
    "duration": 1304,
    "view_count": 168,
    "upload_date": "20211112",
    "uploader": "Labellerr",
    "tags": [
      "data labeling",
      "data labeling tools",
      "image labeling",
      "data labeling service",
      "data labeling platform",
      "data labeling tool",
      "best data labeling software",
      "data labeling companies",
      "data annotation",
      "data preprocessing",
      "annotation software",
      "quality assurance",
      "dataset creation",
      "data labeling software",
      "data validation",
      "labeling software",
      "image annotation",
      "data entry",
      "image segmentation",
      "labeling solutions",
      "image classification",
      "manual annotation",
      "automated image labeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=lt0AakQ3AI4! This is most likely caused by:\n\nNo transcripts were found for any of the requested language codes: ('en',)\n\nFor this video (lt0AakQ3AI4) transcripts are available in the following languages:\n\n(MANUALLY CREATED)\nNone\n\n(GENERATED)\n - hi (\"Hindi (auto-generated)\")\n\n(TRANSLATION LANGUAGES)\nNone\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "QDKRWTC1l2I",
    "title": "Labellerr Podcast: AI Innovation and Challenges ft. VirooPax Mirji, VP AI at WinWire",
    "description": "Explore an insightful conversation with VirooPax Mirji, VP of AI & Data Solutions at WinWire Technologies, as he delves into the challenges of building AI solutions for the healthcare industry. VirooPax shares his experiences managing data science teams and utilizing advanced tools to create bespoke AI-driven solutions for social good.\n\nThis podcast is organized by Labellerr, a leading B2B SaaS platform empowering enterprises to scale their AI/ML operations. Labellerr supports businesses at every stage of their AI journey, from data preparation and annotation to model building, training, deployment, and comprehensive MLOps management.\n\nJoin us to gain valuable insights into overcoming real-world AI challenges and fostering innovation in healthcare and beyond.  \nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=QDKRWTC1l2I",
    "embed_url": "https://www.youtube.com/embed/QDKRWTC1l2I",
    "duration": 1338,
    "view_count": 50,
    "upload_date": "20211020",
    "uploader": "Labellerr",
    "tags": [
      "machine assisted annotation",
      "image annotation services",
      "data categorization service",
      "data",
      "data annotation jobs",
      "annotation workflow",
      "video labeling",
      "dataset creation",
      "labeling software",
      "image labeling",
      "data annotation",
      "data annotation tech",
      "how to make money with data annotation jobs",
      "data annotation tutorial",
      "best data annotation services",
      "best data annotation companies",
      "data annotation in autonomous driving",
      "what is data annotation?",
      "dataannotation.tech"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "so hi everyone this is sumit from labeller we are sas platform helping enterprises scaling up their ai ml operation we help them at their very stage of their ai development cycle like data preparation uh data annotation model building model training deployment and overall seminars so uh today we are going to talk with deru fox uh and video process vp of ai at wind wire so welcome beerup hi guys how are you so yeah i mean uh directly jumping into the you know discussion i just wanted to ask you that what bin why i do and what is your role sure so venuer is almost a 14 plus year company focused on cloud oriented solutions uh cloud microsoft cloud solutions and any kind of cloud solution which uses emerging technologies we typically focus on healthcare retail and manufacturing more on healthcare than anything else we have recently won a lot of awards you know from microsoft recognition from microsoft as well as from the rest of the industry for best in business uh innovative applications using ai using uh data you know modern data platforms so we have a number of those accomplishments to show but essentially we are a very purpose driven innovation purpose-driven company where we focus on innovative solutions which will help our customers gain competitive advantage and you know be the best at serving their customers right at the end of the you know story we just look at the whole picture and try to help them out with a really innovative solutions and your role uh generally involves around ai development so how's that mostly ai but as you know right ai came from data stream or data oriented work so data and ai are both very closely linked together and i have a number of years of experience in the industry with data from the past with automation and ai more recently you can say in the last five to six years i've spent more time with ai but the whole concept of having expert systems has been there for many many many years right people have used expert systems for solving problems all over the world and now we just call them ai right because these are self learning or machine learning oriented systems they're supervised unsupervised systems there's a reinforced learning but all of that is to help humans do a better job and which means we need to use ai in a responsible way to ensure that you know the community is served correctly right so i say that you mentioned somewhere that you are working with vaccination and also with some uh other you know useless which is very interesting so would you like to explain more how you are using ai to solve those kind of problems so before i answer that question fully let me just start with uh defining the boundary of ai for me right machine learning is a small part of overall ai and you know we are very very far away from artificial general intelligence agi as it is called we're very far away from achieving that kind of intelligence which is closer to human intelligence right now we are focused on narrow ai narrow ai meaning solving problems which are specific to industries specific to you know customers who have that problem so just to give you example you talked about vaccination since kovit happened we realized that it would be great to have some innovations in that space to help people uh get their vaccinations right and when you're giving vaccination what's the role of ai and that right if you think about it narrow ai approach will be hey i need to make sure that every one of us whether it is sumit or pune or vero or others we all should get vaccination at the right time as efficiently as possible without any hang ups or wait waiting time and you know with all the regular doses and everything right so intelligence needs to be applied both from an automation angle in the whole last mile of service and delivery and also intelligence ai can be applied in terms of demand forecasting demand planning in terms of making sure enough vaccine vaccines are available for the masses right over the different age groups and using technologies like nlp chat bots to make the experience more smooth and easy for people so that they can sign up they can get their vaccinations and then they can come back again for follow-ups and you know any kind of advice that they need from the medical facilities so for the providers for the clinics for any kind of hospitals and senior citizen community centers we kind of think that vaccination is an important aspect and for that we have attempted to use kind of an ai methodology behind it to drive the whole flow right from end to end from the time the vaccine arrives to storage monitoring the temperature of that vaccine making sure the vaccine is provided to the uh you know in the right lots to people and that they're getting the right set of vaccines at the right time and allowing them to sign up and use chatbot kind of technologies to help them uh you know have a good experience as they go through this stressful procedure so you have worked on several use cases in several industries so what is the biggest pain point of challenge when you you know from starting to end when you try to build something uh with the help of machine learning and all those new technologies yeah so uh there are many challenges to talk about frankly you know because uh as i've mentioned before people are still in the experimental phase of learning how to use ai you know there are many many people are not even aware about how to use it now when you ask about what are the real life challenges here we can think about it in three ways okay the first part is data oriented because many of the data scientists today are spending close to 70 to 80 percent of their time just in preparing the data just in ingesting the data just in improving the quality of data just in making sure that the data is not garbage in because then you'll get garbage out right off the ai system also the recommendations the predictions everything will be bad if the training data is not sufficient or not good enough so that's the one area which is all about data so second area is about uh automation you know because as we see uh you need to commercialize you need to deploy it into production and you yourself has a have a great sas service for labeling and labellerr is the company doing that you must have realized that you know doing these models creating the solutions are one thing but actually deploying it into production requires mlaps right requires something more than devops right uh being able to use um mlop techniques whether it is feature stores or whether it's using any kind of you know tracking versioning control which will allow you to do that so that's a very important part which is ml ops which is the second area where people are struggling a little bit because you not everybody automates it right smoothly so working with igr tools or working with any cloud tools we have figured out our own way and we also work very closely with databricks databricks has ml flow they have a feature store they have many such kind of you know products or solutions you can call which reduces the workload from an envelopes ml ops perspective the third part i will focus more is on the human part of ai human centric ai you know what i mean by that is we are entering a phase of green ai frankly you know what i mean by green air is you've heard of green revolution right everybody is trying to use electric cars and everybody's trying to think of sustainability and all that even in ai the maturity of ai if you look at the overall curve we are still in the early stages and we are we are kind of working with green field ai type of activities we are not yet integrated into fully into our lives right as humans so we have to keep a responsible ai kind of mentality in mind that how do you use ai in the right way in a fair way in a way that is equal and you know applicable to all people in a very uniform and a right manner right you can call it so responsible ai is a big initiative and i'm actually associated with stanford's human center of design right human center ai they call it where you keep the human in the middle right you do not forget that we are humans and we are building solutions to augment and help humans achieve the goal so it's not it's not just super intelligence just to be left alone right like that it has to be fair and equal and applicable to everybody so those are the three areas i would say important areas to consider and their challenges because the last one is a little bit more difficult than the others to satisfy in every country there are different policies in every country there are different laws there are different ways of treating humans and you know humanity in general so being fair to all of them is difficult being reliable you know providing a reliable ai solution which applies equally to everybody is difficult there's a lot of biases built into the system so you must have heard of model drift right or drift that happens due to data right not being up to date now imagine if that bias is built in then what kind of drift will it will cause in the human application of ai right and so automatic biases come in and responsible ai becomes very critical to put some procedures in place to make sure that we as data scientists we as ai engineers we think about the holistic impact to the world and to humanity and make sure responsible ai is good right right so means and also means uh you have mentioned three points and four point i would like to ask how you managing the people your team your developers your engineers and your data science team to collaborate and make a process uh to you know keep doing this uh a successful uh process again and again so how's that challenging yeah so see we are a very automation-driven company we do think about process streamlined processes automated processes and people are an important part of that overall equation people process and technology right and we obviously can learn all about technology we want and apply it but in the end it's the way you apply it the process that you use is it automated is it batch is it manual you know those things need to be considered from a people angle you know the world has a huge shortage of data engineers of data analysts data scientists there's a huge i mean everybody is you know impacted by that even if you're a google you're searching for talent if even if you're a smaller size company even if you're a mid-sized company wind wire right we all of us are impacted by lack of talent but people who are there can be trained in this area can they can learn so i always encourage a learning environment with some mentoring and coach you know kind of behavior from senior experienced people who can help other people and maintaining a community of that you know a community of friends of people who think alike who want to help each other and collaborate to solve these problems so we when we go solve a problem for any type of customer or any kind of you know industry situation we don't look at it just as me or i or you right we look at us working together collaboratively cooperating with each other and helping each other out so there's no one person who's going to have all the knowledge in the world i cannot claim to be an ai expert okay i'm still learning in the field what i can tell you is that we need to learn from each other so that we don't waste time you know making the same mistakes like others right so if if labellerr is doing something great then you know we should learn from you as well as to how you've automated it and how you've made life easier for your people the data scientists are obviously struggling because they're dependent on data engineers and data engineers are struggling because they don't fully understand how ai engineers or data scientists will use the data right so they prepare the data but they don't know they can't talk about the quality of the data that well and so we have to help each other out and collaborate a lot have cross-functional teams who can support each other have the right project management structure the automation strategy and then have the right architecture in mind so that you can use the right tools for the job to achieve the goals of the people make people's life easier right that's really difficult especially in healthcare which is a very regulated market in in the u.s we have fda you know policies which are into place so it's very difficult to apply any ai technique or any kind of new model without getting approvals from the government and from the federal agencies right so we have to keep that in mind and make sure that we you know think accordingly train people accordingly plan prepare accordingly and build the right architecture so that it's a zero trust architecture and ensures that there's no hacking or other things happening along that line right right so you mentioning about the tool uh there's a missed challenge with the tools you need to write set up to you know uh make your team collaborate with each other and so that they can understand that uh the responsibility of each and every people who are involved in overall you know project development so what is the biggest gap or urgent gap if you can say that if this would be provided in the market and in a very efficient manner so what is that one gap that you would like to mention cap from a tool perspective you mean yes software perspective uh technology perspective tool perspective okay it's a good question i mean there are many tools out there and one of the problems with having too many tools is that there is no standard now there's no gold standard right that everybody can follow and because you have tensorflows also you have aws sagemaker also you have azure ml studio also you have databricks also you have so many tools in those systems that people frequently get confused on the right approach and the right tool to use for the job so i think my answer will be brief on this account which is to say that where it is lacking is in the interoperability between the tools you have multiple tools but they need to work hand in glove with each other you know a good example is what is happening in the microsoft world where microsoft is working with gpd3 now gpd3 is an open ai initiative for uh generative you know uh transformer models right so if you think about it um to enable that you need huge amounts of data and do we have the right tools for collecting all the data in the world and bringing it together those tools are there but it's not sufficient right if you think about the application of ai and then you think about the end points that are necessary to be published so that people can consume it again you need to think about how does it all integrate together and work together so we have had experiences with some customers who love uh you know github and use azure devops they use all these tools for deploying the solution uh but there are some new companies coming out now git labs which just went public today in fact this morning in the u.s they went public git labs is becoming more and more popular as it's able to work in a more lightweight fashion along with a whole bunch of multi-cloud and hybrid cloud situations right when we are deploying this solution so there are many tools and databricks is also adding a load of tools query performance related tools feature store tools ml flow uh sql related you know mechanisms and snowflake has been adding a whole bunch of stuff bigquery has been adding everybody is adding their tools the problem is with too many tools you need more time to educate yourself right to learn how to use the tool effectively and apply it so that is a definite challenge that we see in the world that you know people have to keep learning you cannot stop learning you have to keep learning and trying out different tools and no tool is going to be bug free right you will end up with some kind of issue or problem and then you'll have to switch to some other tool and that's the reason it's important not to be very locked into one approach keep an open mind and work with a bunch of different tools and see what makes sense for your uh need right and purpose and then evolve your architecture accordingly later right right so to conclude our discussion today ms i would like to ask how you see the ai adoption in your industry our own overall industry going forward five years five years ahead okay that's a long time and you know just like we say internet internet time in ai time also it's far away but i can tell you that one thing is clear that people are trying to use the ai technologies for the right purpose which is to help humans perform better right and that that is a narrow ai which is applicable whether you're in a call center in a contact center whether you're in a hospital you know giving care to your patients or whether you're sitting in a retail store and trying to you know serve drinks or food or other things to people right or sell products everywhere you go whether you're watching a movie a recommendation engine whether you go sitting in a car you need driverless technology or any kind of ai technology which will keep learning so in the next five years if i had to give you a quick short answer i would say the focus would be more in making sure that these are reliable and responsible ai systems that perform as per spec designed well for all the masses to use right without any bias in that system there are many things we can do beyond that we can really go into ai which is you know like elon musk has been talking about the android uh the robot that he's been creating you know just something that can help humans live better so there are many such advances happening the brain computer interface right the brain neural interface neural link those all these concepts are coming in the next five years they'll become even more useful and applicable but we need to do it with a responsible ai in mind you know so that we don't end up hurting people in the process or hurting humanity in the process and we use it in the right way so over the next five years there'll be a lot of advances you know flying cars you can call it you'll have driverless stack full sub server drive right driving technology flying to the moon flying to mars all those things will happen but in the end we are humans and we are prone to error and we need to make sure that you know we need to pay extra attention with technologies like ai to make sure that we are doing the right thing for the right audiences right so in my own industry with healthcare we care a lot about people's experience right patient experience we care about the nurses experience the doctor's experience you know there a lot of doctors and nurses are really fighting day in and day out against curved and they're getting burnt out right so what can we do for them what can we do in the next three to five years so that another covet does not happen so that we can help be ready for that next big you know pandemic that comes across right so i think the ai will be applied in much more useful ways as we go along and i'm a very optimistic and positive person so i feel uh we should not be scared and we should not worry about our jobs being lost to ai we should instead think about how to use this technology in the responsible way so that everybody gets benefits from that right right so thank you very much you will share so many stories and you know give us lots of insights so thank you hopefully we will talk again uh with you know uh with some more advances in here thank you it my pleasure thank you thanks for taking with me",
    "transcript_chunks": [
      "so hi everyone this is sumit from labeller we are sas platform helping enterprises scaling up their ai ml operation we help them at their very stage of their ai development cycle like data preparation uh data annotation model building model training deployment and overall seminars so uh today we are going to talk with deru fox uh and video process vp of ai at wind wire so welcome beerup hi guys how are you so yeah i mean uh directly jumping into the you know discussion i just wanted to ask you that what bin why i do and what is your role sure so venuer is almost a 14 plus year company focused on cloud oriented solutions uh cloud microsoft cloud solutions and any kind of cloud solution which uses emerging technologies we typically focus on healthcare retail and manufacturing more on healthcare than anything else we have recently won a lot of awards you know from microsoft recognition from microsoft as well as from the rest of the industry for best in business uh innovative applications using ai using uh data you know modern data platforms so we have a number of those accomplishments to show but essentially we are a very purpose driven innovation purpose-driven company where we focus on innovative solutions which will help our customers gain competitive advantage and you know be the best at serving their customers right at the end of the you know story we just look at the whole picture and try to help them out with a really innovative solutions and your role uh generally involves around ai development so how's that mostly ai but as you know right ai came from data stream or data oriented work so data and ai are both very closely linked together and",
      "i have a number of years of experience in the industry with data from the past with automation and ai more recently you can say in the last five to six years i've spent more time with ai but the whole concept of having expert systems has been there for many many many years right people have used expert systems for solving problems all over the world and now we just call them ai right because these are self learning or machine learning oriented systems they're supervised unsupervised systems there's a reinforced learning but all of that is to help humans do a better job and which means we need to use ai in a responsible way to ensure that you know the community is served correctly right so i say that you mentioned somewhere that you are working with vaccination and also with some uh other you know useless which is very interesting so would you like to explain more how you are using ai to solve those kind of problems so before i answer that question fully let me just start with uh defining the boundary of ai for me right machine learning is a small part of overall ai and you know we are very very far away from artificial general intelligence agi as it is called we're very far away from achieving that kind of intelligence which is closer to human intelligence right now we are focused on narrow ai narrow ai meaning solving problems which are specific to industries specific to you know customers who have that problem so just to give you example you talked about vaccination since kovit happened we realized that it would be great to have some innovations in that space to help people uh get their vaccinations right and when",
      "you're giving vaccination what's the role of ai and that right if you think about it narrow ai approach will be hey i need to make sure that every one of us whether it is sumit or pune or vero or others we all should get vaccination at the right time as efficiently as possible without any hang ups or wait waiting time and you know with all the regular doses and everything right so intelligence needs to be applied both from an automation angle in the whole last mile of service and delivery and also intelligence ai can be applied in terms of demand forecasting demand planning in terms of making sure enough vaccine vaccines are available for the masses right over the different age groups and using technologies like nlp chat bots to make the experience more smooth and easy for people so that they can sign up they can get their vaccinations and then they can come back again for follow-ups and you know any kind of advice that they need from the medical facilities so for the providers for the clinics for any kind of hospitals and senior citizen community centers we kind of think that vaccination is an important aspect and for that we have attempted to use kind of an ai methodology behind it to drive the whole flow right from end to end from the time the vaccine arrives to storage monitoring the temperature of that vaccine making sure the vaccine is provided to the uh you know in the right lots to people and that they're getting the right set of vaccines at the right time and allowing them to sign up and use chatbot kind of technologies to help them uh you know have a good experience as they go through",
      "this stressful procedure so you have worked on several use cases in several industries so what is the biggest pain point of challenge when you you know from starting to end when you try to build something uh with the help of machine learning and all those new technologies yeah so uh there are many challenges to talk about frankly you know because uh as i've mentioned before people are still in the experimental phase of learning how to use ai you know there are many many people are not even aware about how to use it now when you ask about what are the real life challenges here we can think about it in three ways okay the first part is data oriented because many of the data scientists today are spending close to 70 to 80 percent of their time just in preparing the data just in ingesting the data just in improving the quality of data just in making sure that the data is not garbage in because then you'll get garbage out right off the ai system also the recommendations the predictions everything will be bad if the training data is not sufficient or not good enough so that's the one area which is all about data so second area is about uh automation you know because as we see uh you need to commercialize you need to deploy it into production and you yourself has a have a great sas service for labeling and labellerr is the company doing that you must have realized that you know doing these models creating the solutions are one thing but actually deploying it into production requires mlaps right requires something more than devops right uh being able to use um mlop techniques whether it is feature stores or",
      "whether it's using any kind of you know tracking versioning control which will allow you to do that so that's a very important part which is ml ops which is the second area where people are struggling a little bit because you not everybody automates it right smoothly so working with igr tools or working with any cloud tools we have figured out our own way and we also work very closely with databricks databricks has ml flow they have a feature store they have many such kind of you know products or solutions you can call which reduces the workload from an envelopes ml ops perspective the third part i will focus more is on the human part of ai human centric ai you know what i mean by that is we are entering a phase of green ai frankly you know what i mean by green air is you've heard of green revolution right everybody is trying to use electric cars and everybody's trying to think of sustainability and all that even in ai the maturity of ai if you look at the overall curve we are still in the early stages and we are we are kind of working with green field ai type of activities we are not yet integrated into fully into our lives right as humans so we have to keep a responsible ai kind of mentality in mind that how do you use ai in the right way in a fair way in a way that is equal and you know applicable to all people in a very uniform and a right manner right you can call it so responsible ai is a big initiative and i'm actually associated with stanford's human center of design right human center ai they call it where",
      "you keep the human in the middle right you do not forget that we are humans and we are building solutions to augment and help humans achieve the goal so it's not it's not just super intelligence just to be left alone right like that it has to be fair and equal and applicable to everybody so those are the three areas i would say important areas to consider and their challenges because the last one is a little bit more difficult than the others to satisfy in every country there are different policies in every country there are different laws there are different ways of treating humans and you know humanity in general so being fair to all of them is difficult being reliable you know providing a reliable ai solution which applies equally to everybody is difficult there's a lot of biases built into the system so you must have heard of model drift right or drift that happens due to data right not being up to date now imagine if that bias is built in then what kind of drift will it will cause in the human application of ai right and so automatic biases come in and responsible ai becomes very critical to put some procedures in place to make sure that we as data scientists we as ai engineers we think about the holistic impact to the world and to humanity and make sure responsible ai is good right right so means and also means uh you have mentioned three points and four point i would like to ask how you managing the people your team your developers your engineers and your data science team to collaborate and make a process uh to you know keep doing this uh a successful uh process again and",
      "again so how's that challenging yeah so see we are a very automation-driven company we do think about process streamlined processes automated processes and people are an important part of that overall equation people process and technology right and we obviously can learn all about technology we want and apply it but in the end it's the way you apply it the process that you use is it automated is it batch is it manual you know those things need to be considered from a people angle you know the world has a huge shortage of data engineers of data analysts data scientists there's a huge i mean everybody is you know impacted by that even if you're a google you're searching for talent if even if you're a smaller size company even if you're a mid-sized company wind wire right we all of us are impacted by lack of talent but people who are there can be trained in this area can they can learn so i always encourage a learning environment with some mentoring and coach you know kind of behavior from senior experienced people who can help other people and maintaining a community of that you know a community of friends of people who think alike who want to help each other and collaborate to solve these problems so we when we go solve a problem for any type of customer or any kind of you know industry situation we don't look at it just as me or i or you right we look at us working together collaboratively cooperating with each other and helping each other out so there's no one person who's going to have all the knowledge in the world i cannot claim to be an ai expert okay i'm still learning in the",
      "field what i can tell you is that we need to learn from each other so that we don't waste time you know making the same mistakes like others right so if if labellerr is doing something great then you know we should learn from you as well as to how you've automated it and how you've made life easier for your people the data scientists are obviously struggling because they're dependent on data engineers and data engineers are struggling because they don't fully understand how ai engineers or data scientists will use the data right so they prepare the data but they don't know they can't talk about the quality of the data that well and so we have to help each other out and collaborate a lot have cross-functional teams who can support each other have the right project management structure the automation strategy and then have the right architecture in mind so that you can use the right tools for the job to achieve the goals of the people make people's life easier right that's really difficult especially in healthcare which is a very regulated market in in the u.s we have fda you know policies which are into place so it's very difficult to apply any ai technique or any kind of new model without getting approvals from the government and from the federal agencies right so we have to keep that in mind and make sure that we you know think accordingly train people accordingly plan prepare accordingly and build the right architecture so that it's a zero trust architecture and ensures that there's no hacking or other things happening along that line right right so you mentioning about the tool uh there's a missed challenge with the tools you need to write set up",
      "to you know uh make your team collaborate with each other and so that they can understand that uh the responsibility of each and every people who are involved in overall you know project development so what is the biggest gap or urgent gap if you can say that if this would be provided in the market and in a very efficient manner so what is that one gap that you would like to mention cap from a tool perspective you mean yes software perspective uh technology perspective tool perspective okay it's a good question i mean there are many tools out there and one of the problems with having too many tools is that there is no standard now there's no gold standard right that everybody can follow and because you have tensorflows also you have aws sagemaker also you have azure ml studio also you have databricks also you have so many tools in those systems that people frequently get confused on the right approach and the right tool to use for the job so i think my answer will be brief on this account which is to say that where it is lacking is in the interoperability between the tools you have multiple tools but they need to work hand in glove with each other you know a good example is what is happening in the microsoft world where microsoft is working with gpd3 now gpd3 is an open ai initiative for uh generative you know uh transformer models right so if you think about it um to enable that you need huge amounts of data and do we have the right tools for collecting all the data in the world and bringing it together those tools are there but it's not sufficient right if you think",
      "about the application of ai and then you think about the end points that are necessary to be published so that people can consume it again you need to think about how does it all integrate together and work together so we have had experiences with some customers who love uh you know github and use azure devops they use all these tools for deploying the solution uh but there are some new companies coming out now git labs which just went public today in fact this morning in the u.s they went public git labs is becoming more and more popular as it's able to work in a more lightweight fashion along with a whole bunch of multi-cloud and hybrid cloud situations right when we are deploying this solution so there are many tools and databricks is also adding a load of tools query performance related tools feature store tools ml flow uh sql related you know mechanisms and snowflake has been adding a whole bunch of stuff bigquery has been adding everybody is adding their tools the problem is with too many tools you need more time to educate yourself right to learn how to use the tool effectively and apply it so that is a definite challenge that we see in the world that you know people have to keep learning you cannot stop learning you have to keep learning and trying out different tools and no tool is going to be bug free right you will end up with some kind of issue or problem and then you'll have to switch to some other tool and that's the reason it's important not to be very locked into one approach keep an open mind and work with a bunch of different tools and see what makes sense",
      "for your uh need right and purpose and then evolve your architecture accordingly later right right so to conclude our discussion today ms i would like to ask how you see the ai adoption in your industry our own overall industry going forward five years five years ahead okay that's a long time and you know just like we say internet internet time in ai time also it's far away but i can tell you that one thing is clear that people are trying to use the ai technologies for the right purpose which is to help humans perform better right and that that is a narrow ai which is applicable whether you're in a call center in a contact center whether you're in a hospital you know giving care to your patients or whether you're sitting in a retail store and trying to you know serve drinks or food or other things to people right or sell products everywhere you go whether you're watching a movie a recommendation engine whether you go sitting in a car you need driverless technology or any kind of ai technology which will keep learning so in the next five years if i had to give you a quick short answer i would say the focus would be more in making sure that these are reliable and responsible ai systems that perform as per spec designed well for all the masses to use right without any bias in that system there are many things we can do beyond that we can really go into ai which is you know like elon musk has been talking about the android uh the robot that he's been creating you know just something that can help humans live better so there are many such advances happening the brain",
      "computer interface right the brain neural interface neural link those all these concepts are coming in the next five years they'll become even more useful and applicable but we need to do it with a responsible ai in mind you know so that we don't end up hurting people in the process or hurting humanity in the process and we use it in the right way so over the next five years there'll be a lot of advances you know flying cars you can call it you'll have driverless stack full sub server drive right driving technology flying to the moon flying to mars all those things will happen but in the end we are humans and we are prone to error and we need to make sure that you know we need to pay extra attention with technologies like ai to make sure that we are doing the right thing for the right audiences right so in my own industry with healthcare we care a lot about people's experience right patient experience we care about the nurses experience the doctor's experience you know there a lot of doctors and nurses are really fighting day in and day out against curved and they're getting burnt out right so what can we do for them what can we do in the next three to five years so that another covet does not happen so that we can help be ready for that next big you know pandemic that comes across right so i think the ai will be applied in much more useful ways as we go along and i'm a very optimistic and positive person so i feel uh we should not be scared and we should not worry about our jobs being lost to ai we should instead think",
      "about how to use this technology in the responsible way so that everybody gets benefits from that right right so thank you very much you will share so many stories and you know give us lots of insights so thank you hopefully we will talk again uh with you know uh with some more advances in here thank you it my pleasure thank you thanks for taking with me"
    ],
    "transcript_word_count": 3669,
    "transcript_chunk_count": 13
  },
  {
    "video_id": "X1_CnBc8Ri8",
    "title": "Labellerr Podcast: AI & Geospatial Innovation Insights with Jamie Conklin, VP Product at Astraea",
    "description": "Watch Jamie Conklin, VP Product at Astraea, as he delves into the challenges and opportunities of building AI-powered geospatial solutions. Gain valuable insights from his experience managing data science workflows to create next-generation geospatial mapping and data provisioning services.\n\nThis engaging talk is brought to you by Labellerr, a leading B2B SaaS platform empowering enterprises to scale their AI-ML operations. Labellerr supports businesses throughout their AI journey, including data preparation, annotation, model building, training, deployment, and seamless MLOps integration.\n\nLearn how cutting-edge tools and expert strategies are revolutionizing AI development.\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\n#DataScience #MachineLearning #AI #MLOps #DataAnnotation",
    "video_url": "https://www.youtube.com/watch?v=X1_CnBc8Ri8",
    "embed_url": "https://www.youtube.com/embed/X1_CnBc8Ri8",
    "duration": 1425,
    "view_count": 47,
    "upload_date": "20211013",
    "uploader": "Labellerr",
    "tags": [
      "machine learning",
      "data labeling",
      "deep learning",
      "interactive machine learning",
      "labeling",
      "azure machine learning",
      "data labeling tools",
      "annotate images for machine learning",
      "labeling tools",
      "image labeling",
      "data labelling",
      "data labeling service",
      "learning",
      "programmatic labeling",
      "data labeling impact",
      "active learning",
      "ai data labeling",
      "dataset labeling",
      "video data labeling",
      "data annotation",
      "image annotation",
      "visual recognition",
      "data preprocessing",
      "natural language processing"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "from labellerr we are sas based tool uh to help enterprises uh scaling up their ai ml operation we help them at with their various stages of the ai journey like data preparation annotation model training model building deployment and overall mlaps now we are in discussion with jamie jamie's vp of product at austria ask so in in most of the cases uh in different sectors means people using ai as a tool of automation or some process improvement so how ai or machine learning help in your case with your product what's the uh value the your client are getting from when you put ai in this uh all this imagery yeah i think that's a that's an interesting question and it's a it's a philosophically um we take a slightly different approach um our our experience is that most companies don't intrinsically care about ai or and certainly not about satellite imagery what they need are answers to important questions and um so the the challenge with this is often that um these answers aren't aggregated together in a uniform way or that the they require a lot of work to extract them out of images or out of multiple sources of data and so our goal is to you know start with our clients and work with them to adjust the key questions that they have and then we look to find a straightforward solution to that problem and answer it as quickly as possible and then over time we can use ai to automate those processes in in a way that saves them money and saves them time uh the the time from the customer's perspective is typically about how fast we can get the answer out after the you know data are collected and so um from our perspective ai reduces the cost of executing uh the answer to a question and it increases the response time or excuse me decreases the response time it increases the value to the customer by by being faster right right so in complete ai dev cycle as a product guy how you come to the picture with the engineering team data science team and you know uh who initiate you know a product enhancement and how it will work the what is the workflow in your organization yeah that that's a um that's a really excellent question um and one thing i can say is that um you know i've been in the business of building uh machine learning and ai tools for a long time and you know it's interesting as you work with software developers um different software developers understand this at different levels and um you know i'm lucky at australia we're a small team but that the software development team really understands and gets what ai is and how data scientists work and so one thing that's kind of funny is software developers often have a mentality that you want to make the tools easy to use for the the least common denominator uh when it comes to you know their user base and what's interesting about data scientists is they have an extraordinarily high tolerance for pain and what i mean by that is that they're not used to having tools that work seamlessly and they're used to having to clutch things together to to get their initial model to work because they're always breaking new ground and so you know working with data scientists is actually very different than than most other software tools because these users have a lot of abilities that that most users don't have when it comes to moving data around or munging data or things like that but they also have a deep appreciation when you make something that they usually spend a lot of time fussing with easier um so um i think that what makes it interesting at australia is as i said our lead software engineer has the deep understanding of of ai and so we're building tools like clipping tools um that make it much easier for a data scientist to just come in and say i just want this little area out of this image out of the stack of images and it just processed a hundred images that it's you know at a clip get those clippings and then use those in his ml workflow another example of uh some that that we've focused on are things like uh chipping algorithms um if you want to build object detection tools you want to chip around uh build chips of images around the objects that you want to detect and um so we've built a shipping tools that support a whole bunch of different strategies uh depending on on what kind of data you're looking at and so it's what i think is kind of unique and or not necessarily unique but but certainly more rare is is finding software development teams that sort of intrinsically understand the domain that their data scientists users are working in um and so that that's a foundation that we're building on um and that i think is really valuable but to answer your question about flow and process um what we're doing is we we work with our data science team and um they often lead uh the the the demand side of what we're trying to do so we'll reach out and we'll have a customer come in and they'll need to understand the impact of a conservation effort for example and our data science team will will examine that and try to figure out what data we need to do that and then they'll say hey jamie we we need data like this or in this organization or we need the ability to cook these data and then we'll work with a software engineering team to devise a way to do that and and we prioritize that work in conjunction with you know the prioritization of our of our software platform um so um you know in in process ways it actually creates more complexity for me because i've got both the backend tools that we're building for our data scientists going at the same time as building our platform itself and so um you know the trick is is finding the the sweet spot of the things that support both of those things simultaneously and pushing those forward first right right so i mean in most of the team that i talk uh the data science team they say that at the prototyping at the experimentation level things are pretty easy and uh they don't face so much challenges but when they're going to production then they are going scaling up then there's a whole lot of you know different ballgame so how you uh you know tackle those kind of challenges as a product guy how you you know kind of orchestrate your your overall team um yeah you know it's it's it's kind of funny um i came into australia after it had been in existence for a couple years and um the the team that was there prior to me always had a very large scale perspective on the work they were doing um and so they were always building tools and capabilities that that could scale to a global level um as an example of one of those things we have an open source package called raster frames and raster frames is designed to make imagery data a first class object in spark and spark is a distributed compute paradigm um and so the idea was make it easier to process imagery at scale using spark uh and so that was the the mentality of the the company when i when i arrived and that ethos has remained um but what's interesting is actually you know in many cases we're actually finding ways to scale down um you know because we have to scale down to solve small problems first at a local level and then scale up to solve them more globally and and actually that's been kind of a funny challenge in a way is when we start with this big scale stuff um it's actually harder for for companies and organizations to start at that level they typically want to start small and do pilot projects and some of the stuff that doesn't scale down as easily as as it does scale up for us um but yeah the the main um but acknowledging that these data are big and heavy and and building out an infrastructure from the very beginning to handle that has been sort of the the ethos of the company um so everything we do is backed by kubernetes and uh so we can easily uh scale up our clusters uh and we can do horizontal scaling very fast um and you know we we set out to be cloud native so everything we've done leverages um aws or other cloud uh technologies uh we're actually cloud agnostic because we do use kubernetes but um we sell it out to leverage those um those tools uh from the get go another example is our ability to render images using serverless compute and you know so using spark to do distributed compute is one paradigm of uh computing large scale data and serverless compute is sort of the opposite end of that um and so we've we've started using serverless compute wherever we can because it's so fast and so affordable it makes it easier for us to you know deliver applications quickly so the example is you know these clipping services that i mentioned those are available through you know using amazon lambdas on the back end and so that's how we can process those images very quickly and we expose that as a service and our data science team just hits that service and boom they get the data the way they want it um and so we're working on building more and more of those kinds of services so that data scientists who use our platform feel that joy right you mentioned that you have a great uh software development team and also the data science team but still uh there's even the best of the team they face issue with the current tool available in the market to you know develop the ai so what's exactly their pinpoint uh that you face in in your ai development cycle then you feel that there is somebody kind of you stuck at some point of time that uh this tool is not working properly and they don't talk to each other there's so many open source so many you know modules are developing at different platform so how you can you know uh collaborate all those stuff and stitch them together yeah um it's an interesting question um the you know it it's a simple thing but it's also a really complex problem um the the variety of data that we need to be able to process actually poses a pretty interesting challenge of uh and and just overall quality of data that you very commonly find um you know when you order data from a data provider like a satellite imagery company um you know they they tend to deliver that data in a standardized way with the metadata that you you can account for so if you work with many satellite providers you you have to build a workflow for each one of them to ingest each of their data but then if you go to an organization that's been collecting data for a long period of time uh using drones and iphones and satellites and you know anything else um the data comes in just a tremendous variety and um you know almost never do we find the uh date and time of an image properly uh annotated in the metadata of the geotiff um and so uh you know using that in ai actually becomes really challenging because if you don't know when the image was taken or you know it's a huge question mark it makes it hard to search for it and catalog it and things like that so that's actually it's a huge challenge um for us is finding ways to render arbitrary data and when i say render arbitrary data you know if you just send me a geotiff even if the coordinates are in it and even if you have the date and time the next question is what are the bands and where are the bands placed um you know sometimes uh you get images where the bands are in red green blue order but other times they do it in frequency order so you'll get blue green red near infrared and if you have hyperspectra for multi-spectral data where are the definitions for all of that stuff and how do you keep all that aligned and then keeping track of of that you know it just it's complex um and so that's been a challenge uh you know actually probably one of the most frustrating challenges our team has dealt with is building an infrastructure that that helps uh our our customers uh bring in data existing data um and like i said you know when we work with our providers we know what we're getting and we know how to deal with it but when we work with um existing data and archive data it gets really tricky um so that that's been a real challenge for us yeah means uh you are talking about that you have put in lots of data so in what format these data generally come from it's an image format like png or dot uh you know jpeg or something like that and collecting such a large volume i i hope a monthly basis you will be collecting all those things and then you know uh feed it to your ai model how this overall operation works out to feeding lots of lots of data a continuous manner yeah um so the the core of of our our platform is our is our catalog um and uh so earth on demand's catalog uh is a stack based catalog which is the spatial temporal asset catalog which is a standard um that's been emerging in the geospatial field and so the first step in everything we've been doing is to catalog all the data that we use and then we've built workflows uh using some of the um workflow engines and kubernetes and these two these workflows allow us to automatically run an analytic and send it through a pipeline and it allows the sort of conditional um you know conditional modality so you know you can run it through and if the imagery is there and if the imagery isn't covered with clouds then you can execute the model and then you can run the model and then you can send that a labeling team for validation and they can validate that model and if um you know if they reject it then you can you know you know use them to create the data by hand or something like that so there's a lot of those flows of working the data through and then combining uh automation with human intelligence uh you know tasks and human intelligence teams um you know the sort of categorically across the industry another big challenge that's faced is the lack of label training data and then i'm guessing that labellerr is well aware of that challenge but you know being able to leverage labeling teams and make them really really efficient is part of the solution to this problem and so um you know we're building workflows to pipe each stage of the process to the next stage and sometimes that next stage is human stage and so that's been sort of one of the the models we've used to you know accelerate our development as well as uh you know build these production workflows and at the end of all of these um we publish data back to our collection to collections in our catalog but we also uh expose apis um and so our customers can actually access data via api or we can push it to them through some sort of notification or or they can or we can build an app for them if they need so we sort of just depending on who our customer is and what their needs and their their uh their internal abilities are we can provide the data in any number of ways uh to support them some of our customers actually get like a pdaf report because that's what they they need right right since yeah you cover all of those uh men's process and you explain it all so it was really insightful uh the challenges that you are overcoming it's a process i'm thinking it's it's a best way that you can manage it so uh last thing that i asked everyone a question is that how you see in your space in next coming five years ai adoption would be like well um you know i was uh i was looking at my slack feed this morning and one of the uh articles was a listing of how many satellites are on orbit today and it's like 7 000 some or something like that um that number is going to go through the roof um you know the number the accessibility of space is going is making the amount of data we can collect uh just you know continue to grow exponentially it's been growing exponentially for years but it's continuing to be exponential growth and um i i forget i think someone told me the other day that there was a statistic that it would take something like seven million people to process the imagery collected by satellites in one day right you know so obviously that's not going to work right like we're just not going to apply that level of work um to to the data that's coming down so the only solution is to automatically process these things um and to to use them to improve uh the ai to become more and more reliable and to continue to answer these questions so i think i think there is huge investment obviously you know globally in this and that is going to lead to more data more data is going to demand more ai to to make use of it but it's also going to create a very competitive market and the price of imagery and the price of data are is going to come down you know just just looking at satellite imagery um you know when i started working at australia just two years ago um a tasked high resolution imagery usually started over three to four hundred dollars but easily was up into the three thousand to ten thousand dollar range if you needed something really really high resolution like a thirty or forty centimeter image and you need it soon you could easily spend three to ten thousand dollars on a single image today uh we can task an image in a few days time for 200 bucks and that price i mean that just changes the gain because now it's cheaper to take an image than it is to send someone out to a site to see what's going on in addition to that the the frequency of coverage is going much much higher um you know you can get an image of any spot on the earth multiple times a day at this point um and it used to be that there was only one uh provider that that had daily coverage of the whole globe but but that's actually changing it there's there's other providers who can capture an image anywhere on the globe multiple times a day and um you know that that's also changing the game another interesting development is you know we're seeing creative platforms uh there are companies that are launching uh balloons and from these balloons they're capturing imagery data and other other data as well some of them are so high up that they can actually um you know maintain persistent surveillance um over huge areas um you know uh like regions of the us um and so um just because of their vantage point they can get a two-minute revisit anywhere in the state of california for hours on end or days on end um and and so you know that the level of um surveillance is growing and that that changes how um you know what we can use these things for so now it's much more practical to use balloon imagery to detect the ignition of forest fires which is a big problem here in the u.s because we can look over large swaths of an area on a regular basis and our goes our geospatial satellites do that as well so now we have multiple platforms collecting large areas all the time um i think that's i think the persistence is going to grow and that you know we're going to see things more and more like video in space almost right right so what are the according to you the most exciting use case that can be solved if we get to the level where the images are very cheap that you we cannot do right now but in future you see that those are the most exciting use cases um my tilt on this is is always um ecological perspective um and i think that better understanding our earth and the impacts that we have on the earth and how we can improve that and and ensure that we're improving it is perhaps the most important thing we can do with these data um so if we can fuel the um esg movement in finance if we can provide tools to help people reforest after forest fires and uh to measure carbon output and to detect um you know ecological hazards um i think that that remote sensing is a key part of preserving and saving our earth and it's hard to it's hard to understate that and it's hard to be um think of something more exciting in my book um it's not uh you know it's not a sexy new app or a simple little thing but i do think that that ai is is a key part of preserving and protecting our our environment and uh that to me is it's really exciting nice and see you putting uh your time and talk to us and really share your story so far it's been really insightful thank you thank you for your time thank you mother welcome yeah you're welcome",
    "transcript_chunks": [
      "from labellerr we are sas based tool uh to help enterprises uh scaling up their ai ml operation we help them at with their various stages of the ai journey like data preparation annotation model training model building deployment and overall mlaps now we are in discussion with jamie jamie's vp of product at austria ask so in in most of the cases uh in different sectors means people using ai as a tool of automation or some process improvement so how ai or machine learning help in your case with your product what's the uh value the your client are getting from when you put ai in this uh all this imagery yeah i think that's a that's an interesting question and it's a it's a philosophically um we take a slightly different approach um our our experience is that most companies don't intrinsically care about ai or and certainly not about satellite imagery what they need are answers to important questions and um so the the challenge with this is often that um these answers aren't aggregated together in a uniform way or that the they require a lot of work to extract them out of images or out of multiple sources of data and so our goal is to you know start with our clients and work with them to adjust the key questions that they have and then we look to find a straightforward solution to that problem and answer it as quickly as possible and then over time we can use ai to automate those processes in in a way that saves them money and saves them time uh the the time from the customer's perspective is typically about how fast we can get the answer out after the you know data are collected and",
      "so um from our perspective ai reduces the cost of executing uh the answer to a question and it increases the response time or excuse me decreases the response time it increases the value to the customer by by being faster right right so in complete ai dev cycle as a product guy how you come to the picture with the engineering team data science team and you know uh who initiate you know a product enhancement and how it will work the what is the workflow in your organization yeah that that's a um that's a really excellent question um and one thing i can say is that um you know i've been in the business of building uh machine learning and ai tools for a long time and you know it's interesting as you work with software developers um different software developers understand this at different levels and um you know i'm lucky at australia we're a small team but that the software development team really understands and gets what ai is and how data scientists work and so one thing that's kind of funny is software developers often have a mentality that you want to make the tools easy to use for the the least common denominator uh when it comes to you know their user base and what's interesting about data scientists is they have an extraordinarily high tolerance for pain and what i mean by that is that they're not used to having tools that work seamlessly and they're used to having to clutch things together to to get their initial model to work because they're always breaking new ground and so you know working with data scientists is actually very different than than most other software tools because these users have a lot of",
      "abilities that that most users don't have when it comes to moving data around or munging data or things like that but they also have a deep appreciation when you make something that they usually spend a lot of time fussing with easier um so um i think that what makes it interesting at australia is as i said our lead software engineer has the deep understanding of of ai and so we're building tools like clipping tools um that make it much easier for a data scientist to just come in and say i just want this little area out of this image out of the stack of images and it just processed a hundred images that it's you know at a clip get those clippings and then use those in his ml workflow another example of uh some that that we've focused on are things like uh chipping algorithms um if you want to build object detection tools you want to chip around uh build chips of images around the objects that you want to detect and um so we've built a shipping tools that support a whole bunch of different strategies uh depending on on what kind of data you're looking at and so it's what i think is kind of unique and or not necessarily unique but but certainly more rare is is finding software development teams that sort of intrinsically understand the domain that their data scientists users are working in um and so that that's a foundation that we're building on um and that i think is really valuable but to answer your question about flow and process um what we're doing is we we work with our data science team and um they often lead uh the the the demand side of what",
      "we're trying to do so we'll reach out and we'll have a customer come in and they'll need to understand the impact of a conservation effort for example and our data science team will will examine that and try to figure out what data we need to do that and then they'll say hey jamie we we need data like this or in this organization or we need the ability to cook these data and then we'll work with a software engineering team to devise a way to do that and and we prioritize that work in conjunction with you know the prioritization of our of our software platform um so um you know in in process ways it actually creates more complexity for me because i've got both the backend tools that we're building for our data scientists going at the same time as building our platform itself and so um you know the trick is is finding the the sweet spot of the things that support both of those things simultaneously and pushing those forward first right right so i mean in most of the team that i talk uh the data science team they say that at the prototyping at the experimentation level things are pretty easy and uh they don't face so much challenges but when they're going to production then they are going scaling up then there's a whole lot of you know different ballgame so how you uh you know tackle those kind of challenges as a product guy how you you know kind of orchestrate your your overall team um yeah you know it's it's it's kind of funny um i came into australia after it had been in existence for a couple years and um the the team that was there prior",
      "to me always had a very large scale perspective on the work they were doing um and so they were always building tools and capabilities that that could scale to a global level um as an example of one of those things we have an open source package called raster frames and raster frames is designed to make imagery data a first class object in spark and spark is a distributed compute paradigm um and so the idea was make it easier to process imagery at scale using spark uh and so that was the the mentality of the the company when i when i arrived and that ethos has remained um but what's interesting is actually you know in many cases we're actually finding ways to scale down um you know because we have to scale down to solve small problems first at a local level and then scale up to solve them more globally and and actually that's been kind of a funny challenge in a way is when we start with this big scale stuff um it's actually harder for for companies and organizations to start at that level they typically want to start small and do pilot projects and some of the stuff that doesn't scale down as easily as as it does scale up for us um but yeah the the main um but acknowledging that these data are big and heavy and and building out an infrastructure from the very beginning to handle that has been sort of the the ethos of the company um so everything we do is backed by kubernetes and uh so we can easily uh scale up our clusters uh and we can do horizontal scaling very fast um and you know we we set out to be cloud",
      "native so everything we've done leverages um aws or other cloud uh technologies uh we're actually cloud agnostic because we do use kubernetes but um we sell it out to leverage those um those tools uh from the get go another example is our ability to render images using serverless compute and you know so using spark to do distributed compute is one paradigm of uh computing large scale data and serverless compute is sort of the opposite end of that um and so we've we've started using serverless compute wherever we can because it's so fast and so affordable it makes it easier for us to you know deliver applications quickly so the example is you know these clipping services that i mentioned those are available through you know using amazon lambdas on the back end and so that's how we can process those images very quickly and we expose that as a service and our data science team just hits that service and boom they get the data the way they want it um and so we're working on building more and more of those kinds of services so that data scientists who use our platform feel that joy right you mentioned that you have a great uh software development team and also the data science team but still uh there's even the best of the team they face issue with the current tool available in the market to you know develop the ai so what's exactly their pinpoint uh that you face in in your ai development cycle then you feel that there is somebody kind of you stuck at some point of time that uh this tool is not working properly and they don't talk to each other there's so many open source so many you",
      "know modules are developing at different platform so how you can you know uh collaborate all those stuff and stitch them together yeah um it's an interesting question um the you know it it's a simple thing but it's also a really complex problem um the the variety of data that we need to be able to process actually poses a pretty interesting challenge of uh and and just overall quality of data that you very commonly find um you know when you order data from a data provider like a satellite imagery company um you know they they tend to deliver that data in a standardized way with the metadata that you you can account for so if you work with many satellite providers you you have to build a workflow for each one of them to ingest each of their data but then if you go to an organization that's been collecting data for a long period of time uh using drones and iphones and satellites and you know anything else um the data comes in just a tremendous variety and um you know almost never do we find the uh date and time of an image properly uh annotated in the metadata of the geotiff um and so uh you know using that in ai actually becomes really challenging because if you don't know when the image was taken or you know it's a huge question mark it makes it hard to search for it and catalog it and things like that so that's actually it's a huge challenge um for us is finding ways to render arbitrary data and when i say render arbitrary data you know if you just send me a geotiff even if the coordinates are in it and even if you have",
      "the date and time the next question is what are the bands and where are the bands placed um you know sometimes uh you get images where the bands are in red green blue order but other times they do it in frequency order so you'll get blue green red near infrared and if you have hyperspectra for multi-spectral data where are the definitions for all of that stuff and how do you keep all that aligned and then keeping track of of that you know it just it's complex um and so that's been a challenge uh you know actually probably one of the most frustrating challenges our team has dealt with is building an infrastructure that that helps uh our our customers uh bring in data existing data um and like i said you know when we work with our providers we know what we're getting and we know how to deal with it but when we work with um existing data and archive data it gets really tricky um so that that's been a real challenge for us yeah means uh you are talking about that you have put in lots of data so in what format these data generally come from it's an image format like png or dot uh you know jpeg or something like that and collecting such a large volume i i hope a monthly basis you will be collecting all those things and then you know uh feed it to your ai model how this overall operation works out to feeding lots of lots of data a continuous manner yeah um so the the core of of our our platform is our is our catalog um and uh so earth on demand's catalog uh is a stack based catalog which is the",
      "spatial temporal asset catalog which is a standard um that's been emerging in the geospatial field and so the first step in everything we've been doing is to catalog all the data that we use and then we've built workflows uh using some of the um workflow engines and kubernetes and these two these workflows allow us to automatically run an analytic and send it through a pipeline and it allows the sort of conditional um you know conditional modality so you know you can run it through and if the imagery is there and if the imagery isn't covered with clouds then you can execute the model and then you can run the model and then you can send that a labeling team for validation and they can validate that model and if um you know if they reject it then you can you know you know use them to create the data by hand or something like that so there's a lot of those flows of working the data through and then combining uh automation with human intelligence uh you know tasks and human intelligence teams um you know the sort of categorically across the industry another big challenge that's faced is the lack of label training data and then i'm guessing that labellerr is well aware of that challenge but you know being able to leverage labeling teams and make them really really efficient is part of the solution to this problem and so um you know we're building workflows to pipe each stage of the process to the next stage and sometimes that next stage is human stage and so that's been sort of one of the the models we've used to you know accelerate our development as well as uh you know build these production",
      "workflows and at the end of all of these um we publish data back to our collection to collections in our catalog but we also uh expose apis um and so our customers can actually access data via api or we can push it to them through some sort of notification or or they can or we can build an app for them if they need so we sort of just depending on who our customer is and what their needs and their their uh their internal abilities are we can provide the data in any number of ways uh to support them some of our customers actually get like a pdaf report because that's what they they need right right since yeah you cover all of those uh men's process and you explain it all so it was really insightful uh the challenges that you are overcoming it's a process i'm thinking it's it's a best way that you can manage it so uh last thing that i asked everyone a question is that how you see in your space in next coming five years ai adoption would be like well um you know i was uh i was looking at my slack feed this morning and one of the uh articles was a listing of how many satellites are on orbit today and it's like 7 000 some or something like that um that number is going to go through the roof um you know the number the accessibility of space is going is making the amount of data we can collect uh just you know continue to grow exponentially it's been growing exponentially for years but it's continuing to be exponential growth and um i i forget i think someone told me the other day that there",
      "was a statistic that it would take something like seven million people to process the imagery collected by satellites in one day right you know so obviously that's not going to work right like we're just not going to apply that level of work um to to the data that's coming down so the only solution is to automatically process these things um and to to use them to improve uh the ai to become more and more reliable and to continue to answer these questions so i think i think there is huge investment obviously you know globally in this and that is going to lead to more data more data is going to demand more ai to to make use of it but it's also going to create a very competitive market and the price of imagery and the price of data are is going to come down you know just just looking at satellite imagery um you know when i started working at australia just two years ago um a tasked high resolution imagery usually started over three to four hundred dollars but easily was up into the three thousand to ten thousand dollar range if you needed something really really high resolution like a thirty or forty centimeter image and you need it soon you could easily spend three to ten thousand dollars on a single image today uh we can task an image in a few days time for 200 bucks and that price i mean that just changes the gain because now it's cheaper to take an image than it is to send someone out to a site to see what's going on in addition to that the the frequency of coverage is going much much higher um you know you can get an",
      "image of any spot on the earth multiple times a day at this point um and it used to be that there was only one uh provider that that had daily coverage of the whole globe but but that's actually changing it there's there's other providers who can capture an image anywhere on the globe multiple times a day and um you know that that's also changing the game another interesting development is you know we're seeing creative platforms uh there are companies that are launching uh balloons and from these balloons they're capturing imagery data and other other data as well some of them are so high up that they can actually um you know maintain persistent surveillance um over huge areas um you know uh like regions of the us um and so um just because of their vantage point they can get a two-minute revisit anywhere in the state of california for hours on end or days on end um and and so you know that the level of um surveillance is growing and that that changes how um you know what we can use these things for so now it's much more practical to use balloon imagery to detect the ignition of forest fires which is a big problem here in the u.s because we can look over large swaths of an area on a regular basis and our goes our geospatial satellites do that as well so now we have multiple platforms collecting large areas all the time um i think that's i think the persistence is going to grow and that you know we're going to see things more and more like video in space almost right right so what are the according to you the most exciting use case that can be solved",
      "if we get to the level where the images are very cheap that you we cannot do right now but in future you see that those are the most exciting use cases um my tilt on this is is always um ecological perspective um and i think that better understanding our earth and the impacts that we have on the earth and how we can improve that and and ensure that we're improving it is perhaps the most important thing we can do with these data um so if we can fuel the um esg movement in finance if we can provide tools to help people reforest after forest fires and uh to measure carbon output and to detect um you know ecological hazards um i think that that remote sensing is a key part of preserving and saving our earth and it's hard to it's hard to understate that and it's hard to be um think of something more exciting in my book um it's not uh you know it's not a sexy new app or a simple little thing but i do think that that ai is is a key part of preserving and protecting our our environment and uh that to me is it's really exciting nice and see you putting uh your time and talk to us and really share your story so far it's been really insightful thank you thank you for your time thank you mother welcome yeah you're welcome"
    ],
    "transcript_word_count": 3846,
    "transcript_chunk_count": 13
  },
  {
    "video_id": "VoUTrEm4efI",
    "title": "Labellerr Podcast: Insights from AI Expert Leonard Apeltsin, Director of Data Science at Anomaly",
    "description": "Join Leonard Apeltsin, Director of Data Science at Anomaly, as he delves into the challenges of developing AI for faulty medical claim recognition. In this insightful discussion, Leonard shares his journey of managing a data science team, navigating the complexities of healthcare systems, and overcoming obstacles in AI-driven solutions.\n\nThis engaging talk is brought to you by Labellerr, a B2B SaaS platform empowering enterprises to scale their AI and ML operations. From data preparation and annotation to model building, training, deployment, and MLOps, Labellerr supports every stage of your AI journey with precision and efficiency.\n\nDiscover how Labellerr drives innovation and success in the AI ecosystem.\nLearn more at www.labellerr.com\nBook a demo: www.labellerr.com/book-a-demo\n\n#DataAnnotation #DataScience #MachineLearning #AI #MLOps",
    "video_url": "https://www.youtube.com/watch?v=VoUTrEm4efI",
    "embed_url": "https://www.youtube.com/embed/VoUTrEm4efI",
    "duration": 1653,
    "view_count": 86,
    "upload_date": "20211012",
    "uploader": "Labellerr",
    "tags": [
      "best data annotation services",
      "data categorization service",
      "data labeling service",
      "good image annotation service",
      "annotation",
      "video annotation",
      "reliable image annotation service",
      "remote sensing",
      "video labeling",
      "data labeling",
      "quality assurance",
      "data preprocessing",
      "annotation workflow",
      "data annotation",
      "image annotation services",
      "data annotation services",
      "benefits of data annotation services",
      "image annotation service",
      "lidar annotation",
      "data annotation service",
      "computer vision"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hi everyone this is sumit from leveler.com uh we are sas platform helping enterprises scaling up their ai ml operation to the production we help them eat at very stage of their ai journey like data preparation annotation model building model training deployment and overall mlaps today we are going to discuss uh some of the challenges in healthcare sector with leonard epstein uh he's a data scientist director at anomaly so hi leonard hi hi good morning i'm pretty good i'm pretty good yeah it's great to be here yeah thank you so much uh getting directly into the discussion we just wanted to understand that uh what exactly is anomaly and what is your daily day-to-day life at the world okay so uh anomaly is a small but rapidly growing uh startup uh that uh attempts to uh fix uh inappropriate healthcare billing using uh machine learning and data science so unfortunately in the united states health care billing is broken it's estimated that two to ten percent of all health care claims have some sort of inappropriate billing patterns on them which is pretty ridiculous when you think about it imagine if your credit card statement came back in ten percent of the time there was an error in it and the errors arise uh for multitude of reasons but mostly because healthcare billing is incredibly uh complex and so in order to resolve the data complexity uh we uh apply data science and machine learning technique techniques to identify the inappropriate billing signals and deal with them appropriately so to implement ai and data science in those kind of quality payments and all those documents how you implement it and what is the process what kind of data that you need and how you go about it so uh anomaly is uh dealing with hundreds of millions of uh healthcare claims about two to ten percent of which uh uh have uh some sort of broken signals uh present so it's a highly balanced data set the one we can manage but something we can work around uh what makes the proper the problem difficult is uh the need for some sort of a domain expertise to comprehend the data because if you want me to find anomalies in that healthcare data i will find anomalies that won't necessarily be indicative of an appropriate building like the other day i was looking through a claim when which a patient was injected uh once with a single dose of a drug uh paid out 2.1 million dollars perfectly legitimate it's the most expensive drug on the market so uh in order to uh differentiate between signals that are unusual and signals that are indicative of broken billing we need domain experts involved and kind of leveraging uh the knowledge of domain experts to uh to label and stratify uh the data is a challenge that we're dealing with appropriately but i think it's definitely a challenge that extends beyond healthcare to other aspects of machine learning where an expert is required to understand the nuances of the data so that the data can be uh can be labeled or categorized appropriately right so you are basically talking about the human and the loops and in your case there should be a domain expert not a normal human who identify the object simple object it's not that simple so means uh it sounds like you have to uh label lots of data and that too with the expensive resources so how you overcome those well uh so uh we've there there there's uh there there is this uh research uh movement pioneered at stanford around uh a weekly uh supervised learning in which one leverages the knowledge of the domain experts to uh to come up with uh thousands of heuristics and simple uh statistics and so forth defining let's say in our case you know what makes uh uh you know defining the various ways in which a claim can be broken and then uh applying uh kind of advanced ideas from weekly supervised learning where we kind of use bayesian techniques to decorate the various heuristics we can label uh the data try to label the data more intelligently of course uh even with application of weekly supervised learning uh some you know human in the loop interface and technology is uh is necessary for those domain experts to then come in and make sure that the predictions particularly the predictions centered around broken claims with inappropriate billing signals that those predictions are are meaningful and uh and and accurate and i can provide feedback in those distances where the predictions are incorrect and that also requires some sort of uh uh interpretability uh component which you know we're built we're not trying to reinvent the wheel here but uh a lot of these tools that we're building out you know where we're building custom to our uh particular use case right right so who uh means uh it sounds like your customer uh like insurance company or hospital uh and they need to you know check this uh you know all those faulty claims and all those kind of stuff so miss uh i think uh accuracy is a key concern yeah so it's very important that we have a high precision because when a customer say when a customer is an insurance company uh obviously we don't want the insurance company uh then going back hassling a a doctor who's uh billing appropriately i do want to mention that some of our customers are also the doctors uh themselves uh it's an important point because if you imagine a surgeon bills for sixty thousand dollar surgery and because of one little nuanced uh error that bill gets delayed by six months well that's that's a sixty thousand dollar payment that's getting delayed by uh six months so this is consider of a concern to all parties but yes uh in the healthcare space uh uh precision is critical this isn't like optimizing uh uh online ads uh the the room for false positives is very low which is why we're doubling down and we're tripling down on precision right right so means in in uh experiment mode and when you are you know testing mode then uh the challenge are different but when you scale up and go to the productions what are those challenges and how you overcome those oh so uh again we we we as i mentioned uh we have uh a lot of data you know hundreds of millions of claims um it's it's it's a manageable data set we're fortunate in that regard but it's still um it's still big data and uh the scaling trainable models particularly when it comes to negative examples um examples of uh claims that have been uh built appropriate but are still in some ways similar to uh to the broken claims scaling uh the uh the negative examples is uh is is difficult um again part of a part of the reason uh resolves around kind of the you know like the the difficulty of the modeling there are there are areas where you know neural networks are appropriate there are areas where combination of neural networks and other techniques are required and yes training uh training these models at scale is definitely you know an issue that my team is you know uh works continually to resolve and improve but there are uh currently no real out of the box solutions to uh to the problem in our particular niche right so means uh in your case uh you are you getting all the data which is real or you also you know incorporate some of the synthetic data because in your case you can create some synthetic scenario of document so yeah this is one advantage yeah this is where again it gets uh uh gets a little tricky so synthetic uh data generation is uh an invaluable tool for certain areas uh of uh ml it's important to understand uh when uh the tool can be can be leveraged for instance in a pure document analysis or imaging analysis you know you you average to uh two similar images together and you get a new type of image that's also legitimately in that class there's a kind of a continuous uh spectrum there in like the let's say that the imaging space uh with uh healthcare claims it's more nuanced because uh the claims are heterogeneous there's lots of uh they're more tabular and heterogeneous there's lots of signal there and for instance when you take two claims that are not broken and you average them together you're probably going to get something that that is broken and if you take two claims that are broken you average them together you're going to get a third claim that's broken in a completely completely different uh way so it's important to understand the uh the geometry of uh one's data and whether or not it is continuous and can be averaged appropriately uh in order uh to be able to kind of uh understand when uh when uh when six synthetic data is useful for one's use case and when it isn't right so means uh how big is your data science team and what concerns data scientists software developer how many uh team members that yeah so yeah so again right now the uh uh the the ml data science team uh consists of uh of uh four people and i just wanna say we're actively uh higher hiring for for more from uh to to add additional uh uh experienced uh motivated individuals to our team uh other some members of the team of think our title does have the title of data data scientists other members have a title of applied machine learning engineer but again at a you know kind of a in a small startup it's very much a uh where many hats sort of environment like for from our standpoint uh you know whether a team team members refer to as a data scientist or a machine learning engineer uh correlated more towards their kind of uh that the types of projects and challenges that they would like to take on and their preferred interests rather than kind of a you know any uh any any particular sense of uh uh sense of title and again i'd be happy to jump into those uh uh those nuances if you like yeah right in your opinion mr you you mentioned that you have a very small team but in your opinion if there's electronic plus people and means how you you know communicate your with your team that this is the responsibility of data scientists what is the responsibility of ml engineer what is the responsibility of software engineer and to provide them overall pipeline so that they can work in collaboration uh with open communication because most of the leaders mentioned this as a very big challenge for that so uh so uh again i like to use a bit of a strange uh uh strange metaphor here but one that you know i think is uh kind of appropriate to kind of uh differentiating uh useful for differentiating those roles so again i'm a big history nerd here and i'm particularly interested in uh uh the napoleonic wars and in the napoleonic wars uh kind of uh the three uh kind of a uh the the three the the the three main ways of of uh holding a warfare and organizing the battle was a kind of uh the the heavy artillery the infantry and uh the calvary now heavier artillery represented what was possible by state-of-the-art technology and it can be used to completely change the direction of battle via technological know-how and uh kind of uh brute force however it was also uh a heavy machinery that was difficult to drag into battle and uh and uh and set up and from my standpoint this is where kind of a machine learning uh infrastructure and machine learning techniques uh exist uh uh today they represent state-of-the-art technology without which certain organizations cannot flourish and cannot conquer in the business space nonetheless uh there's still there there it's it represents difficult technology difficult infrastructure to kind of set up maintain understand requires a lot of mathematical know-how just like the artillery back in napoleon's time required the knowledge of physics to kind of uh fire up appropriately but still it is uh not enough cur you know in the current business landscape to really uh alter the direction of battle in and of itself do you want to quickly mention that by world war one artillery gotten powerful enough to completely determine the course of battle but we're not there yet now the same time you have uh uh the cavalry which is just individuals and horses charging into battle the same way that they had for hundreds of years already uh it was uh an outdated technique seemingly outdated technique that still worked was crucial to the direction of the battle and there was uh it was all about you know speed and medium mobility and understanding and know-how and in some not in some ways this is how uh uh i think of the data science team now obviously this is where the metaphor breaks down because you know data scientists are expected to understand the nuances of the you know machine learning techniques and apply then where appropriately but the kind of the the the most important aspect of their role is to be light and quick and to leverage the appropriate technique at hand and if the correct answer to a particular problem is let's say uh simple statistics if all it takes to really get the result uh for a particular use case that gains value to the client is uh you know quick t-test uh well even though that technique has been around for a hundred years that's the appropriate solution and that is the quick uh solution and so that quickness and and speed arising from both the you know like analytical expertise and understanding of the data is where uh you know data scientists at least uh uh at least on certain teams are really poised uh to uh uh kind of uh to contribute and then of course you have uh uh you have uh you know the the infantry that basically uh does the standard things that need to get done that to kind of keep the battle going in this case uh we have uh you know the the soft the the the software engineers that uh just make sure the infrastructure is running make sure that we have the apis going that can get the the data back and forth uh to the clients and then uh i do also want to say that there's this nuanced niche between software engineer and data scientist that i think of as the product engineer particularly when you're building an analytics product uh having the customer admin you know in mind having the use case in mind and figuring out how to deliver uh the analytic solution appropriately to uh to the customer it takes engineering know-how but it also takes kind of this data science analytics know-how and i do see a bunch of that or data scientists the individuals that started their career data science that transitioned more into this hybrid uh engineering role where they built out uh their analytics into a customer-facing product and i do think it's important for organizations to or also recognize this uh this niche of uh of of product engineer which again is becoming kind of a you know more and more crucial role in certain teams as the teams grow out right interesting metaphor that you use so in that metaphor means in your opinion or in your experience means do data science team work internally only in house team or you always need some uh you know outside help as well like consultant audience and labeling or other other activity yes it it uh it depends uh on uh on the use case uh and again uh so right now i'm building out anomaly but prior to anomaly i was in the founding team of a startup called primer and i kind of led uh that applied machine learning to the natural language processing space and again i uh i mean we're small i uh led uh led the efforts to build out uh primers uh kind of a foundational uh uh prediction and analytics uh stack so help grow that team from four people to uh 100 people and the way i see it especially when the company's small to medium-sized outside of outside help when appropriate uh you know should uh uh should be it should be leveraged when you're a startup you do whatever it needs you you need to get done to be fast and nimble and uh satisfy uh satisfy the clients so uh and you don't want to find yourselves to reinventing uh the the wheels so again that anomaly uh you know we we we have in certain instances at least uh uh spoke to connections in other organizations to see if they could be uh you know uh provide uh provide value to fill the the you know certain certain gaps that you know we later went on to fill ourselves as we grew uh you know uh grew out the team uh now again uh and so for small to medium size companies i think it's important to be aware of all your options now as a company gets larger i think they're there there are more uh kind of uh trade-offs uh discussions to be made having said that i do think that uh kind of uh an organization should never be uh reinventing the wheel but at the same time also being aware of like you know the consequences of let's say uh asking outside helper depending on outside technology let's say uh three years down the line it's important to ask that as well what will happen as the organization grows what are the consequences of this uh decision so you know there's there's nuances at play but one one let's keep an open uh mind and always understand what decision is best for the company right so yeah i mean uh technological technological immensely what do you feel that there's some uh where you feel the gap means you are you are giving all the tools all the you know technology available in the market to your you know data science team so that they can perform their job that's too you know waste their time in unnecessary works and do their specific job so do you think any frustration any any incident that you frustrated you that you don't have any any tool in the market or miss those kind of incidents you want to share yeah well it's more it's it's it's more about uh let's say uh certain gaps in the space again outside of uh you know uh more intelligent labeling better a better human in the loop uh tools are highly desired particularly uh let's say in the area of uh uh of active learning using these tools to understand uh where the gaps uh uh in in in your data line by sampling intelligent you know intelligently uh uh back in the past i've you know uh play played around with a number of tools which again we're still works in progress maybe they've improved at this point but i haven't seen a good uh good active learning uh active learning solution that's where the you know your model is basically uh trying to uh uh make sure that the the the predictions it's confident in are truly uh deserving of that confidence but at the same time appropriately you know labeling the you know uncertain signals uh uh along the decision boundary and uh and so forth uh good interpretability tools i mean we're doing a lot of work in terms internally on model interpretability and i have to say in health care in particular uh models cannot be black box models uh for for for a multitude of reasons but you know one one of which is to make sure that because of the complexity we internally have to understand how the models making the predictions more importantly to gain trust uh with uh kind of the experts in the healthcare space it's important to be able to point out and show how the models are are you know are predicting so there's been a lot of fascinating work in the space but not as many uh uh tools uh for uh uh for interpretability of kind of commercial tools as uh as one would uh like uh i do think and he that there's uh there's also uh in terms of both you know tooling and you know and algorithms there's kind of this uh interesting uh you know dichotomy in the space uh if you look at all the you know recent amazing advancements that you've seen in the machine learning field uh a lot of them obviously have bit in the deep learning space and it's again deep learning is very much relevant to what we do uh if you uh but if you look at the algorithms uh leveraged to win uh uh the top kaggle competitions they're usually you know version of uh you know tree based models a version of you boost uh you know cat boost so part of that discrepancy is that uh you know deep learning works well on kind of more homogeneous data entry-based models work well on heterogeneous data with a variety of features uh there hasn't really been that much of a push uh tool-wise for a hybrid approach so you can imagine a data set where some of the most significant features are homogeneous such as images and text but there's other uh you know kind of a tabular information you see this a lot in healthcare space where maybe uh the clinical notes out written by the doctor outlining the patient's health that's all text that's a homogeneous that can be handled by a transformer but then all the the the charts uh you know demarcating what medication they're taking their blood pressure test results and so forth that's heterogeneous and so you one ideally one needs some sort of hybrid solution that uh you know combines uh uh the signals from uh from tree-based models and network-based models together and again these tools can be built in in a in a custom way these algorithms aren't exactly uh yeah they're very straightforward to implement but it's i find it fascinating that if you don't really see these tools uh you know uh offered uh offered at scale by uh uh by organizations and i think they're they're they're they they're particularly useful to search to to a lot of uh let's say a lot of players in the kind of the machine learning business market right right so i missed to conclude uh our session means i just wanted to understand how you see next five year in your space uh vis-a-vis yeah so i do i do uh see machine learning as being fundamentally transformative to the space it is a very data rich uh space and uh in which a lot of the legacy players uh they basically still depend on uh kind of a manual effort so uh you know uh outsourcing the the effort uh to have you know and have a you know tens of thousands individuals right writing customer uh sql queries to uh go through claims uh you know one at a time or like you know 100 at a time and look for signals of uh of billing and then vastly over charging uh you know the clients to uh uh resolve these signals uh it's it's a strategy that's working it's gaining uh again it's gaining these legacy players like billions of dollars every year but obviously it is not a it's not a strategy for uh 2021 let's put it this way and so uh i see the the machine learning landscape is completely transforming uh that space and that transformation it's gonna have to happen hand-in-hand with kind of uh you know with uh with the domain experts that understand uh the nuances of healthcare and healthcare billing but uh i do hope that uh and it i do know that by uh you know us doing our part to you know resolve these billionaires like the consequences to uh kind of you know to to patients will be will be immediate lower lower health insurance costs kind of like less uh less headaches when it comes to uh billing issues and uh and so forth so i i do do believe we will all uh benefit some respects from this uh transfer transformation and uh uh ai driven health care payments great great friends thank you for your time leonardo and so you have so many insights have a great rest of the day thank you",
    "transcript_chunks": [
      "hi everyone this is sumit from leveler.com uh we are sas platform helping enterprises scaling up their ai ml operation to the production we help them eat at very stage of their ai journey like data preparation annotation model building model training deployment and overall mlaps today we are going to discuss uh some of the challenges in healthcare sector with leonard epstein uh he's a data scientist director at anomaly so hi leonard hi hi good morning i'm pretty good i'm pretty good yeah it's great to be here yeah thank you so much uh getting directly into the discussion we just wanted to understand that uh what exactly is anomaly and what is your daily day-to-day life at the world okay so uh anomaly is a small but rapidly growing uh startup uh that uh attempts to uh fix uh inappropriate healthcare billing using uh machine learning and data science so unfortunately in the united states health care billing is broken it's estimated that two to ten percent of all health care claims have some sort of inappropriate billing patterns on them which is pretty ridiculous when you think about it imagine if your credit card statement came back in ten percent of the time there was an error in it and the errors arise uh for multitude of reasons but mostly because healthcare billing is incredibly uh complex and so in order to resolve the data complexity uh we uh apply data science and machine learning technique techniques to identify the inappropriate billing signals and deal with them appropriately so to implement ai and data science in those kind of quality payments and all those documents how you implement it and what is the process what kind of data that you need and how you go about",
      "it so uh anomaly is uh dealing with hundreds of millions of uh healthcare claims about two to ten percent of which uh uh have uh some sort of broken signals uh present so it's a highly balanced data set the one we can manage but something we can work around uh what makes the proper the problem difficult is uh the need for some sort of a domain expertise to comprehend the data because if you want me to find anomalies in that healthcare data i will find anomalies that won't necessarily be indicative of an appropriate building like the other day i was looking through a claim when which a patient was injected uh once with a single dose of a drug uh paid out 2.1 million dollars perfectly legitimate it's the most expensive drug on the market so uh in order to uh differentiate between signals that are unusual and signals that are indicative of broken billing we need domain experts involved and kind of leveraging uh the knowledge of domain experts to uh to label and stratify uh the data is a challenge that we're dealing with appropriately but i think it's definitely a challenge that extends beyond healthcare to other aspects of machine learning where an expert is required to understand the nuances of the data so that the data can be uh can be labeled or categorized appropriately right so you are basically talking about the human and the loops and in your case there should be a domain expert not a normal human who identify the object simple object it's not that simple so means uh it sounds like you have to uh label lots of data and that too with the expensive resources so how you overcome those well uh so uh",
      "we've there there there's uh there there is this uh research uh movement pioneered at stanford around uh a weekly uh supervised learning in which one leverages the knowledge of the domain experts to uh to come up with uh thousands of heuristics and simple uh statistics and so forth defining let's say in our case you know what makes uh uh you know defining the various ways in which a claim can be broken and then uh applying uh kind of advanced ideas from weekly supervised learning where we kind of use bayesian techniques to decorate the various heuristics we can label uh the data try to label the data more intelligently of course uh even with application of weekly supervised learning uh some you know human in the loop interface and technology is uh is necessary for those domain experts to then come in and make sure that the predictions particularly the predictions centered around broken claims with inappropriate billing signals that those predictions are are meaningful and uh and and accurate and i can provide feedback in those distances where the predictions are incorrect and that also requires some sort of uh uh interpretability uh component which you know we're built we're not trying to reinvent the wheel here but uh a lot of these tools that we're building out you know where we're building custom to our uh particular use case right right so who uh means uh it sounds like your customer uh like insurance company or hospital uh and they need to you know check this uh you know all those faulty claims and all those kind of stuff so miss uh i think uh accuracy is a key concern yeah so it's very important that we have a high precision because when a",
      "customer say when a customer is an insurance company uh obviously we don't want the insurance company uh then going back hassling a a doctor who's uh billing appropriately i do want to mention that some of our customers are also the doctors uh themselves uh it's an important point because if you imagine a surgeon bills for sixty thousand dollar surgery and because of one little nuanced uh error that bill gets delayed by six months well that's that's a sixty thousand dollar payment that's getting delayed by uh six months so this is consider of a concern to all parties but yes uh in the healthcare space uh uh precision is critical this isn't like optimizing uh uh online ads uh the the room for false positives is very low which is why we're doubling down and we're tripling down on precision right right so means in in uh experiment mode and when you are you know testing mode then uh the challenge are different but when you scale up and go to the productions what are those challenges and how you overcome those oh so uh again we we we as i mentioned uh we have uh a lot of data you know hundreds of millions of claims um it's it's it's a manageable data set we're fortunate in that regard but it's still um it's still big data and uh the scaling trainable models particularly when it comes to negative examples um examples of uh claims that have been uh built appropriate but are still in some ways similar to uh to the broken claims scaling uh the uh the negative examples is uh is is difficult um again part of a part of the reason uh resolves around kind of the you know like the",
      "the difficulty of the modeling there are there are areas where you know neural networks are appropriate there are areas where combination of neural networks and other techniques are required and yes training uh training these models at scale is definitely you know an issue that my team is you know uh works continually to resolve and improve but there are uh currently no real out of the box solutions to uh to the problem in our particular niche right so means uh in your case uh you are you getting all the data which is real or you also you know incorporate some of the synthetic data because in your case you can create some synthetic scenario of document so yeah this is one advantage yeah this is where again it gets uh uh gets a little tricky so synthetic uh data generation is uh an invaluable tool for certain areas uh of uh ml it's important to understand uh when uh the tool can be can be leveraged for instance in a pure document analysis or imaging analysis you know you you average to uh two similar images together and you get a new type of image that's also legitimately in that class there's a kind of a continuous uh spectrum there in like the let's say that the imaging space uh with uh healthcare claims it's more nuanced because uh the claims are heterogeneous there's lots of uh they're more tabular and heterogeneous there's lots of signal there and for instance when you take two claims that are not broken and you average them together you're probably going to get something that that is broken and if you take two claims that are broken you average them together you're going to get a third claim that's broken",
      "in a completely completely different uh way so it's important to understand the uh the geometry of uh one's data and whether or not it is continuous and can be averaged appropriately uh in order uh to be able to kind of uh understand when uh when uh when six synthetic data is useful for one's use case and when it isn't right so means uh how big is your data science team and what concerns data scientists software developer how many uh team members that yeah so yeah so again right now the uh uh the the ml data science team uh consists of uh of uh four people and i just wanna say we're actively uh higher hiring for for more from uh to to add additional uh uh experienced uh motivated individuals to our team uh other some members of the team of think our title does have the title of data data scientists other members have a title of applied machine learning engineer but again at a you know kind of a in a small startup it's very much a uh where many hats sort of environment like for from our standpoint uh you know whether a team team members refer to as a data scientist or a machine learning engineer uh correlated more towards their kind of uh that the types of projects and challenges that they would like to take on and their preferred interests rather than kind of a you know any uh any any particular sense of uh uh sense of title and again i'd be happy to jump into those uh uh those nuances if you like yeah right in your opinion mr you you mentioned that you have a very small team but in your opinion if there's electronic plus",
      "people and means how you you know communicate your with your team that this is the responsibility of data scientists what is the responsibility of ml engineer what is the responsibility of software engineer and to provide them overall pipeline so that they can work in collaboration uh with open communication because most of the leaders mentioned this as a very big challenge for that so uh so uh again i like to use a bit of a strange uh uh strange metaphor here but one that you know i think is uh kind of appropriate to kind of uh differentiating uh useful for differentiating those roles so again i'm a big history nerd here and i'm particularly interested in uh uh the napoleonic wars and in the napoleonic wars uh kind of uh the three uh kind of a uh the the three the the the three main ways of of uh holding a warfare and organizing the battle was a kind of uh the the heavy artillery the infantry and uh the calvary now heavier artillery represented what was possible by state-of-the-art technology and it can be used to completely change the direction of battle via technological know-how and uh kind of uh brute force however it was also uh a heavy machinery that was difficult to drag into battle and uh and uh and set up and from my standpoint this is where kind of a machine learning uh infrastructure and machine learning techniques uh exist uh uh today they represent state-of-the-art technology without which certain organizations cannot flourish and cannot conquer in the business space nonetheless uh there's still there there it's it represents difficult technology difficult infrastructure to kind of set up maintain understand requires a lot of mathematical know-how just like the artillery back",
      "in napoleon's time required the knowledge of physics to kind of uh fire up appropriately but still it is uh not enough cur you know in the current business landscape to really uh alter the direction of battle in and of itself do you want to quickly mention that by world war one artillery gotten powerful enough to completely determine the course of battle but we're not there yet now the same time you have uh uh the cavalry which is just individuals and horses charging into battle the same way that they had for hundreds of years already uh it was uh an outdated technique seemingly outdated technique that still worked was crucial to the direction of the battle and there was uh it was all about you know speed and medium mobility and understanding and know-how and in some not in some ways this is how uh uh i think of the data science team now obviously this is where the metaphor breaks down because you know data scientists are expected to understand the nuances of the you know machine learning techniques and apply then where appropriately but the kind of the the the most important aspect of their role is to be light and quick and to leverage the appropriate technique at hand and if the correct answer to a particular problem is let's say uh simple statistics if all it takes to really get the result uh for a particular use case that gains value to the client is uh you know quick t-test uh well even though that technique has been around for a hundred years that's the appropriate solution and that is the quick uh solution and so that quickness and and speed arising from both the you know like analytical expertise and understanding",
      "of the data is where uh you know data scientists at least uh uh at least on certain teams are really poised uh to uh uh kind of uh to contribute and then of course you have uh uh you have uh you know the the infantry that basically uh does the standard things that need to get done that to kind of keep the battle going in this case uh we have uh you know the the soft the the the software engineers that uh just make sure the infrastructure is running make sure that we have the apis going that can get the the data back and forth uh to the clients and then uh i do also want to say that there's this nuanced niche between software engineer and data scientist that i think of as the product engineer particularly when you're building an analytics product uh having the customer admin you know in mind having the use case in mind and figuring out how to deliver uh the analytic solution appropriately to uh to the customer it takes engineering know-how but it also takes kind of this data science analytics know-how and i do see a bunch of that or data scientists the individuals that started their career data science that transitioned more into this hybrid uh engineering role where they built out uh their analytics into a customer-facing product and i do think it's important for organizations to or also recognize this uh this niche of uh of of product engineer which again is becoming kind of a you know more and more crucial role in certain teams as the teams grow out right interesting metaphor that you use so in that metaphor means in your opinion or in your experience means do data science",
      "team work internally only in house team or you always need some uh you know outside help as well like consultant audience and labeling or other other activity yes it it uh it depends uh on uh on the use case uh and again uh so right now i'm building out anomaly but prior to anomaly i was in the founding team of a startup called primer and i kind of led uh that applied machine learning to the natural language processing space and again i uh i mean we're small i uh led uh led the efforts to build out uh primers uh kind of a foundational uh uh prediction and analytics uh stack so help grow that team from four people to uh 100 people and the way i see it especially when the company's small to medium-sized outside of outside help when appropriate uh you know should uh uh should be it should be leveraged when you're a startup you do whatever it needs you you need to get done to be fast and nimble and uh satisfy uh satisfy the clients so uh and you don't want to find yourselves to reinventing uh the the wheels so again that anomaly uh you know we we we have in certain instances at least uh uh spoke to connections in other organizations to see if they could be uh you know uh provide uh provide value to fill the the you know certain certain gaps that you know we later went on to fill ourselves as we grew uh you know uh grew out the team uh now again uh and so for small to medium size companies i think it's important to be aware of all your options now as a company gets larger i think they're",
      "there there are more uh kind of uh trade-offs uh discussions to be made having said that i do think that uh kind of uh an organization should never be uh reinventing the wheel but at the same time also being aware of like you know the consequences of let's say uh asking outside helper depending on outside technology let's say uh three years down the line it's important to ask that as well what will happen as the organization grows what are the consequences of this uh decision so you know there's there's nuances at play but one one let's keep an open uh mind and always understand what decision is best for the company right so yeah i mean uh technological technological immensely what do you feel that there's some uh where you feel the gap means you are you are giving all the tools all the you know technology available in the market to your you know data science team so that they can perform their job that's too you know waste their time in unnecessary works and do their specific job so do you think any frustration any any incident that you frustrated you that you don't have any any tool in the market or miss those kind of incidents you want to share yeah well it's more it's it's it's more about uh let's say uh certain gaps in the space again outside of uh you know uh more intelligent labeling better a better human in the loop uh tools are highly desired particularly uh let's say in the area of uh uh of active learning using these tools to understand uh where the gaps uh uh in in in your data line by sampling intelligent you know intelligently uh uh back in the past",
      "i've you know uh play played around with a number of tools which again we're still works in progress maybe they've improved at this point but i haven't seen a good uh good active learning uh active learning solution that's where the you know your model is basically uh trying to uh uh make sure that the the the predictions it's confident in are truly uh deserving of that confidence but at the same time appropriately you know labeling the you know uncertain signals uh uh along the decision boundary and uh and so forth uh good interpretability tools i mean we're doing a lot of work in terms internally on model interpretability and i have to say in health care in particular uh models cannot be black box models uh for for for a multitude of reasons but you know one one of which is to make sure that because of the complexity we internally have to understand how the models making the predictions more importantly to gain trust uh with uh kind of the experts in the healthcare space it's important to be able to point out and show how the models are are you know are predicting so there's been a lot of fascinating work in the space but not as many uh uh tools uh for uh uh for interpretability of kind of commercial tools as uh as one would uh like uh i do think and he that there's uh there's also uh in terms of both you know tooling and you know and algorithms there's kind of this uh interesting uh you know dichotomy in the space uh if you look at all the you know recent amazing advancements that you've seen in the machine learning field uh a lot of them obviously have",
      "bit in the deep learning space and it's again deep learning is very much relevant to what we do uh if you uh but if you look at the algorithms uh leveraged to win uh uh the top kaggle competitions they're usually you know version of uh you know tree based models a version of you boost uh you know cat boost so part of that discrepancy is that uh you know deep learning works well on kind of more homogeneous data entry-based models work well on heterogeneous data with a variety of features uh there hasn't really been that much of a push uh tool-wise for a hybrid approach so you can imagine a data set where some of the most significant features are homogeneous such as images and text but there's other uh you know kind of a tabular information you see this a lot in healthcare space where maybe uh the clinical notes out written by the doctor outlining the patient's health that's all text that's a homogeneous that can be handled by a transformer but then all the the the charts uh you know demarcating what medication they're taking their blood pressure test results and so forth that's heterogeneous and so you one ideally one needs some sort of hybrid solution that uh you know combines uh uh the signals from uh from tree-based models and network-based models together and again these tools can be built in in a in a custom way these algorithms aren't exactly uh yeah they're very straightforward to implement but it's i find it fascinating that if you don't really see these tools uh you know uh offered uh offered at scale by uh uh by organizations and i think they're they're they're they they're particularly useful to search to to",
      "a lot of uh let's say a lot of players in the kind of the machine learning business market right right so i missed to conclude uh our session means i just wanted to understand how you see next five year in your space uh vis-a-vis yeah so i do i do uh see machine learning as being fundamentally transformative to the space it is a very data rich uh space and uh in which a lot of the legacy players uh they basically still depend on uh kind of a manual effort so uh you know uh outsourcing the the effort uh to have you know and have a you know tens of thousands individuals right writing customer uh sql queries to uh go through claims uh you know one at a time or like you know 100 at a time and look for signals of uh of billing and then vastly over charging uh you know the clients to uh uh resolve these signals uh it's it's a strategy that's working it's gaining uh again it's gaining these legacy players like billions of dollars every year but obviously it is not a it's not a strategy for uh 2021 let's put it this way and so uh i see the the machine learning landscape is completely transforming uh that space and that transformation it's gonna have to happen hand-in-hand with kind of uh you know with uh with the domain experts that understand uh the nuances of healthcare and healthcare billing but uh i do hope that uh and it i do know that by uh you know us doing our part to you know resolve these billionaires like the consequences to uh kind of you know to to patients will be will be immediate lower lower health",
      "insurance costs kind of like less uh less headaches when it comes to uh billing issues and uh and so forth so i i do do believe we will all uh benefit some respects from this uh transfer transformation and uh uh ai driven health care payments great great friends thank you for your time leonardo and so you have so many insights have a great rest of the day thank you"
    ],
    "transcript_word_count": 4272,
    "transcript_chunk_count": 15
  },
  {
    "video_id": "PigDjSpyPcA",
    "title": "Labellerr Podcast: George Williams on Challenges in AI-Driven Chip & Microprocessor Design",
    "description": "Discover how George Williams, Director of Data Science & ML at GSI Technology, tackled the challenges of building AI models to automate chip and microprocessor design. In this insightful talk, he shares his experiences and solutions for overcoming obstacles in AI-driven automation.\n\nThis discussion is hosted by Labellerr, a B2B SaaS platform dedicated to empowering enterprises to scale their AI-ML operations. Labellerr streamlines every stage of the AI lifecycle, including data preparation, annotation, model building, training, deployment, and MLOps.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo\n\n#DataScience #MachineLearning #AI #MLOps #DataAnnotation",
    "video_url": "https://www.youtube.com/watch?v=PigDjSpyPcA",
    "embed_url": "https://www.youtube.com/embed/PigDjSpyPcA",
    "duration": 1302,
    "view_count": 81,
    "upload_date": "20211008",
    "uploader": "Labellerr",
    "tags": [
      "data labeling platform annotation",
      "data solution",
      "data labeling software",
      "data annotation",
      "labeling software",
      "image annotation",
      "quality assurance",
      "data preprocessing",
      "preprocessing data",
      "dataset creation",
      "annotation software",
      "data governance",
      "data labeling",
      "data labeling services",
      "data labeling companies",
      "data labeling and annotation",
      "data labeling tool",
      "image data labeling",
      "audio labeling",
      "image data labeling service",
      "data labeling service",
      "ai data labeling"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "good morning everyone this is sumit from labeller uh joining us uh george williams from gsi technology he is a director in data science in data science and machine learning so hi george how are you morning good good great good to good to meet you and good to be um good to be here talking about this important topic cool miss uh we are just interested to know that we have talked with several people from healthcare and other sectors from the ai but you are the pretty unique because you are aware for networkings and telecommunications so how how is it all about means how you use data science uh those sector sure sure i think it's important to sort of um you know i think look at you know what is impacting a lot of these domains and verticals that you've been looking into you know healthcare autonomous vehicles cybersecurity fintech and ai um you know i think i think you understand this and i think most your audience understands is driving a lot of the innovation and a lot of the the progress um in in these fields over the past few years and that's accelerating rapidly um even further um you know the advances that we're seeing the data-driven nature of these applications and what we see in the compute hardware area is that the innovation needs to actually progress even even faster if we look at the computing hardware that's existed for the last 50 60 70 years um it's been it's been very intel based it's been very cpu-based the microprocessor design hasn't changed fundamentally and you know that convergence was very useful from a business perspective what we're seeing right now is that workloads have different requirements from the hardware they're more data-driven they're they're more driven by decisions that need to be made locally to where the data is collected right so we need we need compute at the edge of the network not just in the data center right we need powerful compute at the edge of the network um and closer to where where the sensors are collecting the data so for example in healthcare right ton of sensors now people are wearing in order to get a sense for for their health right so instead of capturing all this data and shipping it to a powerful data center um what we need to do is to be able to make those ai decisions uh locally um and so uh compute as opposed to being centralized um as it was for 50 or 60 years now we're starting to see a divergence of different kinds of compute hardware to handle these different kinds of scenarios data-driven and iot in particular so now in in the hardware world we talk a lot about this sort of silicon divergence right the novelty and hardware and i think we've all seen i think some of the stories around better compute power in our phones better uh compute power low power compute uh embedded machine learning embedded ai in all sorts of sorts of devices and we're seeing at least from my perspective a divergence of different kinds of microprocessor designs that are uniquely tailored and built to handle different different kinds of workloads and so uh right so this is very low on the technology stack we're talking about right the the hardware that's running all of these all of these software applications and ai applications so we're seeing this diversity of different kinds of compute right to handle the workload of of the future so data science right obviously machine learning ai you know that factors heavily into this because it's actually guiding where we need to go with how we design computers and silicon and chips to handle all of these new kinds of new kinds of applications so yeah it's a very exciting exciting area to be in um in data science um and to see it from a unique perspective which is from you know from the silicon view and yeah it's a very exciting space to be in to use data-driven methodology to to guide where we're going with advanced compute hardware and future compute hardware yeah that's right so most of the people that we talked to the biggest challenge that they face is data collection but in the hardware case you are kind of a building that is collecting the data so i i suppose uh data collection is not a big issue for you right uh that's not necessary necessarily the case so for example um when we're talking about you know traditionally over the last you know ever since the the first transistor was was created creating um uh designing computer chips and microprocessors has been a very human-centric kind of activity right we sort of you know we train the best and the brightest to to be able to create these um these floor plans for these chips and by floor plan i mean um how we lay out the transistors for memory for compute for cash um all these kinds of capabilities you need to to squeeze into a small service area for a chip well it turns out that we're starting to see an inflection point here it's so complicated what needs to be done to design computer chips um that we're starting to look at leveraging algorithms and in particular last few years ai to actually design these chips and with any kind of ai activity right we're going to be feeding it examples we're going to be feeding it data and so the human centric act you know what i envisioned in the next few years is that ai is probably going to be at the end of the day designing a chip from basic specifications um but that's going to be data-driven and so now the human is moving to sort of the in the data science and machine learning uh realm um people are going to be right collecting the data cleansing the data figuring out what the right inputs need to look like for the ai to actually learn how to how to design design future computer chips so we're already starting to see advances here google announced recently their efforts to have ai completely design the specification for chips we've seen this in the data center where google and facebook are leveraging algorithms and ai to um to intelligently route data and applications to the hardware that that is the best fit for it and to optimize power management at data centers so um a lot more ai that's going into into hardware both at the macro and the micro level and the data scientists the human data scientists is central right to this role because as you say right garbage in garbage out right the the only way these things can learn effective algorithms to do these tasks is if we feed it the right data and so a lot of human activity involved in even these activities in manufacturing and creating chips yeah sure so means in various uh industries we see that ai brings lots of uh automation so how uh chip building and those stuff designing and those kind of thing that hardware design ai build means ai can bring automation or improvement or processing how how it will work in your case uh yeah yeah definitely so right now um uh it it is like uh a lot of industries in the sense that um you know we do tend to uh i think a lot of us in this industry um we do tend to feed the hype a little bit with with complete autonomy um but i think what's happening right now is a really a good hybrid between the humans and and the machines so there's a lot of uh workloads that it's good to sort of offload to to robotic process automation or or to to algorithms and so what we're seeing is that um the ai assistant and the human being right there they're actually work they're actually assisting each other in in solving the problem and so that's what i'm seeing now i'm seeing a lot of progress here um in you know human teams leveraging ai to do a lot of the mundane tasks so that right they can they can truly innovate and i think that's that's the trend that i'm going to continue to see for for quite a while uh at least in my industry and i think that's true in general of other industries i've been in right in cyber security for example um it's tricky to have complete autonomous cyber security right you still need a human at the end of it sort of reviewing those decisions i think we're seeing this in healthcare with the radiology um you know we're not completely replacing the radiologist or or the doctor right but ai and algorithms and data science can can really be a useful tool um that you know these experts can can leverage to do their job uh better right whether it's you know trying to um diagnose some kind of issue with a patient or trying to design a really complicated computer chip um the problem is is essentially the same ai can be a huge um assist right to to these experts who need to really innovate very quickly so what is the biggest challenge for you especially uh while building this uh ai-based product yeah so connection or other process yeah i think you know at least in my industry where we're talking about using artificial intelligence to actually design computer chips it can be tricky to actually collect the right kind of data in which to inform an algorithm to to learn part of this is because a proprietary reasons so uh you know obviously when we talk about data science machine learning we like and we talk about volume and velocity and the various v's we need in order to accomplish data science tasks and variety is a very important aspect to the data that you're feeding a lot of these algorithms especially ones based on on deep learning and so that can be tricky in the chip industry where it's highly proprietary right there isn't a whole lot of sharing of of chip design in the in the industry and you know one of the reasons for that is that it's very expensive right to create a chip first chip efforts um first uh you know first family of of a new kind of microprocessor you know that can take the order of millions of dollars right in a couple years right to design a chip so people don't necessarily want to freely give away uh right all of their design files right in order to you know create this nice uber awesome model that can actually spit out right unique um computer chips and so um the proprietary nature i think of the business context i think which we all work on right that that can be sort of antithetical to kind of some of the the data needs we might have in which to create these these powerful algorithms i think it's true in the big pharma in the pharmaceutical industry like there's not a whole lot of sharing of you know drug design details right which you know totally makes sense right toss billions of dollars to make to make these drugs and so i think we're seeing some efforts to promote this kind of sharing without releasing a whole lot of very specific proprietary information such as federated learning notions of differentially private machine learning and that that i think will help to create these these sort of sharing ecosystems um for for the betterment of us all while individual companies don't need to offer up very specific information about um you know their secret sauce and how they how they actually make money it's a kind of if you talk technology wise means people often say that uh while doing the experimentation with the civil model and you know train them and while going to the you know production level they're completely different volume so how it is in your case uh you know in my case it's it's mostly still a lot of research um a lot of the work for uh automating you know the development and even the fabrication of hardware you know it's a research activity um we're not at a point like in say um say recommendation engines right where um there you need a very specific ml operations pipeline right because you're constantly getting new data in your models are drifting significantly um it's something that you just you need to automate at scale right because you have systems that are working right now and they need to continue to work at a high degree of accuracy and and high performance so i i say in my industry that that kind of thing that need to operationalize and productionalize research um you know we're still in our infancy so there's probably less of a need right now for for that kind of ml ops compared to some other some other industries but um in some sense we can leverage a lot of the great work that's now being done to create these mature pipelines uh in other industries such as fintech and such as cyber security such as um the the healthcare care industry but yeah i think it's it's a challenge across the board right you know you you spend a lot of time doing research and you get your insight and boom you want to put it into production right immediately but you know there's a significant number of challenges to to do that fortunately we have a lot of companies that now are are interested in in helping creating products you're starting to see turnkey solutions at the public cloud for creating these data pipelines that incorporate machine learning at scale um so there there are a lot more tools available for your your devops team and your data science team to leverage in order to um to to productionalize you know that that research and those models and all of that data science right so exciting times to be doing data science in yeah right yeah yeah so while we talk about tech stand so do you see any gap uh in current you know tools that you were talking about or is it good enough for in your case yeah so you know as someone who puts both a research hat on and an engineering hat on you know i think for the most part um you know i've leveraged open source for for much of my career uh in open research right if you're in an ai right got a stack of papers that you're always reading and so i think there's in terms of um availability and variety there's so many tools to to choose from and to investigate that i don't think there is a lack right of tools and a lot of them are openly available i think you know at some point um it's it's the interoperability of these things right that becomes the the pain point right so you know i i want to train a model here in my jupiter notebook and that's great um you know you know and i serialize that some kind of format um you know what systems can can take that kind of model um and then what systems can take a large model right like billions of parameters for example and what systems can scale that out uh in a way that's that's useful so i think um like i said like a lot a lot of efforts and companies out there that are starting to to address this um but you know i see it as analogous to you know where we were in software engineering maybe 15 20 years ago right where we had a great a bunch of silos of amazing products and ecosystems but they didn't really all talk to each other um and so we need we need more of that because there are a lot of great tools out there that solve very specific problems so they just need to talk to each other a little better yeah right sam is uh now we are approaching to the closing so i just wanted to ask you the one last question that how do you see the ai adoption in the industry or our understandings all the industry uh in next five years what's your opinion i i i'm gonna be honest with you which is i thought we would be in um an ai winter right now and you know the notion of ai winter right there have been previous ai hype cycles um in the past and you know they've they've peaked and then there's been a dramatic um you know loss of interest right in terms of products in terms of funding in terms of ideas and innovation and so you know i cut my teeth on machine learning neural networks deep learning a good 10 years ago at new york university um and so i've been predicting the last few years that right we would be seeing a drought in terms of ai innovation and i think we're seeing the opposite of that right it continues to grow and to expand and to get a lot of interest the ai startup ecosystem is the strongest it's been um since right since i've i've seen it a lot of aai efforts um are starting to succeed right versus versus fail and so i think it's i'm just looking at the trend and it's just it's just going to get get stronger um and you know you listen to the investor calls right for for any the publicly available investor calls right for any company and and ai and data science and student learning are are often um talked about positively right in those calls in terms of projects that they've undertaken and in terms of um more projects that they see are going to be uh relevant in the future so i think the future is bright uh definitely bright foreign yeah both both in research and in both in the startup ecosystem uh and and also at the at the big tech tech companies you know which are all uh at this point you know all the big all the big tech companies right now are either ai first publicly or or internally or they have major efforts to completely change the way um they look at certain problems with with artificial intelligence and so across the board right i'm i'm being obviously being very uh uh what's the word uh bullish i'm being very bullish with respect to um but i wasn't before i was i was bearish right the last few years but the trend you know speaks for itself and i think it's it's getting stronger i think there are a few things that we have to be careful about as an industry with the hype uh autonomous driving being one of them i think that that industry is is going to have to come to terms with the fact that we're not going to be able to make to meet the promises that were made i think in the last few years with an autonomous driving ecosystem uh that might be damaging for the ai industry as a whole uh i'm not sure but i think there are other areas where it's strong especially in computer vision um and and a lot of the great stuff i'm seeing it in healthcare right now cyber security you know it's i think that's it's going going to only help in cyber security and fintech right i think we've seen the revolution there where um you know bots do a lot of the the bulk of the trading now versus versus human beings um so yeah yeah autonomous i'm a little concerned about the other industries i feel like the ai is just going to continue to have a stronger presence yes really interesting uh interesting insight from thank you thank you for giving us time uh george and hopefully we'll talk more in coming with you been doing lots of research and hopefully when we connect again you have some something new that you have done so we'll talk about that too great thank you have a nice day take care thanks thanks bye",
    "transcript_chunks": [
      "good morning everyone this is sumit from labeller uh joining us uh george williams from gsi technology he is a director in data science in data science and machine learning so hi george how are you morning good good great good to good to meet you and good to be um good to be here talking about this important topic cool miss uh we are just interested to know that we have talked with several people from healthcare and other sectors from the ai but you are the pretty unique because you are aware for networkings and telecommunications so how how is it all about means how you use data science uh those sector sure sure i think it's important to sort of um you know i think look at you know what is impacting a lot of these domains and verticals that you've been looking into you know healthcare autonomous vehicles cybersecurity fintech and ai um you know i think i think you understand this and i think most your audience understands is driving a lot of the innovation and a lot of the the progress um in in these fields over the past few years and that's accelerating rapidly um even further um you know the advances that we're seeing the data-driven nature of these applications and what we see in the compute hardware area is that the innovation needs to actually progress even even faster if we look at the computing hardware that's existed for the last 50 60 70 years um it's been it's been very intel based it's been very cpu-based the microprocessor design hasn't changed fundamentally and you know that convergence was very useful from a business perspective what we're seeing right now is that workloads have different requirements from the hardware they're more",
      "data-driven they're they're more driven by decisions that need to be made locally to where the data is collected right so we need we need compute at the edge of the network not just in the data center right we need powerful compute at the edge of the network um and closer to where where the sensors are collecting the data so for example in healthcare right ton of sensors now people are wearing in order to get a sense for for their health right so instead of capturing all this data and shipping it to a powerful data center um what we need to do is to be able to make those ai decisions uh locally um and so uh compute as opposed to being centralized um as it was for 50 or 60 years now we're starting to see a divergence of different kinds of compute hardware to handle these different kinds of scenarios data-driven and iot in particular so now in in the hardware world we talk a lot about this sort of silicon divergence right the novelty and hardware and i think we've all seen i think some of the stories around better compute power in our phones better uh compute power low power compute uh embedded machine learning embedded ai in all sorts of sorts of devices and we're seeing at least from my perspective a divergence of different kinds of microprocessor designs that are uniquely tailored and built to handle different different kinds of workloads and so uh right so this is very low on the technology stack we're talking about right the the hardware that's running all of these all of these software applications and ai applications so we're seeing this diversity of different kinds of compute right to handle the workload of",
      "of the future so data science right obviously machine learning ai you know that factors heavily into this because it's actually guiding where we need to go with how we design computers and silicon and chips to handle all of these new kinds of new kinds of applications so yeah it's a very exciting exciting area to be in um in data science um and to see it from a unique perspective which is from you know from the silicon view and yeah it's a very exciting space to be in to use data-driven methodology to to guide where we're going with advanced compute hardware and future compute hardware yeah that's right so most of the people that we talked to the biggest challenge that they face is data collection but in the hardware case you are kind of a building that is collecting the data so i i suppose uh data collection is not a big issue for you right uh that's not necessary necessarily the case so for example um when we're talking about you know traditionally over the last you know ever since the the first transistor was was created creating um uh designing computer chips and microprocessors has been a very human-centric kind of activity right we sort of you know we train the best and the brightest to to be able to create these um these floor plans for these chips and by floor plan i mean um how we lay out the transistors for memory for compute for cash um all these kinds of capabilities you need to to squeeze into a small service area for a chip well it turns out that we're starting to see an inflection point here it's so complicated what needs to be done to design computer chips um",
      "that we're starting to look at leveraging algorithms and in particular last few years ai to actually design these chips and with any kind of ai activity right we're going to be feeding it examples we're going to be feeding it data and so the human centric act you know what i envisioned in the next few years is that ai is probably going to be at the end of the day designing a chip from basic specifications um but that's going to be data-driven and so now the human is moving to sort of the in the data science and machine learning uh realm um people are going to be right collecting the data cleansing the data figuring out what the right inputs need to look like for the ai to actually learn how to how to design design future computer chips so we're already starting to see advances here google announced recently their efforts to have ai completely design the specification for chips we've seen this in the data center where google and facebook are leveraging algorithms and ai to um to intelligently route data and applications to the hardware that that is the best fit for it and to optimize power management at data centers so um a lot more ai that's going into into hardware both at the macro and the micro level and the data scientists the human data scientists is central right to this role because as you say right garbage in garbage out right the the only way these things can learn effective algorithms to do these tasks is if we feed it the right data and so a lot of human activity involved in even these activities in manufacturing and creating chips yeah sure so means in various uh industries we see",
      "that ai brings lots of uh automation so how uh chip building and those stuff designing and those kind of thing that hardware design ai build means ai can bring automation or improvement or processing how how it will work in your case uh yeah yeah definitely so right now um uh it it is like uh a lot of industries in the sense that um you know we do tend to uh i think a lot of us in this industry um we do tend to feed the hype a little bit with with complete autonomy um but i think what's happening right now is a really a good hybrid between the humans and and the machines so there's a lot of uh workloads that it's good to sort of offload to to robotic process automation or or to to algorithms and so what we're seeing is that um the ai assistant and the human being right there they're actually work they're actually assisting each other in in solving the problem and so that's what i'm seeing now i'm seeing a lot of progress here um in you know human teams leveraging ai to do a lot of the mundane tasks so that right they can they can truly innovate and i think that's that's the trend that i'm going to continue to see for for quite a while uh at least in my industry and i think that's true in general of other industries i've been in right in cyber security for example um it's tricky to have complete autonomous cyber security right you still need a human at the end of it sort of reviewing those decisions i think we're seeing this in healthcare with the radiology um you know we're not completely replacing the radiologist or",
      "or the doctor right but ai and algorithms and data science can can really be a useful tool um that you know these experts can can leverage to do their job uh better right whether it's you know trying to um diagnose some kind of issue with a patient or trying to design a really complicated computer chip um the problem is is essentially the same ai can be a huge um assist right to to these experts who need to really innovate very quickly so what is the biggest challenge for you especially uh while building this uh ai-based product yeah so connection or other process yeah i think you know at least in my industry where we're talking about using artificial intelligence to actually design computer chips it can be tricky to actually collect the right kind of data in which to inform an algorithm to to learn part of this is because a proprietary reasons so uh you know obviously when we talk about data science machine learning we like and we talk about volume and velocity and the various v's we need in order to accomplish data science tasks and variety is a very important aspect to the data that you're feeding a lot of these algorithms especially ones based on on deep learning and so that can be tricky in the chip industry where it's highly proprietary right there isn't a whole lot of sharing of of chip design in the in the industry and you know one of the reasons for that is that it's very expensive right to create a chip first chip efforts um first uh you know first family of of a new kind of microprocessor you know that can take the order of millions of dollars right in a couple",
      "years right to design a chip so people don't necessarily want to freely give away uh right all of their design files right in order to you know create this nice uber awesome model that can actually spit out right unique um computer chips and so um the proprietary nature i think of the business context i think which we all work on right that that can be sort of antithetical to kind of some of the the data needs we might have in which to create these these powerful algorithms i think it's true in the big pharma in the pharmaceutical industry like there's not a whole lot of sharing of you know drug design details right which you know totally makes sense right toss billions of dollars to make to make these drugs and so i think we're seeing some efforts to promote this kind of sharing without releasing a whole lot of very specific proprietary information such as federated learning notions of differentially private machine learning and that that i think will help to create these these sort of sharing ecosystems um for for the betterment of us all while individual companies don't need to offer up very specific information about um you know their secret sauce and how they how they actually make money it's a kind of if you talk technology wise means people often say that uh while doing the experimentation with the civil model and you know train them and while going to the you know production level they're completely different volume so how it is in your case uh you know in my case it's it's mostly still a lot of research um a lot of the work for uh automating you know the development and even the fabrication of hardware you",
      "know it's a research activity um we're not at a point like in say um say recommendation engines right where um there you need a very specific ml operations pipeline right because you're constantly getting new data in your models are drifting significantly um it's something that you just you need to automate at scale right because you have systems that are working right now and they need to continue to work at a high degree of accuracy and and high performance so i i say in my industry that that kind of thing that need to operationalize and productionalize research um you know we're still in our infancy so there's probably less of a need right now for for that kind of ml ops compared to some other some other industries but um in some sense we can leverage a lot of the great work that's now being done to create these mature pipelines uh in other industries such as fintech and such as cyber security such as um the the healthcare care industry but yeah i think it's it's a challenge across the board right you know you you spend a lot of time doing research and you get your insight and boom you want to put it into production right immediately but you know there's a significant number of challenges to to do that fortunately we have a lot of companies that now are are interested in in helping creating products you're starting to see turnkey solutions at the public cloud for creating these data pipelines that incorporate machine learning at scale um so there there are a lot more tools available for your your devops team and your data science team to leverage in order to um to to productionalize you know that that research and",
      "those models and all of that data science right so exciting times to be doing data science in yeah right yeah yeah so while we talk about tech stand so do you see any gap uh in current you know tools that you were talking about or is it good enough for in your case yeah so you know as someone who puts both a research hat on and an engineering hat on you know i think for the most part um you know i've leveraged open source for for much of my career uh in open research right if you're in an ai right got a stack of papers that you're always reading and so i think there's in terms of um availability and variety there's so many tools to to choose from and to investigate that i don't think there is a lack right of tools and a lot of them are openly available i think you know at some point um it's it's the interoperability of these things right that becomes the the pain point right so you know i i want to train a model here in my jupiter notebook and that's great um you know you know and i serialize that some kind of format um you know what systems can can take that kind of model um and then what systems can take a large model right like billions of parameters for example and what systems can scale that out uh in a way that's that's useful so i think um like i said like a lot a lot of efforts and companies out there that are starting to to address this um but you know i see it as analogous to you know where we were in software engineering maybe 15 20 years",
      "ago right where we had a great a bunch of silos of amazing products and ecosystems but they didn't really all talk to each other um and so we need we need more of that because there are a lot of great tools out there that solve very specific problems so they just need to talk to each other a little better yeah right sam is uh now we are approaching to the closing so i just wanted to ask you the one last question that how do you see the ai adoption in the industry or our understandings all the industry uh in next five years what's your opinion i i i'm gonna be honest with you which is i thought we would be in um an ai winter right now and you know the notion of ai winter right there have been previous ai hype cycles um in the past and you know they've they've peaked and then there's been a dramatic um you know loss of interest right in terms of products in terms of funding in terms of ideas and innovation and so you know i cut my teeth on machine learning neural networks deep learning a good 10 years ago at new york university um and so i've been predicting the last few years that right we would be seeing a drought in terms of ai innovation and i think we're seeing the opposite of that right it continues to grow and to expand and to get a lot of interest the ai startup ecosystem is the strongest it's been um since right since i've i've seen it a lot of aai efforts um are starting to succeed right versus versus fail and so i think it's i'm just looking at the trend and it's",
      "just it's just going to get get stronger um and you know you listen to the investor calls right for for any the publicly available investor calls right for any company and and ai and data science and student learning are are often um talked about positively right in those calls in terms of projects that they've undertaken and in terms of um more projects that they see are going to be uh relevant in the future so i think the future is bright uh definitely bright foreign yeah both both in research and in both in the startup ecosystem uh and and also at the at the big tech tech companies you know which are all uh at this point you know all the big all the big tech companies right now are either ai first publicly or or internally or they have major efforts to completely change the way um they look at certain problems with with artificial intelligence and so across the board right i'm i'm being obviously being very uh uh what's the word uh bullish i'm being very bullish with respect to um but i wasn't before i was i was bearish right the last few years but the trend you know speaks for itself and i think it's it's getting stronger i think there are a few things that we have to be careful about as an industry with the hype uh autonomous driving being one of them i think that that industry is is going to have to come to terms with the fact that we're not going to be able to make to meet the promises that were made i think in the last few years with an autonomous driving ecosystem uh that might be damaging for the ai industry as a",
      "whole uh i'm not sure but i think there are other areas where it's strong especially in computer vision um and and a lot of the great stuff i'm seeing it in healthcare right now cyber security you know it's i think that's it's going going to only help in cyber security and fintech right i think we've seen the revolution there where um you know bots do a lot of the the bulk of the trading now versus versus human beings um so yeah yeah autonomous i'm a little concerned about the other industries i feel like the ai is just going to continue to have a stronger presence yes really interesting uh interesting insight from thank you thank you for giving us time uh george and hopefully we'll talk more in coming with you been doing lots of research and hopefully when we connect again you have some something new that you have done so we'll talk about that too great thank you have a nice day take care thanks thanks bye"
    ],
    "transcript_word_count": 3474,
    "transcript_chunk_count": 12
  },
  {
    "video_id": "oZEwYe8qj98",
    "title": "Labellerr Podcast: Peeyush Rai on Scaling AI, Data Annotation Solutions & Transforming Enterprises",
    "description": "Join Peeyush Rai, VP Engineering at Ciitizen, as he shares his experiences and challenges in scaling AI to production. With over two decades of expertise in software development and AI, Peeyush provides invaluable insights, discussing his transition from a software development professional to an AI leader.\n\nThis insightful talk is hosted by Labellerr, a cutting-edge B2B SaaS platform that accelerates AI-ML operations for enterprises. Labellerr supports businesses at every stage of their AI journey, including data preparation, annotation, model building, training, deployment, and MLOps.\n\nWebsite: www.labellerr.com\nBook a Demo: labellerr.com/book-a-demo\n\n#datascience #machinelearning #ai #MLOps #dataannotation #dataannotationsolutions",
    "video_url": "https://www.youtube.com/watch?v=oZEwYe8qj98",
    "embed_url": "https://www.youtube.com/embed/oZEwYe8qj98",
    "duration": 1680,
    "view_count": 199,
    "upload_date": "20211005",
    "uploader": "Labellerr",
    "tags": [
      "annotation software",
      "annotation workflow",
      "video labeling",
      "image labeling tools",
      "data labeling software",
      "data entry",
      "preprocessing data",
      "dataset creation",
      "labeling software",
      "manual annotation",
      "data validation",
      "data annotation",
      "data annotation services",
      "data annotation company",
      "image annotation",
      "how to make money with data annotation jobs",
      "data annotation tools",
      "data annotation tutorial",
      "data annotation for ai",
      "data preprocessing",
      "data entry jobs",
      "image segmentation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hi everyone uh this is sumit from labeller uh enterprises to build and scale their ai related product into the production and now we are in today we are talking with pius we use a cto vp of engineering at citizen and now we would ask his journey and how he become a data scientist and what is his challenges that he faced thank you thank you so much so yeah means uh to start with we would love to you know understand your journey your story and what you you know you just explain your journey so far and what you are doing right now yeah absolutely so i'll i'll give a quick summary of my journey over the last many years more than two decades uh i uh started as a software engineer early in my career uh grew to leadership roles and you know my initial part of the career was actually sort of experiencing the leading edge on the the whole internet boom the the transformation of applications to the web and then moving the stacks to the cloud right so these were sort of the three big things that happened and over the last 10 years you know data has become uh you know the forefront because these systems now have started generating a lot of data and people have started using starting with analytics and now data science techniques to look at data and and take better decisions uh and sometimes even predict problems and and and solve them before they occur right or address them before they occur so that that has been sort of in the last 10 years i've been focusing on data platforms and uh solutions that use data as that primary sort of you know uh tool to you know take business decisions and solve problems um as a leader in this uh in the in the technology space around software uh you know uh i've had to to to address these problems i've had to uh build stacks build teams right and you know and processes to address and build these platforms and therein is where i gained a lot of experience in uh what does it take to build a data platform or a machine learning platform that uses data as uh as as a very very critical source to then build solutions around predictive and uh uh you know things that you can't see with uh you know traditional heuristics and rule-based systems right and uh so so uh you know one sort of advantage i've had uh you know due to my past experiences i've seen the evolution of traditional software uh you know from the early 2000s where we used to have uh you know people used to write a lot of code uh the cycles used to be year-long cycles to a very open community today you know a lot of open source uh you know a lot of reusability and really agile processes right and a lot of automation around ci cd orchestration and so on so that world has evolved and has become very efficient i mean today to build a uh you know solution is is very very easy around traditional software the data science world and the data world is currently uh in my opinion in that state where software was in 2000s early 2000s and you know it is while it is a very very popular uh popularly developing domain but a lot of the underlying tools technologies infrastructure processes are yet to evolve and and become very very efficient as a result teams are spending a lot of time just plumbing writing code uh and not doing the actual data science work so that's that's been my experience and you know i've tried to solve this problem in my previous few companies and you know as i mentioned to you i'm working on a company that where this is going to be addressed as well cool so we just wanted to know more about your current company citizen means it sounds pretty interesting the idea yeah yeah absolutely so citizen is actually a a healthcare data platform and just to just to be you know if if viewers are going to research citizen it's c-i-i-t-i-z-e-n and uh that's the spelling of the company and we actually recently got acquired by another company so we you know just two weeks back we announced that acquisition so now we are a part of a bigger company called invitate which is a uh which is another healthcare tech company but at citizen you know the the mission was to help patients uh you know get value from their own health data and for those of you who are aware of the challenges in healthcare in the us and around the world right getting and using your own health data is extremely difficult uh in the us it can take anywhere from four to six weeks to to get your data and really uh you know even get access to it and using it is a whole different story right because you get these documents and reports which you don't really understand so the mission of citizen was actually in some ways very simple and in some ways very complex right the the simple was in the way i'll explain it is uh we will help our patients who are on our platform to get their uh healthcare documents medical documents from the providers providers could be their doctors or hospitals or labs and in today's world while these are all digitized they are all unstructured they arrive in pdf forms uh and they arrive in images uh image image forms and so on so it's not some some cds means some some uh mri yes yeah yeah yeah absolutely so it's not it's not a json or an xml you know like a schema type thing which we can pass and we get the data out we don't have that luxury and there in life is the problem because we we have a process to get these documents very efficiently for the patients but then we have to uh to really provide the value to the patients from their data we have to actually extract all the clinical context clinical data out of these documents so if you look at a any document for that matter right there are two aspects to it one is what you see and one is what you understand and if you translate that to the ai and machine learning world or data science world what you see is computer vision and what you understand is nlp so so we use both these techniques to actually see and understand the documents and and if you can imagine even as a human seeing and understanding a medical document is fairly challenging so to train the machine to do that is even more challenging because there is no standardization and it's it's really a very very hard problem uh and i think uh to i i'll explain how we do it but to cut to the end right we are probably one of the companies that have taken this solution to the most advanced stage uh today i feel and i think that's how we got a fairly good value of what we built with the company that actually bought us right so uh so the way we look at i mean obviously there's a lot of details behind it but when we get a document we essentially do the ocr to get the text out of it we also do a computer revision classification model to categorize a document sometimes we find multiple documents combined together so how to split them and then using the using both signatures right so visual signatures as well as text signatures we then uh you know break the documents into different sections and then for each section for example if it's a medication section or a lab results section or a history section history of present illness or family history so then with that context uh the nlp becomes more targeted right because if you try to feed a large document to an nlp engine it's going to produce garbage right so we've taken innovative ways to and we have filed about 15 patents around this whole thing uh but we use these combined models to then go and really sort of break the problem down into sections and then hone in like uh very domain specific nlp like for medications we have one nlp engine for cancer we have another nlp engine because the terminology is the the disease models are very very different for each of these so we've built that and that's one of the ips we have built and uh so while building this whole pipeline right we have a team of about eight to ten data scientists and uh a large part of our first year first year to year and a half was spent in just sort of figuring out the infrastructure to do this and so part of my learning is around that and even today i would say that you know we are not fully there yet because uh you know for our solution uh using closed platforms like sagemakers etc or google was not an option they are closed platforms we wanted to use best of breed uh nlp libraries or computer vision models and if you use open source then we have to stitch everything together build the pipelines right so there were those kind of challenges and uh data scientists are not software engineers right so expecting them to really write code and build the pipelines that was not possible so we had to actually hire a new set of people software engineers and uh data data engineers and so on to build these pipelines which pretty much doubled our budget for this solution so uh that's the quick summary of what we do at the end of the pipeline we actually are able to extract uh individual entities like cancer stage histologic type or medications or procedures uh you know with the values dates so it's a it's a very very complex problem that we're trying to solve but i think we have pushed the envelope to you know about 80 percent now in most of these things which is yeah in most of the uh healthcare ai product development their product manager their main issue is to collecting the data yeah that how to you know uh surpass this uh regulations and privacy and all those kind of stuff so you know start making your product so means uh like step zero is the biggest problem but when we browse the step zero and we start doing this pipeline the data preparation the pdas then you know model building and training the models and deploying it to the you know deploying it to then do the complete amalops in those uh process what is the biggest challenge for uh if as i would ask you as a healthcare based ai product then what is the biggest challenge yeah so i think with the with the data itself right so getting the data is just the first step then managing the data is i think very shortly follows which is non-trivial again you have lots of data you have sensitive data you have to keep it secure you have to sometimes remove the pii you know de-identify it for certain use cases uh you also have to maintain different versions of the data because you know data is not static it keeps changing it gets transformed so there's a whole data pipeline before even it gets to the model training etc right so that's the first part of the pipeline that we built right and of course under this there is like open source tools ml ops orchestration right all these things come to it then you get into uh the the model training and experiment management again so most data scientists end up using a jupiter notebook which is really a prototyping tool it is not a production quality production level ide so now to sort of really graduate a data scientist to from using a jupiter notebook to saying okay i want to know what features you used in your experiments i want to see the results and can you justify why the model that you're proposing is going to be the most optimal model right and so to to there are some open source technologies available at mla scoop flow etc that allow you to do that but to deploy them and integrate them and build the pipelines it takes months right it takes months to develop that and by that time the data scientist continues to use a jupiter notebook and you know just do some local experiments and nobody knows what's going on at that point then you get into the model serving and this is again you know the data scientists don't know how to really build a service or a rest api around that uh and most of them are not used to the newer tools like selden core and you know some of these kuflow also has a serving module but you know while the tools are available stitching them and using them is a whole different ball game and and for that you really have to train new engineers and build them into the picture so all in all by the time you get your pipeline well oiled and complete it is about six months to one year right easily that goes by and then as you said then you start actually seeing what you're building in real life you're still now you're working on more your local environments your data is on your laptops your models are on your jupiter notebook and really nothing is getting served you are just doing your f1 scores and accuracy and those kind of things which in isolation they don't mean anything it's like running a unit test and saying i have unit test coverage of uh 90 but when you put that in production running the whole you know ci cd and those things reveal the real right so these have been the challenges really right and uh i think in my opinion currently in general making the data science workflows more efficient uh and uh and and uh the the removing the the requirement to for data scientists to code a lot of uh production quality stuff right in python or whichever language they're using uh is a barrier today it takes it takes the cycle time to be that one year one and a half years type thing before you can see value and i think that's a big barrier today uh in the data science world so means you just mentioned that in the production level there's a whole lot of new challenge that you have not seen in the experimentation you know uh module so what how you overcome those challenges most of the people they discuss that this is a production is another ball game right here so how are you doing yeah so so if you if you just think simply right so you have created a model and you've done your uh experimentation you have a thesis saying that okay i think this this is the most optimal model right so when you put that into production you need to monitor that model right uh in traditional software when you put the software into production you monitor it you don't just don't deploy it and forget about it right you have to monitor it you have to see whether it's up whether there are any bugs so in a similar way uh a machine learning model needs to be monitored so you need to monitor what data it is seeing what inference it is doing right and and and capture that and push that back into your pipeline in near real time right not everything needs to be like in real time but you need to have a very uh iterative cycle of looking at that data and uh and seeing whether data is drifting right you trained on this type of data but the actual data it is seeing seeing is completely different then you need to take corrective actions right you need to fix your data that you're using for training or sometimes the model starts drifting right based on biases and so on it may be trained on bias data or you know whatever so so that's a feedback loop and and at citizen we have actually uh implemented a feedback loop we call it active learning uh right which again took a long time for us to deploy and build but it is there now and we actually this is sort of near real-time monitoring of the model right so that's one part the second part is you're monitoring the model but you also need to create a context around what this model is doing for your business right because the software may be uh you know running and you know your model is being served but is it really solving the problem it was meant to solve in the first place right so for example like citizen the model was supposed to automate our extraction process from the documents right now if you i'll take a simple example initially we sort of built a model for high recall and low accuracy because we said oh let's just find everything that we can find in the documents the the problem that it created downstream was uh the people who were reviewing the data for final approval they had to deal with a lot more data and that that time took uh that cycle time took a long time because uh you know medical data you cannot just say oh my machine is performing at 90 and i'm not not even going to look at it right so with healthcare data this is one challenge when you're when i'm when i'm telling you that this is your data it better be 100 accurate so you need a human in the loop to review it and approve it but if you're going to throw thousands of records at a human it's going to take more time so that was one issue uh then now we are sort of you know converting that to high precision and low recall and very targeted uh for certain sections so now the numbers are now reducing and our cycles are getting more and more efficient so now we are actually seeing the real business value right the earlier uh inference that we were doing was oh yeah our machine learning models are doing great but is it solving the problem it's supposed to solve so that's the that's the other part which people don't realize and again for most companies and i also advise a bunch of startups that are trying to build data pipelines and machine learning pipelines it takes about two years for them to get to this state of asking the right question around the the you know the ai of the data science solution what is it doing for you right and so the earlier you can ask that question in the cycle the the more the earlier you will fix those things because if you take two years to build and then start asking questions then two years have passed and this is the problem that most companies are facing today so right now means uh this is a product it's about b2c space yeah it's a b to b to c it's a it's a data platform so on one side it is consumers that on board and we get the data on the other side we have researchers pharmaceutical companies uh or cancer research companies that run clinical trials so they are the data consumers they take the data after patients consent and they try to use that data to solve you know create drugs or create clinical trials or recruit patients for clinical trials right uh and then the producers of data are the patients right there it's their data so so they are so it's a two-sided marketplace so to speak so what do you think mr enterprises understanding around ai in your geography means new as how is it means how well they understand the ai how it works and all those kind of stuff yeah i mean that's a that's a pretty broad question i would say that the way i would like to describe it right now is uh in the enterprise world there are larger enterprises that have uh you know started ai initiatives within their business units right so you know everybody says oh we have all this data what do we do with it right let's hire some data scientists and that's how it starts so they hire the data scientists and then the data scientists as usual do their you know prototyping and so on and so that cycle starts from there and uh and then by the time they start looking at the value it you know two years would be passing so that's how most enterprises are adopting the smaller companies right i mean you know there are fortune 1000 type enterprises and then there are smaller companies the smaller companies uh have even a bigger challenge because their data science teams are much smaller they hire eight to 10 people teams and they are expected to build something with data and again they struggle initially with the setting up the pipelines and really setting up the whole monitoring and measurement system right data management solution so a lot of effort goes into that uh then there are some really advanced tech companies like you know tesla or uber or you know these kind of companies of course facebook and google are at a different league but you won't count them but you know there are companies that are using very very specific tech uh around ai to solve to build solutions like tesla is obviously using it for self-driving uh uber is using it for various things as well and so they have lots of money lots of resources their ai teams are in hundreds uh right data science teams are in hundreds so they build it build everything on their own right so and they've already invested in that so let's leave those aside but the companies that are really trying to you know the enterprises that are non these big tech companies uh let's say it's a bank or an insurance company or fintech financial company uh they are just starting to get these initiatives going and as this problem as they as they start deploying these solutions the need for data scientists the need for uh seamless tools and infrastructure uh around this the data science pipelines data management uh and then the the need for measuring the business value of these things these are all going to become very important in the next few years all right right so yeah means i think you have given enough insight uh around this you know in your geography that how how the ai is adopting so what do you think means in especially in healthcare space in next five years going to be like you know uh about ai adoption and why visit with uh with the patient or hospitals and other regulatory authorities yeah healthcare is actually a very interesting domain in general not just with respect to data science but if you think about it there is really cutting-edge research and and development happening on some very specific things like you know people are looking at a lot of uh imaging data to pinpoint a certain disease type right like there are there are certain things like you know radiology images or x-rays or scans those are being done for very specific like you know you can identify if a person has cancer or not based on the image that is being automated and that's a very specific solution and a lot of good work has happened there then there is work happening in the genomic side of things where people are now trying to predict uh you know based on the gene uh sequencing what could be potential problems with uh with uh with uh productive productivity predictive right so yeah so uh you know i'm advising a company that actually uh does prediction of uh of a unborn child about these things based on the parents genes and stuff like that so it's uh very very interesting stuff is happening in those very specific areas then there's a whole side of a patient care right which is the remote uh nowadays because of it etc there has been just remote management of patients so there a lot of the nlp and computer vision stuff is uh coming into picture right you want to analyze what patients is saying about them and what the doctor is saying and then you know extract the context of that and make clinical uh conclusions or records of that right so that's that's a whole different area the third is uh just a clinical data which is what citizen is into uh the challenge there is that the real deep data exists in documents and not it's not a structured thing because the emr systems in the u.s are completely closed right they are trying to open up using some regulations but they only open up like the door a little bit right they don't open up and say okay this is the data you can use it and there are reasons for it you know i don't think i want to get into that but healthcare is a very complicated space so there are multiple different varieties of solutions being developed right and so i think each of these people are now applying data science or you know machine learning techniques uh to solve certain problems right uh purely around data there is obviously the problem that we are trying to solve which is uh taking unstructured documents and producing data then the next level problem will be taking the data and then sort of you know creating more like a correlation or a prediction model around you know around the patients and that that is not something that citizens mission is it's to produce the data but i'm sure there'll be other companies that will come out that will take that data and then build the predictive models around that data or build correlation models that you know i think my uh founder gives an example that uh migraine and drinking wine have a correlation right something like that yeah right so these are things that people don't won't discover until they have data with them and they start to start building correlation models right so these are the different types of things that are going to happen it's very very wide it's not right like in in autonomous cars we can we can say there are these five things that machine learning will do in autonomous cars but in healthcare it's like very very wide it can be used for various things yeah infinite variables are there right yeah i think uh you have given us enough insight thank you for your time and i hope uh we will discuss more on some other topic in coming months okay so thank you thank you thank you",
    "transcript_chunks": [
      "hi everyone uh this is sumit from labeller uh enterprises to build and scale their ai related product into the production and now we are in today we are talking with pius we use a cto vp of engineering at citizen and now we would ask his journey and how he become a data scientist and what is his challenges that he faced thank you thank you so much so yeah means uh to start with we would love to you know understand your journey your story and what you you know you just explain your journey so far and what you are doing right now yeah absolutely so i'll i'll give a quick summary of my journey over the last many years more than two decades uh i uh started as a software engineer early in my career uh grew to leadership roles and you know my initial part of the career was actually sort of experiencing the leading edge on the the whole internet boom the the transformation of applications to the web and then moving the stacks to the cloud right so these were sort of the three big things that happened and over the last 10 years you know data has become uh you know the forefront because these systems now have started generating a lot of data and people have started using starting with analytics and now data science techniques to look at data and and take better decisions uh and sometimes even predict problems and and and solve them before they occur right or address them before they occur so that that has been sort of in the last 10 years i've been focusing on data platforms and uh solutions that use data as that primary sort of you know uh tool to you",
      "know take business decisions and solve problems um as a leader in this uh in the in the technology space around software uh you know uh i've had to to to address these problems i've had to uh build stacks build teams right and you know and processes to address and build these platforms and therein is where i gained a lot of experience in uh what does it take to build a data platform or a machine learning platform that uses data as uh as as a very very critical source to then build solutions around predictive and uh uh you know things that you can't see with uh you know traditional heuristics and rule-based systems right and uh so so uh you know one sort of advantage i've had uh you know due to my past experiences i've seen the evolution of traditional software uh you know from the early 2000s where we used to have uh you know people used to write a lot of code uh the cycles used to be year-long cycles to a very open community today you know a lot of open source uh you know a lot of reusability and really agile processes right and a lot of automation around ci cd orchestration and so on so that world has evolved and has become very efficient i mean today to build a uh you know solution is is very very easy around traditional software the data science world and the data world is currently uh in my opinion in that state where software was in 2000s early 2000s and you know it is while it is a very very popular uh popularly developing domain but a lot of the underlying tools technologies infrastructure processes are yet to evolve and and become very",
      "very efficient as a result teams are spending a lot of time just plumbing writing code uh and not doing the actual data science work so that's that's been my experience and you know i've tried to solve this problem in my previous few companies and you know as i mentioned to you i'm working on a company that where this is going to be addressed as well cool so we just wanted to know more about your current company citizen means it sounds pretty interesting the idea yeah yeah absolutely so citizen is actually a a healthcare data platform and just to just to be you know if if viewers are going to research citizen it's c-i-i-t-i-z-e-n and uh that's the spelling of the company and we actually recently got acquired by another company so we you know just two weeks back we announced that acquisition so now we are a part of a bigger company called invitate which is a uh which is another healthcare tech company but at citizen you know the the mission was to help patients uh you know get value from their own health data and for those of you who are aware of the challenges in healthcare in the us and around the world right getting and using your own health data is extremely difficult uh in the us it can take anywhere from four to six weeks to to get your data and really uh you know even get access to it and using it is a whole different story right because you get these documents and reports which you don't really understand so the mission of citizen was actually in some ways very simple and in some ways very complex right the the simple was in the way i'll explain it",
      "is uh we will help our patients who are on our platform to get their uh healthcare documents medical documents from the providers providers could be their doctors or hospitals or labs and in today's world while these are all digitized they are all unstructured they arrive in pdf forms uh and they arrive in images uh image image forms and so on so it's not some some cds means some some uh mri yes yeah yeah yeah absolutely so it's not it's not a json or an xml you know like a schema type thing which we can pass and we get the data out we don't have that luxury and there in life is the problem because we we have a process to get these documents very efficiently for the patients but then we have to uh to really provide the value to the patients from their data we have to actually extract all the clinical context clinical data out of these documents so if you look at a any document for that matter right there are two aspects to it one is what you see and one is what you understand and if you translate that to the ai and machine learning world or data science world what you see is computer vision and what you understand is nlp so so we use both these techniques to actually see and understand the documents and and if you can imagine even as a human seeing and understanding a medical document is fairly challenging so to train the machine to do that is even more challenging because there is no standardization and it's it's really a very very hard problem uh and i think uh to i i'll explain how we do it but to cut to the end",
      "right we are probably one of the companies that have taken this solution to the most advanced stage uh today i feel and i think that's how we got a fairly good value of what we built with the company that actually bought us right so uh so the way we look at i mean obviously there's a lot of details behind it but when we get a document we essentially do the ocr to get the text out of it we also do a computer revision classification model to categorize a document sometimes we find multiple documents combined together so how to split them and then using the using both signatures right so visual signatures as well as text signatures we then uh you know break the documents into different sections and then for each section for example if it's a medication section or a lab results section or a history section history of present illness or family history so then with that context uh the nlp becomes more targeted right because if you try to feed a large document to an nlp engine it's going to produce garbage right so we've taken innovative ways to and we have filed about 15 patents around this whole thing uh but we use these combined models to then go and really sort of break the problem down into sections and then hone in like uh very domain specific nlp like for medications we have one nlp engine for cancer we have another nlp engine because the terminology is the the disease models are very very different for each of these so we've built that and that's one of the ips we have built and uh so while building this whole pipeline right we have a team of about eight to ten",
      "data scientists and uh a large part of our first year first year to year and a half was spent in just sort of figuring out the infrastructure to do this and so part of my learning is around that and even today i would say that you know we are not fully there yet because uh you know for our solution uh using closed platforms like sagemakers etc or google was not an option they are closed platforms we wanted to use best of breed uh nlp libraries or computer vision models and if you use open source then we have to stitch everything together build the pipelines right so there were those kind of challenges and uh data scientists are not software engineers right so expecting them to really write code and build the pipelines that was not possible so we had to actually hire a new set of people software engineers and uh data data engineers and so on to build these pipelines which pretty much doubled our budget for this solution so uh that's the quick summary of what we do at the end of the pipeline we actually are able to extract uh individual entities like cancer stage histologic type or medications or procedures uh you know with the values dates so it's a it's a very very complex problem that we're trying to solve but i think we have pushed the envelope to you know about 80 percent now in most of these things which is yeah in most of the uh healthcare ai product development their product manager their main issue is to collecting the data yeah that how to you know uh surpass this uh regulations and privacy and all those kind of stuff so you know start making your product so",
      "means uh like step zero is the biggest problem but when we browse the step zero and we start doing this pipeline the data preparation the pdas then you know model building and training the models and deploying it to the you know deploying it to then do the complete amalops in those uh process what is the biggest challenge for uh if as i would ask you as a healthcare based ai product then what is the biggest challenge yeah so i think with the with the data itself right so getting the data is just the first step then managing the data is i think very shortly follows which is non-trivial again you have lots of data you have sensitive data you have to keep it secure you have to sometimes remove the pii you know de-identify it for certain use cases uh you also have to maintain different versions of the data because you know data is not static it keeps changing it gets transformed so there's a whole data pipeline before even it gets to the model training etc right so that's the first part of the pipeline that we built right and of course under this there is like open source tools ml ops orchestration right all these things come to it then you get into uh the the model training and experiment management again so most data scientists end up using a jupiter notebook which is really a prototyping tool it is not a production quality production level ide so now to sort of really graduate a data scientist to from using a jupiter notebook to saying okay i want to know what features you used in your experiments i want to see the results and can you justify why the model that you're",
      "proposing is going to be the most optimal model right and so to to there are some open source technologies available at mla scoop flow etc that allow you to do that but to deploy them and integrate them and build the pipelines it takes months right it takes months to develop that and by that time the data scientist continues to use a jupiter notebook and you know just do some local experiments and nobody knows what's going on at that point then you get into the model serving and this is again you know the data scientists don't know how to really build a service or a rest api around that uh and most of them are not used to the newer tools like selden core and you know some of these kuflow also has a serving module but you know while the tools are available stitching them and using them is a whole different ball game and and for that you really have to train new engineers and build them into the picture so all in all by the time you get your pipeline well oiled and complete it is about six months to one year right easily that goes by and then as you said then you start actually seeing what you're building in real life you're still now you're working on more your local environments your data is on your laptops your models are on your jupiter notebook and really nothing is getting served you are just doing your f1 scores and accuracy and those kind of things which in isolation they don't mean anything it's like running a unit test and saying i have unit test coverage of uh 90 but when you put that in production running the whole you know ci cd",
      "and those things reveal the real right so these have been the challenges really right and uh i think in my opinion currently in general making the data science workflows more efficient uh and uh and and uh the the removing the the requirement to for data scientists to code a lot of uh production quality stuff right in python or whichever language they're using uh is a barrier today it takes it takes the cycle time to be that one year one and a half years type thing before you can see value and i think that's a big barrier today uh in the data science world so means you just mentioned that in the production level there's a whole lot of new challenge that you have not seen in the experimentation you know uh module so what how you overcome those challenges most of the people they discuss that this is a production is another ball game right here so how are you doing yeah so so if you if you just think simply right so you have created a model and you've done your uh experimentation you have a thesis saying that okay i think this this is the most optimal model right so when you put that into production you need to monitor that model right uh in traditional software when you put the software into production you monitor it you don't just don't deploy it and forget about it right you have to monitor it you have to see whether it's up whether there are any bugs so in a similar way uh a machine learning model needs to be monitored so you need to monitor what data it is seeing what inference it is doing right and and and capture that and push that",
      "back into your pipeline in near real time right not everything needs to be like in real time but you need to have a very uh iterative cycle of looking at that data and uh and seeing whether data is drifting right you trained on this type of data but the actual data it is seeing seeing is completely different then you need to take corrective actions right you need to fix your data that you're using for training or sometimes the model starts drifting right based on biases and so on it may be trained on bias data or you know whatever so so that's a feedback loop and and at citizen we have actually uh implemented a feedback loop we call it active learning uh right which again took a long time for us to deploy and build but it is there now and we actually this is sort of near real-time monitoring of the model right so that's one part the second part is you're monitoring the model but you also need to create a context around what this model is doing for your business right because the software may be uh you know running and you know your model is being served but is it really solving the problem it was meant to solve in the first place right so for example like citizen the model was supposed to automate our extraction process from the documents right now if you i'll take a simple example initially we sort of built a model for high recall and low accuracy because we said oh let's just find everything that we can find in the documents the the problem that it created downstream was uh the people who were reviewing the data for final approval they had to deal",
      "with a lot more data and that that time took uh that cycle time took a long time because uh you know medical data you cannot just say oh my machine is performing at 90 and i'm not not even going to look at it right so with healthcare data this is one challenge when you're when i'm when i'm telling you that this is your data it better be 100 accurate so you need a human in the loop to review it and approve it but if you're going to throw thousands of records at a human it's going to take more time so that was one issue uh then now we are sort of you know converting that to high precision and low recall and very targeted uh for certain sections so now the numbers are now reducing and our cycles are getting more and more efficient so now we are actually seeing the real business value right the earlier uh inference that we were doing was oh yeah our machine learning models are doing great but is it solving the problem it's supposed to solve so that's the that's the other part which people don't realize and again for most companies and i also advise a bunch of startups that are trying to build data pipelines and machine learning pipelines it takes about two years for them to get to this state of asking the right question around the the you know the ai of the data science solution what is it doing for you right and so the earlier you can ask that question in the cycle the the more the earlier you will fix those things because if you take two years to build and then start asking questions then two years have passed and",
      "this is the problem that most companies are facing today so right now means uh this is a product it's about b2c space yeah it's a b to b to c it's a it's a data platform so on one side it is consumers that on board and we get the data on the other side we have researchers pharmaceutical companies uh or cancer research companies that run clinical trials so they are the data consumers they take the data after patients consent and they try to use that data to solve you know create drugs or create clinical trials or recruit patients for clinical trials right uh and then the producers of data are the patients right there it's their data so so they are so it's a two-sided marketplace so to speak so what do you think mr enterprises understanding around ai in your geography means new as how is it means how well they understand the ai how it works and all those kind of stuff yeah i mean that's a that's a pretty broad question i would say that the way i would like to describe it right now is uh in the enterprise world there are larger enterprises that have uh you know started ai initiatives within their business units right so you know everybody says oh we have all this data what do we do with it right let's hire some data scientists and that's how it starts so they hire the data scientists and then the data scientists as usual do their you know prototyping and so on and so that cycle starts from there and uh and then by the time they start looking at the value it you know two years would be passing so that's how most enterprises are adopting",
      "the smaller companies right i mean you know there are fortune 1000 type enterprises and then there are smaller companies the smaller companies uh have even a bigger challenge because their data science teams are much smaller they hire eight to 10 people teams and they are expected to build something with data and again they struggle initially with the setting up the pipelines and really setting up the whole monitoring and measurement system right data management solution so a lot of effort goes into that uh then there are some really advanced tech companies like you know tesla or uber or you know these kind of companies of course facebook and google are at a different league but you won't count them but you know there are companies that are using very very specific tech uh around ai to solve to build solutions like tesla is obviously using it for self-driving uh uber is using it for various things as well and so they have lots of money lots of resources their ai teams are in hundreds uh right data science teams are in hundreds so they build it build everything on their own right so and they've already invested in that so let's leave those aside but the companies that are really trying to you know the enterprises that are non these big tech companies uh let's say it's a bank or an insurance company or fintech financial company uh they are just starting to get these initiatives going and as this problem as they as they start deploying these solutions the need for data scientists the need for uh seamless tools and infrastructure uh around this the data science pipelines data management uh and then the the need for measuring the business value of these things these",
      "are all going to become very important in the next few years all right right so yeah means i think you have given enough insight uh around this you know in your geography that how how the ai is adopting so what do you think means in especially in healthcare space in next five years going to be like you know uh about ai adoption and why visit with uh with the patient or hospitals and other regulatory authorities yeah healthcare is actually a very interesting domain in general not just with respect to data science but if you think about it there is really cutting-edge research and and development happening on some very specific things like you know people are looking at a lot of uh imaging data to pinpoint a certain disease type right like there are there are certain things like you know radiology images or x-rays or scans those are being done for very specific like you know you can identify if a person has cancer or not based on the image that is being automated and that's a very specific solution and a lot of good work has happened there then there is work happening in the genomic side of things where people are now trying to predict uh you know based on the gene uh sequencing what could be potential problems with uh with uh with uh productive productivity predictive right so yeah so uh you know i'm advising a company that actually uh does prediction of uh of a unborn child about these things based on the parents genes and stuff like that so it's uh very very interesting stuff is happening in those very specific areas then there's a whole side of a patient care right which is the remote uh nowadays",
      "because of it etc there has been just remote management of patients so there a lot of the nlp and computer vision stuff is uh coming into picture right you want to analyze what patients is saying about them and what the doctor is saying and then you know extract the context of that and make clinical uh conclusions or records of that right so that's that's a whole different area the third is uh just a clinical data which is what citizen is into uh the challenge there is that the real deep data exists in documents and not it's not a structured thing because the emr systems in the u.s are completely closed right they are trying to open up using some regulations but they only open up like the door a little bit right they don't open up and say okay this is the data you can use it and there are reasons for it you know i don't think i want to get into that but healthcare is a very complicated space so there are multiple different varieties of solutions being developed right and so i think each of these people are now applying data science or you know machine learning techniques uh to solve certain problems right uh purely around data there is obviously the problem that we are trying to solve which is uh taking unstructured documents and producing data then the next level problem will be taking the data and then sort of you know creating more like a correlation or a prediction model around you know around the patients and that that is not something that citizens mission is it's to produce the data but i'm sure there'll be other companies that will come out that will take that data and then",
      "build the predictive models around that data or build correlation models that you know i think my uh founder gives an example that uh migraine and drinking wine have a correlation right something like that yeah right so these are things that people don't won't discover until they have data with them and they start to start building correlation models right so these are the different types of things that are going to happen it's very very wide it's not right like in in autonomous cars we can we can say there are these five things that machine learning will do in autonomous cars but in healthcare it's like very very wide it can be used for various things yeah infinite variables are there right yeah i think uh you have given us enough insight thank you for your time and i hope uh we will discuss more on some other topic in coming months okay so thank you thank you thank you"
    ],
    "transcript_word_count": 4663,
    "transcript_chunk_count": 16
  },
  {
    "video_id": "zoP7kDubCN0",
    "title": "Labellerr Podcast: Jagadish Venkataraman on AI's Role in Healthcare, Geolocation & Medical Imaging",
    "description": "Watch Jagadish Venkataraman, Director of Machine Learning at Tempus Labs, share his inspiring journey in AI development. Gain valuable insights into the challenges he faced and the innovative solutions he implemented while building AI systems and managing machine learning projects tailored to healthcare use cases.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=zoP7kDubCN0",
    "embed_url": "https://www.youtube.com/embed/zoP7kDubCN0",
    "duration": 1261,
    "view_count": 139,
    "upload_date": "20211004",
    "uploader": "Labellerr",
    "tags": [
      "data analytics",
      "data categorization",
      "data collection",
      "data labeling",
      "data nerd",
      "data tagging",
      "data analysis",
      "annotation software",
      "data preprocessing",
      "tagging software",
      "image labeling",
      "data validation",
      "image annotation",
      "video labeling",
      "data annotation",
      "data annotation services",
      "image annotation services",
      "data annotation tech",
      "data annotation tools",
      "data labeling service",
      "data science",
      "lidar annotation",
      "video annotation",
      "what is data labeling",
      "best data labeling companies"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hi jadish thank you for your time uh how are you media i'm doing well how are you how's the weather there weather is okay it's still hot waiting for it to cool now yeah it must be pretty miss morning for you early morning for you i think it's for you uh yeah not too early but yeah okay okay so jumping right into the uh you know in this session i would like to ask you that your background your story so far in the healthcare and biotech uh space cool so yeah so i think my background um my my phd was more in like so the wireless communications information theory space um and i was i started my career around the semiconductor industry uh almost 15 years ago now and i was there for the first four or five years of my initial journey and then i kind of shifted over to the navigation space where i was again with the startup where we were trying to sort of complement gps with a bunch of terrestrial positioning algorithms to be able to help you know like nine one one so when when somebody is stuck at like the basement of a parking lot and they have to call 9-1-1 and they cannot talk they have to come and locate you and gps often fails in those challenging locations and so this startup was trying to see how they could best complement gps and give them like a standardized 10 meter accuracy radius and also like vertical information so that they could actually go find the people right so that's kind of where i was um so some personal experiences from my side on my family side with the healthcare system and then i started reading up a little bit more about like um some of the um the methods and analyses used in medical image processing and and then i realized that there was like a lot of parallel happening between uh you know techniques that i was using and versus what folks were using in like surgery or diagnostics or a lot of the imaging techniques right um and then i just felt like that would be something that was more uh relatable for me essentially because it's um sort of closer to touching people's lives uh not that some of the others i was doing was not but this was just it just felt a little bit more meaningful and gratifying to sort of gravitate towards that space right so and then i started doing a little bit of um sort of course work i tried to collaborate with a couple of professors at uc san francisco a bunch of hobby projects um but i think really my career sort of uh took a pivot when um i got into like striker endoscopy probably about six or six years ago now and i think pretty much the deep learning wave that was happening um where computer vision traditional computer vision was kind of being turned on instead with uh you know like with a lot of the the deep learning and the machine learning uh techniques that were coming through i kind of latched on to that if you will uh and you know that's kind of how i got into that space and then after that being with the alphabet um where essentially there was a combination of surgical analytics work that i was doing and also a bunch of sort of aging and diseases uh manifestation of aging where i was with the yeast lab working on some uk biobank data so i think all of that together gave me a little bit more perspective on like real world data and you know what are the techniques that we want to be using um and so right now i'm with tempest labs as director of imaging ai where i direct the radiology and the cell imaging teams here so the goal is really to be able to combine different data modalities effectively to be able to personalize or sort of provide precision medicine treatment recommendations to patients that's kind of my story to where i am that's pretty interesting that you uh saw the parallel between geolocation and medical imaging that's really amazing yeah to me the the biggest parallel to me was more just data in some sense uh because in the traditional way of sort of working in the semiconductor space and you know navigation space it was more about like how do you design a dsp algorithms how do you get it working you know how do you uh sort of reduce error rates and things of that nature right but um kind of towards the um end of my journey with nexnav which was a navigation company i was with we started looking at data coming from a lot of our towers and we started sort of mining the data a little bit for like noise and jammers and you know things of that nature and that's when i started realizing that there was a lot of pattern recognition work that was happening both in the traditional or rather classical machine learning space as well as some of the new newer techniques that were coming up right so to me that was like the the bridge i mean the fact that um like this is also like sort of semi-big data where essentially we were trying to mine for patterns and improve performance and the same thing was kind of happening in the medical imaging space as well right right so what are you seeing that a broad challenge in overall healthcare ai space that you see again like i said i want to kind of speak to the the uh the medical imaging space maybe because that's kind of where my um most of my experience is so the biggest challenge is that like i i've been facing right from my alphabet times now it's just data so i think it's all about um you know like having the right kind of data and i think it's a cliche now to say that you can have the um most the fancy algorithm in the world but if your data quality is not good uh it's not going to happen whereas if you have really the right kind of data at scale and diversity that you care about even the numbers algorithm can do the job for you right i mean it's that's kind of you know so we are somewhere in between where you have some data there's some missingness there is some noise and you want to figure out like okay what is it that can work what cannot especially in the medical imaging community i feel like the last couple of years maybe things are changing a little bit but there's always this uh sort of um um you know lack of understanding of where is the boundary between privacy and wanting to sort of uh advance science right so a lot a lot of the clinicians like surgeons um people we work with everybody obviously wants to make treatment decisions better they want to advance science they want to push the envelope but then they also have this thing at the back of their mind okay uh what kind of patient concept do i have and what i mean and even though even though patients consent for research um is it okay can i actually make a commercial product out of it using somebody's data and then deploy it in the real world and build a billion dollar company out of it and then you know is that what the patient is consenting to right i mean there's a lot of um gray area there in terms of figuring out like um what is uh what are they really signing up for and how can companies go into it i think again i'm not a legal expert i think a lot of people are like figuring that out and you know uh understanding how best to work with that data but but definitely work in progress in terms of like you know um where we want to be in terms of the scale and quality of the data right right yeah but really i can understand that there's so many concern with the use of ai and with the privacy and all so what do you think is what's your take that uh enterprises understanding of ai and ml as a technology means some people have their different understanding around yeah some find is pretty evil some kind of very useful so what do you uh see as a technology what healthcare community is seeing it is yes i can probably speak to it more from um sort of a customer standpoint uh if you will i think so this that's both right from from an enterprise standpoint when the company is going out and um building it i think oftentimes it's now so you have to say that your company is ai more or less right everybody is doing it and you know if you don't have that buzzword somewhere in your pitch deck then you're doing something wrong it looks like but um from a consumer standpoint i think um i have seen the broad spectrum from um you know clinicians who come in and they're extremely skeptical of uh you know what ai is capable of doing to the point where i mean there's also this sort of fear that is uh ai replacing the radiologists the pathologist right i mean that's kind of that uh aspect to it as well and then there's the other end where they just expect um an ai algorithm to walk into the operating room and just solve everything for them you know they just think it's a magic wand you can even you know things will just work something so it's kind of the two ends and just trying to sort of um educate them as okay what is really possible today with the kind of because you show them a scene the ai algorithm can barely differentiate the liver from the kidney from you know something that's the kind of algorithm status we are at so it's if you go and ask them should i actually go reset the liver from this side or that side or you know understand the tissue and there's so much complexity there and that to understand the scene and actually recommend the treatment i don't think it's um it's quite there yet so it's essentially trying to break down like the high level task into like little bite-sized pieces that that we can teach the machine to sort of understand the scene and you know interpret it and give feedback back to this clinician and then sort of build up the little pieces to eventually solve for like a more complex problem right so both the machine learning team understanding that and the user understanding that and converging on what exactly we want to be solving as a team i think is critical because if you don't if you if you end up saying oh this is the latest uh model that deep my deep mind put out i'm gonna do it and then uh you implement it and then surgeon is like whatever time i'm not gonna use it you know so so that so it's essentially working coherently with them to understand what is it that they want to be solving and then going ahead and trying techniques right right so yeah means uh generally uh what happens means ai helps the clinicians the doctors to you know perform their job better that's the does the you know the idea that monopod people are pushing and they're kind of liking it uh when they come to know that this is the technology this helps them to perform that automate some of the uh tasks that they need to do while checking up a question and also sort of supplements segmentation and you know uh the medication and all so means uh there's one thing that uh people find it very difficult this experimentation with the machine learning model is uh one thing but while screening up and building a production level you know going to production level there are challenges are different so what is your challenge when you find and how you go from that yeah so i think 100 going from like a training paradigm to like an broad inference paradigm where you want to be deploying at scale it's like it's a huge challenge i think that's not just a challenging healthcare i think it's a challenge with all algorithms right so specifically with healthcare i think because there's so much uh discussion about like um bias in the data and you know diversity lack of diversity in the data that you're training the model with so usually if you look at um you know like the way companies go about it right so they go and strike um some sort of um big partnership with a couple of different hospitals uh you know like and then that's kind of how you get like let's say big data to train your algorithms or something but there is already an institutional bias in just doing that right so you essentially are kind of limiting yourself to those data sets where the surgeons are used to performing surgery a certain way or they have a certain demographic of patients that always come to that maybe it's not fully representative of the you know like the hispanic population of the asian population what not right so just by introducing that institutional bias right at the outset you've kind of already tailored your algorithm to that specific population and you know that specific instrumentation that is used in that hospital and so on right so being able to identify the right validation cohorts and being able to bring up uh like you know the bring in like the different um sites with different uh demographics and different instruments and you know different qualities in the images and having your algorithm tested on those and then reporting that in like a peer-reviewed journal really is like the holy grail of the whole thing right i think and even if you want to eventually go to like an fda submission or whatever so that's that's that is the path that you have to take so that is a huge lift and uh you know so it's not it's not just a huge lift from an algorithm site it's a huge lift from a legal standpoint and an ops standpoint you know like um and to be able to get data at that kind of scale convince everybody that it's working because at the end of the day a clinician is going to look at your numbers and alter a person's treatment plan right and they and and their career is on the line the patient's life is not the thing i mean there's so many things that and they were like why am i using your numbers right and so we have to be able to back it up for them to do that right right yeah that sounds means uh this humongous task to collect all those data and train your model and you know build something that will work on the different demography that you discuss because every demography have different you know gene genes and those kind of stuff and things get changes with the weather and whatnot and i can only imagine how complex and how many variables uh are there so means uh while building this kind of solution what do you think is the biggest gap in the product or you know a solution on which you which you use to build this kind of solution so what is the biggest gap if you can you know have any thoughts on it biggest gap in terms of like what is out there or uh give us biggest gap in terms of like the unmet need and the understanding from the enterprise wow what the gap in what sense well so there are several solutions in the market so the product which companies and enterprises use to build this kind of you know product ai nml the ml ops and all the data training data preparation model training model deployment so do you mean that's a technical problem let's see uh in healthcare there's uh more challenging than the business business problem is more challenging than the technical property but if you talk only about a technical problem so do you see any gap in the in the market that products that are available are not uh up to the mark or they kind of lag at somewhere that not a robust product that you can use in your daily operation yeah i mean i can i can speak to some of the uh you know like off-the-shelf tooling that's available right i mean there's so many offerings from like aws or gcp or you know where they essentially give you easier ways to like build pipelines you know create deployment containers you know deploy them you know there's a lot of um sort of easy to um use kind of tools that are available out there right but the challenge is really in being able to customize that for your specific need and being able to scale with that right so every every application is different every model is different and you know so you you have your own model monitoring requirements you have your own hyper parameter space in which you're you're searching and you have your own end validation metric that that you care about right and so it may not be um so it could very well be especially in the medical space a multi-stage machine learning model so it's not like you look at an image and you uh let's say you were able to identify the tumor region uh correctly or what not right but the algorithm doesn't stop there so what do you do with that identified tumor region it's like okay you go and characterize it and then you go and combine it with the the genetic information with that patient and eventually come up with like a predictor that says okay this is how this patient is going to respond to treatment or something like that right so that is the full chain so if you look at it in sort of piecemeal um maybe the tools available out there are giving you like the best segmentation monitoring performance or whatnot but maybe that is not really translating into performance at the treatment response level right so you want to be able to build something end to end to uh really care about that end to end is different for different people right and they're different for different applications different for different even within our space like it's and each application that you want to deploy is different right so i i feel like that is a challenge that um we are still trying to understand and overcome in some ways like how do you um customize all of the tooling that is available what do you build in house what do you leverage from uh you know what is out there striking that uh right combination and then obviously um it's the team size matters the compute power matters you know all of those come into play when you uh put all this together right so from from a pure deployment standpoint that's kind of the gap i see like where does where does research stop where does engineering begin and you know is that is that sort of um non-overlapping or is there like a big overlap between who understands machine learning and who does engineering you know uh sort of thing that that's the boundary that we are trying to understand right right so yeah ms you have given so much insight in in very short time about your work and all in general uh about you know machine learning production level deployment and all so uh one last question uh how you see in next five years in your serviceable geography the ai adoption uh in terms of your user internal enterprises and in general audience means how you see what's your hope yeah i mean especially with the pandemic i think you can see that there is a lot more um sort of like teledoc applications and people are starting to use like you know apps they have like doctor consultation via whatsapp you know what not right so i think generally i feel like this is um a little bit of the the data barrier has been broken down in my opinion um you know like how people uh deal with medical data and how they perceive medical data uh in general so my my uh sort of um hope uh for this uh the next five years as well as sort of where i think the thing the trend is heading is that there will be definitely broader acceptance in terms of like um how do you start integrating with hospital systems and uh you know and how do you effectively create de-identifying data sets that you can do research on uh and and publish right and i also feel like uh more recently the fda came out with this breakthrough status application where essentially you can come up with a proof of concept i saw so first of all software is now recognized as a medical device so that is a big step so you can you don't have to have a physical device it's just a software as a medical device and then fda came out with this breakthrough medical device status where essentially you can take a data set and a validation set prove throughout your algorithm and immediately file to the fda and if they assign a breakthrough status to your algorithm that opens a lot of doors for you so you can basically go to hospitals and say hey look we got the breakthrough status now we just want to show that thing can work at scale and you know like put more resources into it and because it's it's almost like a semi-guaranteed way of getting an fda approved algorithm out into the market if they give you the breakthrough status so i feel like with some of these things uh again ai healthcare is not the consumer space so you you will not see apps come out like every three months or every six months with new device so it's it's not that it's not going to happen but just the fact that people are now more understanding of okay it is going to play a role how how can we contribute and you know how can we do so while respecting like safety and privacy and all of that i think is where this thing is heading so right right so means yeah means i think you have put all and summarize everything in a very concise manner so thank you for our time uh jagdish uh we will hopefully talk more on this and we'll see your progress in coming days and weeks thank you no thanks thanks for having me just great to meet you",
    "transcript_chunks": [
      "hi jadish thank you for your time uh how are you media i'm doing well how are you how's the weather there weather is okay it's still hot waiting for it to cool now yeah it must be pretty miss morning for you early morning for you i think it's for you uh yeah not too early but yeah okay okay so jumping right into the uh you know in this session i would like to ask you that your background your story so far in the healthcare and biotech uh space cool so yeah so i think my background um my my phd was more in like so the wireless communications information theory space um and i was i started my career around the semiconductor industry uh almost 15 years ago now and i was there for the first four or five years of my initial journey and then i kind of shifted over to the navigation space where i was again with the startup where we were trying to sort of complement gps with a bunch of terrestrial positioning algorithms to be able to help you know like nine one one so when when somebody is stuck at like the basement of a parking lot and they have to call 9-1-1 and they cannot talk they have to come and locate you and gps often fails in those challenging locations and so this startup was trying to see how they could best complement gps and give them like a standardized 10 meter accuracy radius and also like vertical information so that they could actually go find the people right so that's kind of where i was um so some personal experiences from my side on my family side with the healthcare system and then i started reading up",
      "a little bit more about like um some of the um the methods and analyses used in medical image processing and and then i realized that there was like a lot of parallel happening between uh you know techniques that i was using and versus what folks were using in like surgery or diagnostics or a lot of the imaging techniques right um and then i just felt like that would be something that was more uh relatable for me essentially because it's um sort of closer to touching people's lives uh not that some of the others i was doing was not but this was just it just felt a little bit more meaningful and gratifying to sort of gravitate towards that space right so and then i started doing a little bit of um sort of course work i tried to collaborate with a couple of professors at uc san francisco a bunch of hobby projects um but i think really my career sort of uh took a pivot when um i got into like striker endoscopy probably about six or six years ago now and i think pretty much the deep learning wave that was happening um where computer vision traditional computer vision was kind of being turned on instead with uh you know like with a lot of the the deep learning and the machine learning uh techniques that were coming through i kind of latched on to that if you will uh and you know that's kind of how i got into that space and then after that being with the alphabet um where essentially there was a combination of surgical analytics work that i was doing and also a bunch of sort of aging and diseases uh manifestation of aging where i was with",
      "the yeast lab working on some uk biobank data so i think all of that together gave me a little bit more perspective on like real world data and you know what are the techniques that we want to be using um and so right now i'm with tempest labs as director of imaging ai where i direct the radiology and the cell imaging teams here so the goal is really to be able to combine different data modalities effectively to be able to personalize or sort of provide precision medicine treatment recommendations to patients that's kind of my story to where i am that's pretty interesting that you uh saw the parallel between geolocation and medical imaging that's really amazing yeah to me the the biggest parallel to me was more just data in some sense uh because in the traditional way of sort of working in the semiconductor space and you know navigation space it was more about like how do you design a dsp algorithms how do you get it working you know how do you uh sort of reduce error rates and things of that nature right but um kind of towards the um end of my journey with nexnav which was a navigation company i was with we started looking at data coming from a lot of our towers and we started sort of mining the data a little bit for like noise and jammers and you know things of that nature and that's when i started realizing that there was a lot of pattern recognition work that was happening both in the traditional or rather classical machine learning space as well as some of the new newer techniques that were coming up right so to me that was like the the bridge i mean",
      "the fact that um like this is also like sort of semi-big data where essentially we were trying to mine for patterns and improve performance and the same thing was kind of happening in the medical imaging space as well right right so what are you seeing that a broad challenge in overall healthcare ai space that you see again like i said i want to kind of speak to the the uh the medical imaging space maybe because that's kind of where my um most of my experience is so the biggest challenge is that like i i've been facing right from my alphabet times now it's just data so i think it's all about um you know like having the right kind of data and i think it's a cliche now to say that you can have the um most the fancy algorithm in the world but if your data quality is not good uh it's not going to happen whereas if you have really the right kind of data at scale and diversity that you care about even the numbers algorithm can do the job for you right i mean it's that's kind of you know so we are somewhere in between where you have some data there's some missingness there is some noise and you want to figure out like okay what is it that can work what cannot especially in the medical imaging community i feel like the last couple of years maybe things are changing a little bit but there's always this uh sort of um um you know lack of understanding of where is the boundary between privacy and wanting to sort of uh advance science right so a lot a lot of the clinicians like surgeons um people we work with everybody",
      "obviously wants to make treatment decisions better they want to advance science they want to push the envelope but then they also have this thing at the back of their mind okay uh what kind of patient concept do i have and what i mean and even though even though patients consent for research um is it okay can i actually make a commercial product out of it using somebody's data and then deploy it in the real world and build a billion dollar company out of it and then you know is that what the patient is consenting to right i mean there's a lot of um gray area there in terms of figuring out like um what is uh what are they really signing up for and how can companies go into it i think again i'm not a legal expert i think a lot of people are like figuring that out and you know uh understanding how best to work with that data but but definitely work in progress in terms of like you know um where we want to be in terms of the scale and quality of the data right right yeah but really i can understand that there's so many concern with the use of ai and with the privacy and all so what do you think is what's your take that uh enterprises understanding of ai and ml as a technology means some people have their different understanding around yeah some find is pretty evil some kind of very useful so what do you uh see as a technology what healthcare community is seeing it is yes i can probably speak to it more from um sort of a customer standpoint uh if you will i think so this that's both right from",
      "from an enterprise standpoint when the company is going out and um building it i think oftentimes it's now so you have to say that your company is ai more or less right everybody is doing it and you know if you don't have that buzzword somewhere in your pitch deck then you're doing something wrong it looks like but um from a consumer standpoint i think um i have seen the broad spectrum from um you know clinicians who come in and they're extremely skeptical of uh you know what ai is capable of doing to the point where i mean there's also this sort of fear that is uh ai replacing the radiologists the pathologist right i mean that's kind of that uh aspect to it as well and then there's the other end where they just expect um an ai algorithm to walk into the operating room and just solve everything for them you know they just think it's a magic wand you can even you know things will just work something so it's kind of the two ends and just trying to sort of um educate them as okay what is really possible today with the kind of because you show them a scene the ai algorithm can barely differentiate the liver from the kidney from you know something that's the kind of algorithm status we are at so it's if you go and ask them should i actually go reset the liver from this side or that side or you know understand the tissue and there's so much complexity there and that to understand the scene and actually recommend the treatment i don't think it's um it's quite there yet so it's essentially trying to break down like the high level task into like little",
      "bite-sized pieces that that we can teach the machine to sort of understand the scene and you know interpret it and give feedback back to this clinician and then sort of build up the little pieces to eventually solve for like a more complex problem right so both the machine learning team understanding that and the user understanding that and converging on what exactly we want to be solving as a team i think is critical because if you don't if you if you end up saying oh this is the latest uh model that deep my deep mind put out i'm gonna do it and then uh you implement it and then surgeon is like whatever time i'm not gonna use it you know so so that so it's essentially working coherently with them to understand what is it that they want to be solving and then going ahead and trying techniques right right so yeah means uh generally uh what happens means ai helps the clinicians the doctors to you know perform their job better that's the does the you know the idea that monopod people are pushing and they're kind of liking it uh when they come to know that this is the technology this helps them to perform that automate some of the uh tasks that they need to do while checking up a question and also sort of supplements segmentation and you know uh the medication and all so means uh there's one thing that uh people find it very difficult this experimentation with the machine learning model is uh one thing but while screening up and building a production level you know going to production level there are challenges are different so what is your challenge when you find and how you go from",
      "that yeah so i think 100 going from like a training paradigm to like an broad inference paradigm where you want to be deploying at scale it's like it's a huge challenge i think that's not just a challenging healthcare i think it's a challenge with all algorithms right so specifically with healthcare i think because there's so much uh discussion about like um bias in the data and you know diversity lack of diversity in the data that you're training the model with so usually if you look at um you know like the way companies go about it right so they go and strike um some sort of um big partnership with a couple of different hospitals uh you know like and then that's kind of how you get like let's say big data to train your algorithms or something but there is already an institutional bias in just doing that right so you essentially are kind of limiting yourself to those data sets where the surgeons are used to performing surgery a certain way or they have a certain demographic of patients that always come to that maybe it's not fully representative of the you know like the hispanic population of the asian population what not right so just by introducing that institutional bias right at the outset you've kind of already tailored your algorithm to that specific population and you know that specific instrumentation that is used in that hospital and so on right so being able to identify the right validation cohorts and being able to bring up uh like you know the bring in like the different um sites with different uh demographics and different instruments and you know different qualities in the images and having your algorithm tested on those and then reporting",
      "that in like a peer-reviewed journal really is like the holy grail of the whole thing right i think and even if you want to eventually go to like an fda submission or whatever so that's that's that is the path that you have to take so that is a huge lift and uh you know so it's not it's not just a huge lift from an algorithm site it's a huge lift from a legal standpoint and an ops standpoint you know like um and to be able to get data at that kind of scale convince everybody that it's working because at the end of the day a clinician is going to look at your numbers and alter a person's treatment plan right and they and and their career is on the line the patient's life is not the thing i mean there's so many things that and they were like why am i using your numbers right and so we have to be able to back it up for them to do that right right yeah that sounds means uh this humongous task to collect all those data and train your model and you know build something that will work on the different demography that you discuss because every demography have different you know gene genes and those kind of stuff and things get changes with the weather and whatnot and i can only imagine how complex and how many variables uh are there so means uh while building this kind of solution what do you think is the biggest gap in the product or you know a solution on which you which you use to build this kind of solution so what is the biggest gap if you can you know have any thoughts on it",
      "biggest gap in terms of like what is out there or uh give us biggest gap in terms of like the unmet need and the understanding from the enterprise wow what the gap in what sense well so there are several solutions in the market so the product which companies and enterprises use to build this kind of you know product ai nml the ml ops and all the data training data preparation model training model deployment so do you mean that's a technical problem let's see uh in healthcare there's uh more challenging than the business business problem is more challenging than the technical property but if you talk only about a technical problem so do you see any gap in the in the market that products that are available are not uh up to the mark or they kind of lag at somewhere that not a robust product that you can use in your daily operation yeah i mean i can i can speak to some of the uh you know like off-the-shelf tooling that's available right i mean there's so many offerings from like aws or gcp or you know where they essentially give you easier ways to like build pipelines you know create deployment containers you know deploy them you know there's a lot of um sort of easy to um use kind of tools that are available out there right but the challenge is really in being able to customize that for your specific need and being able to scale with that right so every every application is different every model is different and you know so you you have your own model monitoring requirements you have your own hyper parameter space in which you're you're searching and you have your own end validation metric",
      "that that you care about right and so it may not be um so it could very well be especially in the medical space a multi-stage machine learning model so it's not like you look at an image and you uh let's say you were able to identify the tumor region uh correctly or what not right but the algorithm doesn't stop there so what do you do with that identified tumor region it's like okay you go and characterize it and then you go and combine it with the the genetic information with that patient and eventually come up with like a predictor that says okay this is how this patient is going to respond to treatment or something like that right so that is the full chain so if you look at it in sort of piecemeal um maybe the tools available out there are giving you like the best segmentation monitoring performance or whatnot but maybe that is not really translating into performance at the treatment response level right so you want to be able to build something end to end to uh really care about that end to end is different for different people right and they're different for different applications different for different even within our space like it's and each application that you want to deploy is different right so i i feel like that is a challenge that um we are still trying to understand and overcome in some ways like how do you um customize all of the tooling that is available what do you build in house what do you leverage from uh you know what is out there striking that uh right combination and then obviously um it's the team size matters the compute power matters you know all of",
      "those come into play when you uh put all this together right so from from a pure deployment standpoint that's kind of the gap i see like where does where does research stop where does engineering begin and you know is that is that sort of um non-overlapping or is there like a big overlap between who understands machine learning and who does engineering you know uh sort of thing that that's the boundary that we are trying to understand right right so yeah ms you have given so much insight in in very short time about your work and all in general uh about you know machine learning production level deployment and all so uh one last question uh how you see in next five years in your serviceable geography the ai adoption uh in terms of your user internal enterprises and in general audience means how you see what's your hope yeah i mean especially with the pandemic i think you can see that there is a lot more um sort of like teledoc applications and people are starting to use like you know apps they have like doctor consultation via whatsapp you know what not right so i think generally i feel like this is um a little bit of the the data barrier has been broken down in my opinion um you know like how people uh deal with medical data and how they perceive medical data uh in general so my my uh sort of um hope uh for this uh the next five years as well as sort of where i think the thing the trend is heading is that there will be definitely broader acceptance in terms of like um how do you start integrating with hospital systems and uh you know and",
      "how do you effectively create de-identifying data sets that you can do research on uh and and publish right and i also feel like uh more recently the fda came out with this breakthrough status application where essentially you can come up with a proof of concept i saw so first of all software is now recognized as a medical device so that is a big step so you can you don't have to have a physical device it's just a software as a medical device and then fda came out with this breakthrough medical device status where essentially you can take a data set and a validation set prove throughout your algorithm and immediately file to the fda and if they assign a breakthrough status to your algorithm that opens a lot of doors for you so you can basically go to hospitals and say hey look we got the breakthrough status now we just want to show that thing can work at scale and you know like put more resources into it and because it's it's almost like a semi-guaranteed way of getting an fda approved algorithm out into the market if they give you the breakthrough status so i feel like with some of these things uh again ai healthcare is not the consumer space so you you will not see apps come out like every three months or every six months with new device so it's it's not that it's not going to happen but just the fact that people are now more understanding of okay it is going to play a role how how can we contribute and you know how can we do so while respecting like safety and privacy and all of that i think is where this thing is heading so right",
      "right so means yeah means i think you have put all and summarize everything in a very concise manner so thank you for our time uh jagdish uh we will hopefully talk more on this and we'll see your progress in coming days and weeks thank you no thanks thanks for having me just great to meet you"
    ],
    "transcript_word_count": 3958,
    "transcript_chunk_count": 14
  },
  {
    "video_id": "ryyUSvukAtg",
    "title": "Track Safety project: Project setup and Image annotation on Labellerr platform",
    "description": "Data annotation demo for Bounding box image annotation for ensuring Track safety through Computer Vision.\n\nBlog Link: https://blog.labellerr.com/index.php/2021/06/08/ensuring-railway-track-safety-using-computer-vision/\n\nFor Your data labelling needs. \nVisit: www.labellerr.com",
    "video_url": "https://www.youtube.com/watch?v=ryyUSvukAtg",
    "embed_url": "https://www.youtube.com/embed/ryyUSvukAtg",
    "duration": 85,
    "view_count": 36,
    "upload_date": "20210616",
    "uploader": "Labellerr",
    "tags": [
      "machine learning",
      "dataset creation",
      "object detection",
      "deep learning",
      "bounding box annotation",
      "data science",
      "model deployment",
      "data annotation demo",
      "data annotation",
      "image annotation",
      "AI data labeling",
      "object detection annotation",
      "safety monitoring AI",
      "annotation for transportation",
      "computer vision demo",
      "ML data annotation",
      "automated image labeling",
      "AI for track inspection",
      "annotation tutorial",
      "computer vision annotation",
      "safety AI solutions",
      "data annotation tutorial"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=ryyUSvukAtg! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "MefAt81_-Uk",
    "title": "Image Annotation for Hard Hat Object Detection: Demo Walkthrough Video",
    "description": "During the year 2019, in the United States of America, the number of preventable deaths increased by 2% which totals to 4,572 injuries that could have been avoided. Recent data from the US National Safety Council tells that such injuries are increasing rapidly with increasing construction, transportation, warehousing, and agriculture. In the year 2019, the industry which experienced the largest number of fatal injuries that could have been prevented was the construction industry. Here is a use case that explains the usage of AI to reduce accidents on construction-like sites.\nHence, you can just go focus on your work and leave all your headache on Labellerr: An AI-enabled Data Annotation SAAS Platform. Just go through the video to have a quick walkthrough of how easy and feasible it is to perform text annotation using Labellerr.\nWebsite: https://www.labellerr.com/\nContact: support@tensormatics.com",
    "video_url": "https://www.youtube.com/watch?v=MefAt81_-Uk",
    "embed_url": "https://www.youtube.com/embed/MefAt81_-Uk",
    "duration": 129,
    "view_count": 70,
    "upload_date": "20210614",
    "uploader": "Labellerr",
    "tags": [
      "deep learning",
      "machine learning",
      "image segmentation",
      "data annotation",
      "dataset creation",
      "pixel classification",
      "AI data annotation",
      "data science",
      "construction safety AI",
      "AI for construction sites",
      "data preprocessing",
      "industrial safety AI",
      "annotation SAAS platform",
      "data labeling for safety",
      "AI for workplace safety",
      "transportation safety AI",
      "warehousing safety AI",
      "agriculture safety AI",
      "annotation walkthrough",
      "AI use case safety",
      "accident reduction AI",
      "artificial intelligence"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=MefAt81_-Uk! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "FNfmaQkUw14",
    "title": "Data Annotation for fitness AI model training: pose estimation",
    "description": "Experience the truly automated machine learning experience with Labellerr’s complete ML suite. Just plug in your data via a range of connectors as FTP, Local storage, Google Drive, AWS S3, Azure Blob etc. \n\nAllow our inbuilt Auto ML feature to suggest annotations based on your requirement. Leverage the Auto-label feature to annotate your data with 10x speed and save crucial man-hours. Get a list of confidence scores of the assigned labels and verify only those with a low score.\n\nConnect Labellerr to the Cloud-Based Compute service of your choice with assured data privacy and train your machine learning models on the go. Without the hustle of downloading the data, Service-specific data conversion formats, Data Leakage to name a few.\n\nVisit our website: www.labellerr.com and mention your use case in brief and our customer engineer will contact you and help you prepare the plan and get you running on a trial with us to validate.\n\n#dataannotation #computervision #visiondata #dataformachinelearning #fitnessai #aiinfitness",
    "video_url": "https://www.youtube.com/watch?v=FNfmaQkUw14",
    "embed_url": "https://www.youtube.com/embed/FNfmaQkUw14",
    "duration": 383,
    "view_count": 169,
    "upload_date": "20210517",
    "uploader": "Labellerr",
    "tags": [
      "ai training",
      "data mapping",
      "ai models",
      "data labeling",
      "deep learning models",
      "data annotation tools",
      "data labeling software",
      "image annotation",
      "data preprocessing",
      "annotation software",
      "image classification",
      "image segmentation",
      "dataset creation",
      "dataset generation",
      "fitness data",
      "data models",
      "ml training",
      "data annotation",
      "ai sports",
      "ai fitness",
      "machine learning",
      "data visualization",
      "computer vision",
      "deep learning",
      "computer graphics",
      "data analysis",
      "data science",
      "visual data"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=FNfmaQkUw14! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "vk_Le1l3n8g",
    "title": "How Conversational AI and IOT are Redefining the Future of Smart Homes",
    "description": "The worldwide connected home market is projected to grow at a CAGR of 25% from 2020 to 2025. And 86% of millennials would pay more for a connected home. [Source]\n\nAI and IoT-enabled applications and devices are making homes more secure, comfortable, and safe. Technologies are adding a personalized touch to the home security environment and predictive automation along with predictive maintenance is enhancing customer satisfaction. \n\nHome automation is an in-vogue trend and AI and IOT are leading factors in the exponential growth of smart home markets. These technologies are no longer considered as luxury, in fact, they are a necessity now. \n\nThe prices of these house-tech devices have sloped down hence they can now cater to a wider audience than just the premium sector. Cloud-based smart home intelligence is the most preferred choice among buyers.",
    "video_url": "https://www.youtube.com/watch?v=vk_Le1l3n8g",
    "embed_url": "https://www.youtube.com/embed/vk_Le1l3n8g",
    "duration": 15,
    "view_count": 266,
    "upload_date": "20210512",
    "uploader": "Labellerr",
    "tags": [
      "internet of things",
      "iot ecosystem",
      "future tech",
      "edge computing",
      "data analytics",
      "sensor networks",
      "cloud computing",
      "network optimization",
      "iot trends",
      "network security",
      "digital transformation",
      "machine learning",
      "industry 4.0",
      "wireless networks",
      "tech disruption",
      "home security AI",
      "data visualization",
      "secure networks",
      "big data",
      "automation systems",
      "data management",
      "ai automation",
      "secure IoT",
      "data governance",
      "data science",
      "big data analytics",
      "network efficiency"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=vk_Le1l3n8g! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "nXyUnV--MQs",
    "title": "Text Annotation for Named Entity Recognition using Labellerr - An AI based Data Annotation Platform",
    "description": "Named Entity Recognition is a method of extracting information from texts. It basically extracts named entities from texts such as name of a place, name of a game, etc. Neural Networks can be trained to extract the name of such entitites but as we all know that Deep Learning requires a lot of data. Hence, you can just go focus on your work and leave all your headache on Labellerr: An AI enabled Data Annotation SAAS Platform. Just go through the video to have a quick walkthrough of how much easy and feasible it is to perform text annotation using Labellerr.\nWebsite: https://www.labellerr.com/\nContact: support@tensormatics.com",
    "video_url": "https://www.youtube.com/watch?v=nXyUnV--MQs",
    "embed_url": "https://www.youtube.com/embed/nXyUnV--MQs",
    "duration": 221,
    "view_count": 155,
    "upload_date": "20210505",
    "uploader": "Labellerr",
    "tags": [
      "data augmentation",
      "computer vision",
      "ai training",
      "data labeling software",
      "vision datasets",
      "ml training",
      "deep learning models",
      "visual recognition",
      "data management",
      "data annotation SAAS",
      "data annotation",
      "machine learning",
      "ai tools",
      "deep learning",
      "data preprocessing",
      "annotation software",
      "annotation methods",
      "dataset creation",
      "labeling software",
      "data labeling tools",
      "image annotation",
      "labeling solutions",
      "artificial intelligence",
      "image labeling",
      "training data"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=nXyUnV--MQs! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "TA8SsOiO66Y",
    "title": "Intelligent Document Processing Automation with Data Annotation bounding box, OCR",
    "description": "This use case of structured OCR annotation in Intelligent Document Processing is widely applicable to industries such as healthcare, logistics, warehousing, government, financial services, etc",
    "video_url": "https://www.youtube.com/watch?v=TA8SsOiO66Y",
    "embed_url": "https://www.youtube.com/embed/TA8SsOiO66Y",
    "duration": 228,
    "view_count": 213,
    "upload_date": "20210415",
    "uploader": "Labellerr",
    "tags": [
      "intelligent document processing",
      "ocr",
      "data annotation",
      "computer vision",
      "digitization",
      "machine learning",
      "ocr technology",
      "document digitization",
      "digital transformation",
      "data mining",
      "ai ocr",
      "AI document processing",
      "advanced ocr",
      "data labeling",
      "ocr solutions",
      "machine learning OCR",
      "smart ocr",
      "ai data labeling",
      "data labeling for OCR",
      "ocr algorithms",
      "data pipelines",
      "digital workflows",
      "industry OCR solutions",
      "ocr software",
      "deep learning ocr"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=TA8SsOiO66Y! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "CGISJWBd80c",
    "title": "Revolutionizing entertainment industry with Labellerr, Automated AI & Data Annotation SAAS platform",
    "description": "Humans can quickly understand the intent of the features like the color of the poster, expressions on the face of the actors to appropriately decide genres like horror, comedy, drama, documentary, etc. Let us take examples of popular video streaming services such as Netflix, Amazon Prime, Spotify, and Hulu have already adopted use cases related to genre prediction like:\n\n1. Personalized movie recommendation.\n2. Automatically generating thumbnails.\n\nOur problem statement acts as a base for the above use cases. When we predict the movie genre using a poster then it directly helps the streaming service provider to create personalized movie recommendations upon the viewer’s past history. \n\nAlso, different thumbnails are shown to different viewers for the same movie or a TV show based upon what type of genre the viewer usually prefers.\n\nEarlier some of them used the traditional machine learning approach to solve the problem. The main motive was to predict the movie genre from the plot summaries. The traditional approach involved using Naive Bayes and RNN to predict the movie genre using plot summaries. Another approach to solving the problem was to use the Bernoulli event model to learn the likelihood of a genre using movie ratings.\n\nChallenges in the current scenario:\n\n*There were several challenges that were encountered using the traditional machine learning approach. \n* The audience was more likely to make decisions using posters rather than movie ratings or plot summaries. So, this approach was not much effective to create customized advertisements and recommendations for the audience.\n* Since the visuals are more effective and convincing in making a decision hence the prediction using plot summaries which basically involves text is very naive. \n* According to PwC, there is a decline of -65.9% in year-on-year box office revenue in the year 2020. Also, total revenue of US$ 22.6 Billion was generated by OTT in the year 2020 in the US [Source].\nWhat are the solution and new ways of solving the problem?\nHumans are correctly able to identify the genre of a movie by just looking at it. We can suppose that the different characteristics of the posters such as colors, text-based features, facial expressions of the actors, can be used as input features to our CNN-based deep learning models.\nInspiration of our approach\n\n1. Color plays an important role in deciding the genre of the movie.\n2. Facial expression is a very important factor in the prediction of movie genres using posters.\n3. Animated Movies can be predicted using just colors and poster type.\n4. Raw pixels contain very important information about the image.\n\nThe core AI solution:\n\nCNN-based image classification models are one of the techniques for performing multi-label classification. CNN-based have been proved to outperform every other traditional machine learning algorithm. \n\nThe deep learning model is trained using a dataset that contains three classes namely action, comedy, animation, war, fantasy, history, romance, documentary, science fiction, war, mystery, fantasy, drama, etc. The images are normalized before using them for training our CNN-based deep learning multi-label image classification problem. \n\nThen the final trained model is tested on the testing data and the performance is recorded. Another technique that can be used for the task is the transfer learning technique. The models having complex architecture and these models are already trained on a much larger dataset. These models can be fine-tuned on the dataset of our choice for the genre prediction.\n\nPossible impacts made by our solution:\n\nThe internet advertising market in the United States is expected to reach a value of US$ 153 Billion by 2024. Although, during the pandemic, the revenue reduced from US$ 125.2 Billion to US$ 121.0 Billion in 2020. \n\nHence, our solution is a possible approach to get customers more involved in the advertisements which will, in turn, increase the total revenue of the entertainment industry. Also, the effective cost involved in the advertisement will reduce too because the history and past interaction details of the target viewers are already available. [Source]\n\nAnnotating the dataset on the Labellerr:\n\nTo prepare a dataset for your model you don’t need to try out the hectic approach of manually performing the data annotation. Leave all your problems and tensions on Labellerr. \n\nHere is a demo video that will walk you through the process: https://blog.labellerr.com/index.php/2021/04/07/revolutionizing-the-entertainment-industry-for-better-customer-",
    "video_url": "https://www.youtube.com/watch?v=CGISJWBd80c",
    "embed_url": "https://www.youtube.com/embed/CGISJWBd80c",
    "duration": 92,
    "view_count": 115,
    "upload_date": "20210407",
    "uploader": "Labellerr",
    "tags": [
      "computer graphics",
      "deep learning models",
      "visual data science",
      "visual storytelling",
      "augmented movie",
      "data annotation tools",
      "ai storytelling",
      "ai visuals",
      "data labeling",
      "image tagging",
      "3d visuals",
      "data annotation",
      "movies",
      "image annotation",
      "data",
      "machine learning",
      "artificial intelligence",
      "entertainment",
      "deep learning",
      "ai trends",
      "data science",
      "3d modeling",
      "image classification",
      "visual recognition",
      "augmented reality",
      "computer vision",
      "visual data processing"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=CGISJWBd80c! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "a5LWO5EPU4Y",
    "title": "Labellerr, AI in Retail - Supply Chain Machine Learning Annotation",
    "description": "",
    "video_url": "https://www.youtube.com/watch?v=a5LWO5EPU4Y",
    "embed_url": "https://www.youtube.com/embed/a5LWO5EPU4Y",
    "duration": 92,
    "view_count": 98,
    "upload_date": "20210322",
    "uploader": "Labellerr",
    "tags": [
      "retail AI solutions",
      "algorithm optimization",
      "AI data labeling",
      "computer vision retail",
      "retail automation",
      "AI supply chain",
      "retail machine learning",
      "automated data annotation",
      "data annotation",
      "digital transformation",
      "machine learning",
      "AI in retail",
      "deep learning",
      "data science",
      "dataset creation",
      "data preprocessing",
      "deep learning models",
      "computer vision",
      "machine learning annotation",
      "data labeling",
      "feature engineering",
      "retail analytics"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=a5LWO5EPU4Y! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "HivXwak-B6Q",
    "title": "Attribute Recognisation by Labeller, Machine Learning Data Annotation Platform, Saas",
    "description": "",
    "video_url": "https://www.youtube.com/watch?v=HivXwak-B6Q",
    "embed_url": "https://www.youtube.com/embed/HivXwak-B6Q",
    "duration": 90,
    "view_count": 94,
    "upload_date": "20210317",
    "uploader": "Labellerr",
    "tags": [
      "visual recognition",
      "image labeling",
      "data labeling",
      "cnn architecture",
      "object detection",
      "transfer learning",
      "image segmentation",
      "data science",
      "model evaluation",
      "data annotation platform",
      "ai training",
      "feature extraction",
      "ai models",
      "feature mapping",
      "image detection",
      "AI data labeling",
      "data annotation",
      "deep learning",
      "machine learning",
      "data preprocessing",
      "dataset creation",
      "image classification",
      "deep learning models",
      "data augmentation",
      "computer vision"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=HivXwak-B6Q! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "89fr8Up_bM8",
    "title": "Fashion Parsing - Data Annotation by Labellerr Automated AI and Data Annotation SAAS platform",
    "description": "",
    "video_url": "https://www.youtube.com/watch?v=89fr8Up_bM8",
    "embed_url": "https://www.youtube.com/embed/89fr8Up_bM8",
    "duration": 201,
    "view_count": 53,
    "upload_date": "20210309",
    "uploader": "Labellerr",
    "tags": [
      "SAAS platform",
      "image segmentation",
      "image labeling",
      "annotation platforms",
      "annotation software",
      "visual data",
      "training data",
      "computer vision",
      "annotation strategies",
      "data labeling",
      "visual recognition",
      "image processing",
      "data annotation",
      "deep learning",
      "machine learning",
      "dataset creation",
      "deep learning models",
      "image annotation",
      "data preprocessing",
      "labeling software",
      "image classification",
      "data science",
      "video annotation",
      "what is deep learning"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=89fr8Up_bM8! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "KgFyTT_iNvk",
    "title": "How to Create a New Image Annotation Project on Labellerr - A Step-by-Step Guide",
    "description": "Learn how to easily set up an image annotation project using Labellerr's powerful platform. Discover step-by-step guidance for creating projects, managing workflows, and enabling smooth collaboration for your data labeling tasks.\n\nChapters\n0:00 Introduction\n0:07 Creating a New Project on Labellerr\n0:56 Selecting Image Data Type\n1:10 Connecting Data Sources\n1:30 Uploading Celebrity Images for Annotation\n2:16 Creating a Dataset\n2:43 Configuring Annotation Templates\n3:06 Setting Up Bounding Box for Pets\n3:17 Using Polygon Tool for Car Mirror\n3:56 Adding Key Points for Faces\n4:26 Connecting Machine Learning Models\n4:47 Setting Up Workflow and QC Process\n5:12 Adding Users to the Project\n5:29 Naming and Creating the Project\n5:40 Starting the Labeling Process\n6:01 Annotating with Polygon and Bounding Box Tools\n7:25 Marking Key Points for Faces\n8:01 Adjusting Image Brightness and Zoom\n8:27 Reviewing and Monitoring Annotations\n9:30 Conclusion and Contact Information\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=KgFyTT_iNvk",
    "embed_url": "https://www.youtube.com/embed/KgFyTT_iNvk",
    "duration": 588,
    "view_count": 1170,
    "upload_date": "20210308",
    "uploader": "Labellerr",
    "tags": [
      "image annotation",
      "image annotation tool",
      "data annotation",
      "annotation tool",
      "video annotation",
      "image segmentation",
      "image annotation software",
      "python image annotation",
      "image annotation tutorial",
      "quality assurance",
      "image markup",
      "annotation software",
      "data preprocessing",
      "markup software",
      "pixel annotation",
      "annotation methods",
      "data labeling tools",
      "data visualization",
      "image label",
      "labeling software",
      "software solutions",
      "python annotation",
      "reliable image annotation service"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "in this video i'm gonna record uh the steps on how to create a new project on labellerr for let's say an image annotation that you want to or you want to set it up for your team so first while you are on a screen it could be that you might be having already some projects created and then one of the projects would be already selected however you would have an option in case you is the first time that you're logged in into deploy then you would have an option of create new project okay you can click here either or go to settings screen and click on create new project okay and what you will see is there are various options listed depending on the kind of license that you own based on that different data types would be activated so let's take an example of image data today okay so i'll select the image data and then it will ask me various options um where from where i would like to connect my data so in a single project you can actually have your data connected from multiple sources even from single source you can have multiple different connections right but for the sake of simplicity i will upload it from my local so today we will uh do some sort of celebrities images where we will identify some sku's some sort of objects that they have picked up or the fashion clothing that they have worn right so so this is the random google search images let's like say like this this contains pictures of celebrities like rats pit angelina jolie and mark zuckerberg and a dog and let me do create data set now what you will see the system is going to upload that images also what you are able to see there are various other data sets that my team which my team has uploaded so i can see that as well okay so i can either link them up as well and add them into the project but let us continue for the sake of simplicity and there i am able to choose either i can use these templates or i can choose my own custom annotation configuration so here i am on the configuration screen i can name this template as saying that celebrity configuration version one and here so one of the images contains this pattern car so i can say this pets as i want to draw a bounding cursor on the pet then i might want to mark the mirror of the car with a polygon pets i would mark as bonding box in case i want to name the particular definition about the pit saying that what color is it so i can choose input box let's say and so done so within pet i would be able to describe various properties of the pet and then let us choose uh phrase and we would say some sort of dots let's say for now we will use just dot as for face and i could say whether any celebrities in the picture so i can say yes and now so yeah and let's move next and so depending on again from the license i would have an access to order label capabilities in the system i can connect to various models machine learning models available or the deep learning models available or it could be models or even the algorithms right so without taking this complexity for now and just moving on to the next step so i can set various uh you know how many what stages or what workflow that i want to set up i want to set up annotation i want to set up a qc process and i also want enable my customer to be part parties to participate in the annotation process apart from that you as you can see we can also enable gold standard labeling in the in the system where we can actually have automated evaluation but let's skip that for now and then i can add various users here apart from what myself who is login from support because i'm a super admin so i can add on more users and i can just hit create and let's name this celebrities image project and just create project and this is done now let's move on so uh directly to start labeling just think of it that i have given the access to an annotator the annotator will go in there and so you would see here this right so let me use the polygon tool to mark the the mirror and you can see that right so yeah and i can adjust this right correct uh i can even you know increase or decrease the the radius and all and plus i can let's say mark the face of the pet but and just like let me say i want to just color you know take the whole pet in this so so let's say now by chance i by mistakenly i you know annotated onto the face so i can actually adjust that as you can see it's very smooth here bounding box so that is done whether any celebrities in the picture no right so i've marked already the bits so face we would use for these celebrities and you can see there there's different annotations being mapped so i can hit submit and also while in that happens you can see that there is time being noted overall session time how much time i am taking and also individual thing and also as you can see i can you know adjust the picture and zoom in right and let's mark the faces here right oh so we'll be using dot for the faces here so we configured face so various key points and we can even configure them together similarly you know these represent various annotations for the face right right and i can even adjust the brightness and all you can see that you know it helps easily or yes celebrities are there in the picture i can hit submit and let's say i'm not sure what to annotate here uh what i can do i can say oh i'm not sure how to annotate so i can just say hit submit now let's move on to the activity uh how this so i can actually as a supervisor i can have a detailed look into what activity is going on real time as you can see here right so who did what and how different uh how the images are moving to different statuses so and then you know as a reviewer i can go in there and you know look into this how what was annotated and i can see that who annotated what what was the answer that was made right and i can even edit it and reject it or accept it so that you know and i can also see various who did what right so and and you can also see the draft is being saved so as i'm annotating something so it's continuously saving on the my work so that it is not lost even if there is some bad internet connection right so yeah i think this is what i wanted to share now how simple it is right and for more details visit www.labellerr.com in case you have any question do reach out to uh support tensormatics.com thank you have a nice day",
    "transcript_chunks": [
      "in this video i'm gonna record uh the steps on how to create a new project on labellerr for let's say an image annotation that you want to or you want to set it up for your team so first while you are on a screen it could be that you might be having already some projects created and then one of the projects would be already selected however you would have an option in case you is the first time that you're logged in into deploy then you would have an option of create new project okay you can click here either or go to settings screen and click on create new project okay and what you will see is there are various options listed depending on the kind of license that you own based on that different data types would be activated so let's take an example of image data today okay so i'll select the image data and then it will ask me various options um where from where i would like to connect my data so in a single project you can actually have your data connected from multiple sources even from single source you can have multiple different connections right but for the sake of simplicity i will upload it from my local so today we will uh do some sort of celebrities images where we will identify some sku's some sort of objects that they have picked up or the fashion clothing that they have worn right so so this is the random google search images let's like say like this this contains pictures of celebrities like rats pit angelina jolie and mark zuckerberg and a dog and let me do create data set now what you will see the system is going to",
      "upload that images also what you are able to see there are various other data sets that my team which my team has uploaded so i can see that as well okay so i can either link them up as well and add them into the project but let us continue for the sake of simplicity and there i am able to choose either i can use these templates or i can choose my own custom annotation configuration so here i am on the configuration screen i can name this template as saying that celebrity configuration version one and here so one of the images contains this pattern car so i can say this pets as i want to draw a bounding cursor on the pet then i might want to mark the mirror of the car with a polygon pets i would mark as bonding box in case i want to name the particular definition about the pit saying that what color is it so i can choose input box let's say and so done so within pet i would be able to describe various properties of the pet and then let us choose uh phrase and we would say some sort of dots let's say for now we will use just dot as for face and i could say whether any celebrities in the picture so i can say yes and now so yeah and let's move next and so depending on again from the license i would have an access to order label capabilities in the system i can connect to various models machine learning models available or the deep learning models available or it could be models or even the algorithms right so without taking this complexity for now and just moving on to the next",
      "step so i can set various uh you know how many what stages or what workflow that i want to set up i want to set up annotation i want to set up a qc process and i also want enable my customer to be part parties to participate in the annotation process apart from that you as you can see we can also enable gold standard labeling in the in the system where we can actually have automated evaluation but let's skip that for now and then i can add various users here apart from what myself who is login from support because i'm a super admin so i can add on more users and i can just hit create and let's name this celebrities image project and just create project and this is done now let's move on so uh directly to start labeling just think of it that i have given the access to an annotator the annotator will go in there and so you would see here this right so let me use the polygon tool to mark the the mirror and you can see that right so yeah and i can adjust this right correct uh i can even you know increase or decrease the the radius and all and plus i can let's say mark the face of the pet but and just like let me say i want to just color you know take the whole pet in this so so let's say now by chance i by mistakenly i you know annotated onto the face so i can actually adjust that as you can see it's very smooth here bounding box so that is done whether any celebrities in the picture no right so i've marked already the bits so face we",
      "would use for these celebrities and you can see there there's different annotations being mapped so i can hit submit and also while in that happens you can see that there is time being noted overall session time how much time i am taking and also individual thing and also as you can see i can you know adjust the picture and zoom in right and let's mark the faces here right oh so we'll be using dot for the faces here so we configured face so various key points and we can even configure them together similarly you know these represent various annotations for the face right right and i can even adjust the brightness and all you can see that you know it helps easily or yes celebrities are there in the picture i can hit submit and let's say i'm not sure what to annotate here uh what i can do i can say oh i'm not sure how to annotate so i can just say hit submit now let's move on to the activity uh how this so i can actually as a supervisor i can have a detailed look into what activity is going on real time as you can see here right so who did what and how different uh how the images are moving to different statuses so and then you know as a reviewer i can go in there and you know look into this how what was annotated and i can see that who annotated what what was the answer that was made right and i can even edit it and reject it or accept it so that you know and i can also see various who did what right so and and you can also see the draft is being",
      "saved so as i'm annotating something so it's continuously saving on the my work so that it is not lost even if there is some bad internet connection right so yeah i think this is what i wanted to share now how simple it is right and for more details visit www.labellerr.com in case you have any question do reach out to uh support tensormatics.com thank you have a nice day"
    ],
    "transcript_word_count": 1271,
    "transcript_chunk_count": 5
  },
  {
    "video_id": "um3FikXdEf4",
    "title": "AI Powered Data Annotation to Build Incredible Computer Vision & NLP Applications - Labellerr",
    "description": "The world runs on #data and for any #machinelearning model to perform in the most optimal manner input is the key. The reason why AmazonGo is successful is because of its prompt facial recognition technology. It’s a seamless process for customers to just pick up a product and leave. \n\nFor this technology to work successfully the most important component is data labeling. Every face is labeled with its genuine personal profile and thereby it’s an amazing experience for both brand and its customers. \n\nFeed-in the right data and you will experience the magic of machine learning technologies. Check out the self-help data annotation platform by Labellerr. It enables you to label the data all by urself in the most simple way. You can also hire data annotators from their marketplace. Basically, Labellerr is the one-stop-shop for all your data annotation requirements. \n\nTry labeling for free at - www.labellerr.com",
    "video_url": "https://www.youtube.com/watch?v=um3FikXdEf4",
    "embed_url": "https://www.youtube.com/embed/um3FikXdEf4",
    "duration": 14,
    "view_count": 26,
    "upload_date": "20210303",
    "uploader": "Labellerr",
    "tags": [
      "data labeling",
      "data annotation",
      "annotation",
      "data",
      "machine learning",
      "deep learning",
      "computer vision",
      "artificial intelligence",
      "ai",
      "ml",
      "data preprocessing",
      "data science",
      "annotation software",
      "AI data annotation",
      "NLP applications",
      "data annotation platform",
      "automated annotation",
      "AI annotation tools",
      "AI for NLP",
      "computer vision AI",
      "image annotation",
      "text annotation",
      "large language models",
      "machine learning for beginners",
      "neural networks"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=um3FikXdEf4! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "if03NtreFWI",
    "title": "AI-Powered Data Annotation Services and Project Analytics Dashboard - www.labellerr.com",
    "description": "Labellerr offers AI-powered data annotation services to streamline your machine learning projects while providing full visibility into project analytics and performance. Track progress in real-time, monitor workflows, and analyze annotation velocity at file, label, and user levels. Simplify data labeling, ensure quality, and save time with our cutting-edge platform.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=if03NtreFWI",
    "embed_url": "https://www.youtube.com/embed/if03NtreFWI",
    "duration": 166,
    "view_count": 86,
    "upload_date": "20210303",
    "uploader": "Labellerr",
    "tags": [
      "data labeling service",
      "data science",
      "3d annotation",
      "lidar annotation",
      "video annotation",
      "data analytics",
      "data categorization",
      "data collection",
      "data labeling",
      "data nerd",
      "data tagging",
      "data preprocessing",
      "video labeling",
      "data visualization",
      "annotation software",
      "machine learning",
      "image annotation",
      "data annotation",
      "data annotation services",
      "how to make money with data annotation jobs",
      "data annotation tech",
      "data annotation tools",
      "annotation",
      "best data labeling services"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=if03NtreFWI! This is most likely caused by:\n\nNo transcripts were found for any of the requested language codes: ('en',)\n\nFor this video (if03NtreFWI) transcripts are available in the following languages:\n\n(MANUALLY CREATED)\nNone\n\n(GENERATED)\n - hi (\"Hindi (auto-generated)\")\n\n(TRANSLATION LANGUAGES)\nNone\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "nUo76jTOls4",
    "title": "Machine Learning Powered Fashion Data Annotation (Landmark detection in Fashion - Virtual styling)",
    "description": "47% of return, refund, and exchanges occur because the product does not fulfill the customer’s expectations.\n\nA Global web index report highlights that 56% of clothing/ shoes sold online were returned just in the year 2019.  The increased rate of returns not only concerns the retailer but increases the load on the customer side as well. 33% of online retailers admit that they offer free returns but offset the cost of returns by charging for delivery and 20% said they would increase the cost of the product to cover the return expenses.\n\nComputer vision AI-driven fashion landmark technology is one of the ways. Fashion landmark detection aims to predict the positions of functional key points defined on clothing, such as the corners of the neckline, the hemline, and the cuff. \n\nNot only do these landmarks indicate the functional regions of clothing, but they also implicitly capture their bounding boxes, forming the design, pattern, and category of clothing.\n\nScale variations and non-rigid deformations in clothing lead to increased challenges in more complex patterns than restricted deformations, makes landmark detection even more data-hungry to satisfy algorithms and this differentiates it from known pose estimation.\n\nNow you wonder, the solution is good but how to access good quality&nbsp; labeled data for training the model.\n\nHere’s your solution - http://www.labellerr.com\n\nWait are you wondering how to get this integrated within a few weeks into your production system without investing in building an AI team!\n\nVisit our website at http://www.labellerr.com and mention your use case in brief and our customer engineer will contact you and help you prepare the plan and get you running on a trial with us to validate. \n\nMail us at labellerr@tensormatics.com for more info.",
    "video_url": "https://www.youtube.com/watch?v=nUo76jTOls4",
    "embed_url": "https://www.youtube.com/embed/nUo76jTOls4",
    "duration": 168,
    "view_count": 315,
    "upload_date": "20210301",
    "uploader": "Labellerr",
    "tags": [
      "fashion",
      "fashion technology",
      "machine learning",
      "deep learning",
      "neural network",
      "ecommerce",
      "cpg",
      "data",
      "data annotation",
      "annotate",
      "data labeling",
      "landmark detection",
      "virtual trial room",
      "virtual trial",
      "online shopping",
      "augmented shopping",
      "ecommerce trends",
      "virtual wardrobe",
      "ecommerce innovation",
      "ai fashion",
      "trend forecasting",
      "consumer insights",
      "virtual shopping",
      "virtual retail",
      "online fitting",
      "smart ecommerce",
      "fashion analytics",
      "smart fitting",
      "deep learning trends"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] you [Music] [Music] [Music] [Music] you [Music] [Music] [Music] [Music] you",
    "transcript_chunks": [
      "[Music] you [Music] [Music] [Music] [Music] you [Music] [Music] [Music] [Music] you"
    ],
    "transcript_word_count": 12,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "NB_a0sw5QEs",
    "title": "Crubside Pickup Made Efficient with Data Annotation",
    "description": "Curbside pickup at retail stores surges 208% during coronavirus pandemic - CNBC\n\nClick and pick has become the new shopping mantra for retail stores. Data annotation can be of great help to enable efficient pickup from shelves and prepare for incoming orders.\n\n- DataAnnotation can enable executives to pre-prepare the orders in real-time and print receipts before the customer arrive. \n- With accurate data annotation, the executives can get notified of the pickups done at the store.  They can replenish the out-of-stock items in real-time\n- Executives can also get notified about customer arrivals and can use their apps to manage the inflow of customers to ensure safe distancing during the pandemic\n\nExplore Labellerr a one-stop-shop, high-quality data annotation platform that provides exceptional annotating accuracy to the Retail and CPG industry. \n\nReach out to us to know more about how you can implement our world-class data annotation platform for your business. \n\nP.S - With our plug-n-play, API solution businesses can customize according to their scenario easily and swiftly!",
    "video_url": "https://www.youtube.com/watch?v=NB_a0sw5QEs",
    "embed_url": "https://www.youtube.com/embed/NB_a0sw5QEs",
    "duration": 10,
    "view_count": 20,
    "upload_date": "20210123",
    "uploader": "Labellerr",
    "tags": [
      "pickup services",
      "computer vision apps",
      "data labeling tools",
      "ecommerce analytics",
      "inventory tracking",
      "inventory software",
      "digital retail",
      "customer pickup",
      "data visualization",
      "warehouse management",
      "digital commerce",
      "data labeling software",
      "deep learning models",
      "digital transformation",
      "visual data",
      "ecommerce tools",
      "data annotation",
      "online",
      "ecommerce",
      "inventory management",
      "computer vision",
      "deep learning",
      "inventory tools",
      "ecommerce trends",
      "data labeling",
      "inventory control"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=NB_a0sw5QEs! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "PHgkCn5PHDk",
    "title": "Accurate Data Annotation Increases Online Sales",
    "description": "\"Nearly 80% of UK consumers would switch online retailers if they had a negative #onlineshopping experience.\nArtificial Intelligence can transform #eCommerce and #retail in exceptional ways that make buyers more loyal to the platform.\n\nData annotation has a key role to play here. Annotating images and videos and tagging them with specific captions and keywords make it super easy to classify products and give online shoppers an incredible user experience. \n\nAccurate Image and Video tagging help in improving search relevance thereby, the system can recommend better products. \n\nSign Up on Labellerr, making AI journey easier, and start labeling by connecting your data from cloud/edge such as GCP, AWS, Azure to integrate computer vision AI into your retail stores or warehouses!",
    "video_url": "https://www.youtube.com/watch?v=PHgkCn5PHDk",
    "embed_url": "https://www.youtube.com/embed/PHgkCn5PHDk",
    "duration": 7,
    "view_count": 21,
    "upload_date": "20210122",
    "uploader": "Labellerr",
    "tags": [
      "deep learning models",
      "image annotation",
      "video annotation",
      "AI in retail",
      "AI in ecommerce",
      "image tagging",
      "video tagging",
      "AI for retail",
      "AI for ecommerce",
      "retail AI solutions",
      "annotation platform",
      "data annotation tools",
      "data annotation",
      "machine learning",
      "ml",
      "ai",
      "artificial intelligence",
      "data",
      "big data",
      "online",
      "sales",
      "data labeling",
      "labeller",
      "deep learning",
      "ai tools",
      "data science",
      "data management",
      "big data analytics",
      "data analysis",
      "labeling software"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=PHgkCn5PHDk! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "mKg39CkCaFk",
    "title": "Walmart Use Case with Data Annotation",
    "description": "Here is an interesting use case by retail giant @Wallmart. They launched a machine learning initiative to ensure comprehensive catalog maintenance of their retail inventory. If any item in the catalog was not present then it would result in misrepresentation of its availability to the end consumer. \n\nSince Walmart uses AI technology to classify the products for its system to accurately sense availability it was an extremely crucial task for them to classify inventory accurately. So with the help of a trusted Data Annotation partner, they could use ML and achieve a significant improvement in their retail item coverage from 91% to 98%. \n\nSignUp on @Labeller, the Fastest and smartest Data Labeling platform to build AI/ML/Deep learning model faster!",
    "video_url": "https://www.youtube.com/watch?v=mKg39CkCaFk",
    "embed_url": "https://www.youtube.com/embed/mKg39CkCaFk",
    "duration": 12,
    "view_count": 99,
    "upload_date": "20210115",
    "uploader": "Labellerr",
    "tags": [
      "annotation software",
      "annotation methods",
      "data taxonomy",
      "data insight",
      "deep learning",
      "ai tools",
      "labeling software",
      "image annotation",
      "data analysis",
      "image labeling",
      "annotation platform",
      "AI in retail",
      "dataset creation",
      "data validation",
      "deep learning models",
      "data annotation",
      "data labeling",
      "labeller",
      "data",
      "data for machine learning",
      "machine learning",
      "machinelearning",
      "artificial intelligence",
      "data labels",
      "data preprocessing"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=mKg39CkCaFk! This is most likely caused by:\n\nNo transcripts were found for any of the requested language codes: ('en',)\n\nFor this video (mKg39CkCaFk) transcripts are available in the following languages:\n\n(MANUALLY CREATED)\nNone\n\n(GENERATED)\n - fr (\"French (auto-generated)\")[TRANSLATABLE]\n\n(TRANSLATION LANGUAGES)\n - en (\"English\")\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "MpF_FxCBFjc",
    "title": "How Planogram (POG) Compliance can Help Retailers and CPGs Achieve High Sales",
    "description": "Here is an interesting case study, a top healthy-snack manufacturer in Australia was able to increase a 25.7% increase in net sales per store by optimizing the shelf location for all their SKUs. Using computer vision they identified ‘hot’ and ‘cold’ spots on the shelf and used this to ascertain which SKU should hold the prime positioning.\n\nIn a particular store, they experimented with placing the same product at eye-level in all the rows (1,2,3 &amp; 4). Interestingly they observed that the product was picked up from row 2 more than all the other rows.\n\nSuch granular observations need real-time monitoring of customer touch-points. It was done using computer vision technology coupled with data annotation and the best planogram was designed.",
    "video_url": "https://www.youtube.com/watch?v=MpF_FxCBFjc",
    "embed_url": "https://www.youtube.com/embed/MpF_FxCBFjc",
    "duration": 9,
    "view_count": 35,
    "upload_date": "20210114",
    "uploader": "Labellerr",
    "tags": [
      "computer vision",
      "machine learning",
      "wholesale",
      "data labeling",
      "data annotation",
      "inventory management",
      "purchase behavior",
      "visual merchandising",
      "wholesale strategies",
      "retail automation",
      "data management",
      "visual retail",
      "inventory tracking",
      "wholesale automation",
      "data labeling tools",
      "inventory software",
      "SKU optimization",
      "retail technology",
      "planogram software",
      "shelf positioning",
      "real-time monitoring",
      "retail AI",
      "computer vision in retail"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=MpF_FxCBFjc! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "4fcED0gPoCU",
    "title": "An interesting Data Annotation (Labeling) Fact!",
    "description": "👉  The standard frame rate of a video is 24 frames per second\n👉  To accurately label a 5-minute video annotator needs to process 7200 images\n👉  Let's conservatively assume the time taken for this task is 30 seconds per image.\n👉 In total, this task represents 60 hours of labor for an individual!\n\nThat's a lot of time and effort to label a 5 min video! That is why companies are looking out for a reliable partner who enables them to perform Data Labelling at scale with accuracy. \n\nSign Up on Labellerr, making AI journey easier, and start labeling by connecting your data from cloud/edge such as GCP, AWS, Azure to integrate computer vision AI into your retail stores or warehouses!\n\nWebsite: https://labellerr.com/\nLinkedIn: https://www.linkedin.com/company/labellerr",
    "video_url": "https://www.youtube.com/watch?v=4fcED0gPoCU",
    "embed_url": "https://www.youtube.com/embed/4fcED0gPoCU",
    "duration": 25,
    "view_count": 89,
    "upload_date": "20210110",
    "uploader": "Labellerr",
    "tags": [
      "ml",
      "ai",
      "artificialintelligence",
      "artificial intelligence",
      "computer vision",
      "data annotation",
      "data labeling",
      "fact",
      "fun facts",
      "fun fact",
      "data",
      "data label",
      "labeling",
      "labeling automation",
      "automation",
      "retail",
      "retail industry",
      "ai automation",
      "deep learning",
      "ai trends",
      "data mining",
      "video annotation",
      "data analytics",
      "machine learning",
      "data science",
      "image annotation",
      "annotation at scale",
      "AI data labeling",
      "cloud data labeling",
      "AWS annotation",
      "big data",
      "facts",
      "random facts"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "[Music] [Music] [Music] you",
    "transcript_chunks": [
      "[Music] [Music] [Music] you"
    ],
    "transcript_word_count": 4,
    "transcript_chunk_count": 1
  },
  {
    "video_id": "v9mproCMJ4g",
    "title": "Bounding Box annotation in a retail use case with Labellerr, automated annotation data platform",
    "description": "This video covers the annotation screen of Labellerr ML assisted data annotation platform which shows a use case scenario of customer shopping in a retail store which can be used to build models to identify customer interaction and activities in retail store",
    "video_url": "https://www.youtube.com/watch?v=v9mproCMJ4g",
    "embed_url": "https://www.youtube.com/embed/v9mproCMJ4g",
    "duration": 58,
    "view_count": 197,
    "upload_date": "20201218",
    "uploader": "Labellerr",
    "tags": [
      "data preprocessing",
      "annotation workflow",
      "visual data",
      "data annotation platform",
      "image segmentation",
      "ai projects",
      "retail annotation",
      "project management",
      "labeling software",
      "image classification",
      "ML assisted annotation",
      "data annotation",
      "data labeling",
      "deep learning",
      "retail",
      "AI",
      "machine learning",
      "deep learning models",
      "image annotation",
      "data science",
      "annotation software",
      "dataset creation",
      "deep learning projects",
      "computer vision",
      "image labeling",
      "bounding box annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=v9mproCMJ4g! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "URZbeXMQHZs",
    "title": "Create AI Annotation Projects Effortlessly | Labellerr - End-to-End Data Annotation Solution",
    "description": "Learn how to create AI annotation projects effortlessly with Labellerr, your trusted End-to-End Data Annotation Solution. Whether it's image classification, bounding boxes, or AI-driven automation, Labellerr makes it simple to annotate images, text, videos, audio, and documents.\n\nChapters\n0:01 Introduction\n1:01 Create New Project\n2:01 Classification\n4:02 Bounding Box Annotation\n\nSign up today and start your journey:\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=URZbeXMQHZs",
    "embed_url": "https://www.youtube.com/embed/URZbeXMQHZs",
    "duration": 301,
    "view_count": 139,
    "upload_date": "20201102",
    "uploader": "Labellerr",
    "tags": [
      "image annotation",
      "automatic annotation",
      "data annotation in autonomous vehicles",
      "what is data annotation?",
      "data annotation processes",
      "annotations",
      "video annotation",
      "computer vision data annotation",
      "automating annotation",
      "ai data annotation",
      "data annotation tech",
      "data annotation in autonomous driving",
      "event annotation",
      "data annotation",
      "autonomous driving annotation",
      "data annotation services",
      "training data annotation",
      "annotation software",
      "benefits of data annotation services"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "hello everyone welcome to the user guide series of labellerr the end-to-end ai data annotation solution for images text video audio and document in this video i shall be walking you through on how to create a project in labellerr platform and while i do that i am taking certain assumptions that you have already signed up on our website labellerr by visiting www.labellerr.com and and now you are into announce assuming that your domain is demo.labellerr.com which i own i have this domain it's demo.tbr.com i would see this screen like this the very first time i log in and in for your case it could be that you would it would tell you that you need to create your first project so what you can do is click here create new project and let's say we are building uh image annotation project okay and in this case we will have our data on local and let's say images data set one which i will upload my data into images dataset one i can save it so that i can use it even later in other projects as well or in case i am building it at the project level or i can add more data to this data set okay so let me choose few images from my local system which i have downloaded already as you can see so here there are five images and i click on create data set so now as you can see here the five files are five images that has been uploaded and i can hit next and let me create two types of questions here uh one is classification so classification question could be okay the it could be about image quality so while my annotators attack this and so they might want to rate the image quality high medium and low right and done so we added this question and similarly we can add more questions in case of classification at an image level and now if we want to have more detailed annotation like bounding boxes so let's say i want to identify person or i want to identify a car in the in the images then i can do that so i have two pounding boxes i can even go and further depth where i can add few attributes for each person level but let's continue like this and this is automated model selection wherein it this is automated ai based annotation feature which is available to premium customers and then i can enable review on my project wherein first an annotator will do an annotate and then the reviewer will preview that image and then i can add users or i can skip i can so the image project one so i named this project as image project one and i hit create and let's see quickly okay so this project is created and if you can see here let's see whether images has been loaded or not okay and let me start labeling yes so this is the first image okay and i can say okay let me take bonding box and draw around the person and this is person so in case the attributes would be then i would choose those attributes and then i can choose a car and this could be a car and yes and then i hit as you can see this bounding boxes and i say image quality medium and summit yes and then next will appear and i can even skip if i'm not sure i am not sure these are the remarks and yes and i skipped it right and similarly",
    "transcript_chunks": [
      "hello everyone welcome to the user guide series of labellerr the end-to-end ai data annotation solution for images text video audio and document in this video i shall be walking you through on how to create a project in labellerr platform and while i do that i am taking certain assumptions that you have already signed up on our website labellerr by visiting www.labellerr.com and and now you are into announce assuming that your domain is demo.labellerr.com which i own i have this domain it's demo.tbr.com i would see this screen like this the very first time i log in and in for your case it could be that you would it would tell you that you need to create your first project so what you can do is click here create new project and let's say we are building uh image annotation project okay and in this case we will have our data on local and let's say images data set one which i will upload my data into images dataset one i can save it so that i can use it even later in other projects as well or in case i am building it at the project level or i can add more data to this data set okay so let me choose few images from my local system which i have downloaded already as you can see so here there are five images and i click on create data set so now as you can see here the five files are five images that has been uploaded and i can hit next and let me create two types of questions here uh one is classification so classification question could be okay the it could be about image quality so while my annotators attack this and so",
      "they might want to rate the image quality high medium and low right and done so we added this question and similarly we can add more questions in case of classification at an image level and now if we want to have more detailed annotation like bounding boxes so let's say i want to identify person or i want to identify a car in the in the images then i can do that so i have two pounding boxes i can even go and further depth where i can add few attributes for each person level but let's continue like this and this is automated model selection wherein it this is automated ai based annotation feature which is available to premium customers and then i can enable review on my project wherein first an annotator will do an annotate and then the reviewer will preview that image and then i can add users or i can skip i can so the image project one so i named this project as image project one and i hit create and let's see quickly okay so this project is created and if you can see here let's see whether images has been loaded or not okay and let me start labeling yes so this is the first image okay and i can say okay let me take bonding box and draw around the person and this is person so in case the attributes would be then i would choose those attributes and then i can choose a car and this could be a car and yes and then i hit as you can see this bounding boxes and i say image quality medium and summit yes and then next will appear and i can even skip if i'm not sure i am not",
      "sure these are the remarks and yes and i skipped it right and similarly"
    ],
    "transcript_word_count": 614,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "ucuTN2GEMa4",
    "title": "Understanding RGB with Data Annotation Solutions | Labellerr",
    "description": "Discover how data annotation solutions enhance RGB image processing with step-by-step insights into breaking down colored images into red, green, and blue channels. Using tools like NumPy and OpenCV in a Jupyter Notebook, this example highlights the potential of accurate data labeling in computer vision tasks.\n\nExplore Labellerr’s tailored data annotation solutions to transform your machine learning projects today.\n\nWebsite: https://www.labellerr.com\nBook a Demo: https://www.labellerr.com/book-a-demo",
    "video_url": "https://www.youtube.com/watch?v=ucuTN2GEMa4",
    "embed_url": "https://www.youtube.com/embed/ucuTN2GEMa4",
    "duration": 265,
    "view_count": 186,
    "upload_date": "20201015",
    "uploader": "Labellerr",
    "tags": [
      "video labeling",
      "image classification",
      "manual annotation",
      "image labeling",
      "data preprocessing",
      "data labeling tools",
      "video tracking",
      "labeling software",
      "computer vision",
      "data markup",
      "image segmentation",
      "data validation",
      "data annotation",
      "data annotation services",
      "data annotation company",
      "image annotation",
      "video annotation",
      "image annotation services",
      "annotation",
      "lidar annotation",
      "data annotation tools",
      "data annotation service",
      "data annotation for ai",
      "annotation software"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "good morning friends uh my name is puneet and i'm a machine learning practitioner and today uh this is this is my first attempt to create a simple video on the concepts that i have learned in last decade of my experience in solving various idea and data science related use cases and including uh my recent uh last two years of experience while building labellerr and end-to-end data annotation solutions for deep learning use cases okay in today's video i would be focusing on uh showing a concept on how a colored image is built from red green and blue channel and how you can look at that those channels and its pixel in detail and this i am demonstrating with the help of opencv library uh which is a computer vision library and numpy which is a numerical pi computer numerical computing python library okay and this i am showcasing is in python okay so let's start um so we will first start with the pip installation within a jupyter notebook as you can see this is a screen of the jupiter notebook and also as you can see the installation is already done on my system and that's why it is showing requirement already satisfied but in your use case as long as you have python this command will help you install uh these packages okay and the second step would be to import these libraries the this is a more standard way of importing it and you can find it over the internet as well and majority of the uh tutorials like this so for the purpose of this terminal today i have taken a very cute uh image of a child which is crying and as you can see this so this is the image of the child and now computer vision opencv library provide this read function to read images so i have to have this jpeg image so let's read this and and if i want to show get into the detail of how it looks like so this is this is a typical mathematical representation of different pixel values in different channels okay and what we do is we copy this image and uh then mark each of uh uh keep that one of the channels which is red on the uh red which is on this the last the third dimension and put everything other channels zero and one as all the pixels are zero so we'll only will be left with the red color and similarly we do it for green and blue respectively and and we mark other pixel values as zero respectively and we create three copies basically in each case and let's see and now let's plot these images so so red contains only pixel values with red channel green g contains with the pixel values with green color and v contains the image with pixel values only in blue channel which is the zeroth channel okay and let's plot these one by one okay and if you can see this is our first colored image and then if you see this i'm plotting the r here and this is red color and then i'm plotting the green color and then eventually i'm plotting the blue color right and so so this is this is all i wanted to demonstrate thanks for watching and uh so i hope this view like this video uh and i would require that if you can also share your use cases on how this concept could be useful in industrial use cases please do leave your comments in this video thanks bye have a good day",
    "transcript_chunks": [
      "good morning friends uh my name is puneet and i'm a machine learning practitioner and today uh this is this is my first attempt to create a simple video on the concepts that i have learned in last decade of my experience in solving various idea and data science related use cases and including uh my recent uh last two years of experience while building labellerr and end-to-end data annotation solutions for deep learning use cases okay in today's video i would be focusing on uh showing a concept on how a colored image is built from red green and blue channel and how you can look at that those channels and its pixel in detail and this i am demonstrating with the help of opencv library uh which is a computer vision library and numpy which is a numerical pi computer numerical computing python library okay and this i am showcasing is in python okay so let's start um so we will first start with the pip installation within a jupyter notebook as you can see this is a screen of the jupiter notebook and also as you can see the installation is already done on my system and that's why it is showing requirement already satisfied but in your use case as long as you have python this command will help you install uh these packages okay and the second step would be to import these libraries the this is a more standard way of importing it and you can find it over the internet as well and majority of the uh tutorials like this so for the purpose of this terminal today i have taken a very cute uh image of a child which is crying and as you can see this so this is the image",
      "of the child and now computer vision opencv library provide this read function to read images so i have to have this jpeg image so let's read this and and if i want to show get into the detail of how it looks like so this is this is a typical mathematical representation of different pixel values in different channels okay and what we do is we copy this image and uh then mark each of uh uh keep that one of the channels which is red on the uh red which is on this the last the third dimension and put everything other channels zero and one as all the pixels are zero so we'll only will be left with the red color and similarly we do it for green and blue respectively and and we mark other pixel values as zero respectively and we create three copies basically in each case and let's see and now let's plot these images so so red contains only pixel values with red channel green g contains with the pixel values with green color and v contains the image with pixel values only in blue channel which is the zeroth channel okay and let's plot these one by one okay and if you can see this is our first colored image and then if you see this i'm plotting the r here and this is red color and then i'm plotting the green color and then eventually i'm plotting the blue color right and so so this is this is all i wanted to demonstrate thanks for watching and uh so i hope this view like this video uh and i would require that if you can also share your use cases on how this concept could be useful in industrial use",
      "cases please do leave your comments in this video thanks bye have a good day"
    ],
    "transcript_word_count": 615,
    "transcript_chunk_count": 3
  },
  {
    "video_id": "7z18QH4lniA",
    "title": "Labellerr - Data Labelling Platform (B2B SaaS)",
    "description": "Quick view about labellerr features - 2D bounding box, Instance segmentation, image multi-label classification, semantic segmentation, face landmark keypoint annotation",
    "video_url": "https://www.youtube.com/watch?v=7z18QH4lniA",
    "embed_url": "https://www.youtube.com/embed/7z18QH4lniA",
    "duration": 21,
    "view_count": 111,
    "upload_date": "20201012",
    "uploader": "Labellerr",
    "tags": [
      "vision datasets",
      "annotation platform",
      "image classification",
      "visual data",
      "data annotation tool",
      "instance segmentation",
      "visual recognition",
      "data preprocessing",
      "software solutions",
      "ai training",
      "keypoint annotation",
      "data annotation",
      "machine learning",
      "labeling software",
      "computer vision",
      "image labeling",
      "labeling solutions",
      "data labeling platform",
      "annotation software",
      "dataset creation",
      "image annotation",
      "B2B SaaS",
      "deep learning",
      "image segmentation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=7z18QH4lniA! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "tq-IDLb84OI",
    "title": "Labeling process on Labellerr",
    "description": "Use Labellerr tool to build good quality training data for your AI/ML models 10x faster.\n\nWhen you sign in Labellerr tool for the first time you will automatically be redirected to the project creation page. Or you can go to the settings screen to create a new project.",
    "video_url": "https://www.youtube.com/watch?v=tq-IDLb84OI",
    "embed_url": "https://www.youtube.com/embed/tq-IDLb84OI",
    "duration": 24,
    "view_count": 203,
    "upload_date": "20201004",
    "uploader": "Labellerr",
    "tags": [
      "machine learning projects",
      "deep learning models",
      "object detection",
      "dataset creation",
      "labeling software",
      "computer vision projects",
      "vision datasets",
      "image labeling",
      "data science",
      "data labeling tools",
      "visual recognition",
      "data labeling",
      "image tagging",
      "image segmentation",
      "tagging software",
      "data annotation",
      "machine learning",
      "annotation software",
      "data preprocessing",
      "deep learning",
      "image annotation",
      "manual annotation",
      "visual data",
      "computer vision"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=tq-IDLb84OI! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "CMOvBUBAhOM",
    "title": "Creating new project and importing data in Labellerr",
    "description": "Use Labellerr tool to build good quality training data for your AI/ML models 10x Faster.\n\nWhen you sign in Labellerr tool for the first time you will automatically be redirected to the project creation page. Or you can go to the settings screen to create a new project.",
    "video_url": "https://www.youtube.com/watch?v=CMOvBUBAhOM",
    "embed_url": "https://www.youtube.com/embed/CMOvBUBAhOM",
    "duration": 74,
    "view_count": 33,
    "upload_date": "20201004",
    "uploader": "Labellerr",
    "tags": [
      "data preprocessing",
      "deep learning projects",
      "annotation workflow",
      "annotation tools",
      "data labeling platform",
      "AI annotation",
      "cloud annotation",
      "annotation service",
      "ML data labeling",
      "AI SaaS",
      "labeling for ML",
      "Labellerr features",
      "machine learning projects",
      "labeling software",
      "project management",
      "deep learning",
      "image annotation",
      "data annotation",
      "image segmentation",
      "annotation software",
      "dataset creation",
      "machine learning",
      "image classification",
      "computer vision"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=CMOvBUBAhOM! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "RlAjRxCi29I",
    "title": "AI and Deep Learning assisted image classification data annotation and labeling",
    "description": "Labellerr to enable enterprises to unlock data potential via AI enablement. At the core our platform helps in AI assisted unstructured data(image,text,audio, video, documents,geospatial) annotation to build AI/Ml models faster and accurate leading to increased revenues and costs reductions to the tune of 15-25%. \n\nLet me know in case you think this could be useful for your Data Science and AI teams to boost their productivity!\n\nYou can visit https://www.labellerr.com",
    "video_url": "https://www.youtube.com/watch?v=RlAjRxCi29I",
    "embed_url": "https://www.youtube.com/embed/RlAjRxCi29I",
    "duration": 118,
    "view_count": 50,
    "upload_date": "20200918",
    "uploader": "Labellerr",
    "tags": [
      "image labeling",
      "AI data labeling",
      "computer vision projects",
      "data augmentation",
      "image processing",
      "image annotation",
      "image segmentation",
      "data science",
      "ai projects",
      "image classification",
      "data annotation",
      "deep learning",
      "annotation software",
      "machine learning",
      "dataset creation",
      "feedback loops",
      "data preprocessing",
      "labeling software",
      "computer vision",
      "machine learning projects",
      "visual data",
      "deep learning projects",
      "vision datasets"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=RlAjRxCi29I! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "9CE30FjUlao",
    "title": "Labellerr Demo(www.labellerr.com)",
    "description": "Struggling with any 'Data Annotation' Work for building AI models?\n\nLooking for an enterprise 'Data Labeling' tool which can label data in any form(image/text/OCR/audio/video)? \n\nTry @Labellerr(www.labellerr.com) for FREE http://bit.ly/37aYX7r \n\nElse email your query to support@tensormatics.com. We would be happy to understand your current processes and any pain points.\n\nLinkedin: http://bit.ly/2NHO9H3\nFacebook: http://bit.ly/2qNJ1Ii\nTwitter: http://bit.ly/32JAzaq\nQuora: http://bit.ly/2EVnl0N",
    "video_url": "https://www.youtube.com/watch?v=9CE30FjUlao",
    "embed_url": "https://www.youtube.com/embed/9CE30FjUlao",
    "duration": 486,
    "view_count": 304,
    "upload_date": "20200108",
    "uploader": "Labellerr",
    "tags": [
      "data labeling tools",
      "deep learning models",
      "data annotation tools",
      "annotation platform",
      "AI data annotation",
      "text annotation",
      "audio annotation",
      "video annotation",
      "annotation demo",
      "Data Annotation",
      "Machine Learning",
      "Deep Learning",
      "Data Labeling",
      "Ai",
      "ML",
      "Labellerr",
      "labeling software",
      "image labeling",
      "ai training",
      "data annotation",
      "deep learning",
      "machine learning",
      "annotation software",
      "dataset creation",
      "image annotation"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=9CE30FjUlao! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  },
  {
    "video_id": "aTcIRF8eFOQ",
    "title": "Labellerr Demo(www.labellerr.com)",
    "description": "Struggling with any 'Data Annotation' Work for building AI models?\n\nLooking for an enterprise 'Data Labeling' tool which can label data in any form(image/text/OCR/audio/video)? \n\nTry @Labellerr(www.labellerr.com) for FREE http://bit.ly/37aYX7r \n\nElse email your query to support@tensormatics.com. We would be happy to understand your current processes and any pain points.\n\nLinkedin: http://bit.ly/2NHO9H3\nFacebook: http://bit.ly/2qNJ1Ii\nTwitter: http://bit.ly/32JAzaq\nQuora: http://bit.ly/2EVnl0N",
    "video_url": "https://www.youtube.com/watch?v=aTcIRF8eFOQ",
    "embed_url": "https://www.youtube.com/embed/aTcIRF8eFOQ",
    "duration": 732,
    "view_count": 904,
    "upload_date": "20191226",
    "uploader": "Labellerr",
    "tags": [
      "data science",
      "ml projects",
      "deep learning models",
      "annotation software",
      "data labeling tools",
      "AI data annotation",
      "data preprocessing",
      "image annotation",
      "text annotation",
      "Data labeling",
      "Machine Learning",
      "Artificial Intelligence",
      "deep learning",
      "data annotation",
      "object detection",
      "AI model",
      "AI",
      "ML",
      "tensorflow",
      "auto labeling",
      "auto ml",
      "AI algorithms",
      "labellerr",
      "dataset creation",
      "labeling software",
      "computer vision",
      "ai tools",
      "deep learning basics",
      "deep learning using tensorflow"
    ],
    "categories": [
      "Science & Technology"
    ],
    "transcript": "",
    "transcript_chunks": [],
    "transcript_word_count": 0,
    "transcript_chunk_count": 0,
    "transcript_error": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=aTcIRF8eFOQ! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!"
  }
]